II. RELATED WORK

A. Blood Stratification and Analysis Methods

Automated blood component analysis has primarily focused on cell-level detection and classification rather than spatial stratification detection. Early approaches leveraged classical image processing techniques for blood cell counting and morphological analysis, but these methods lacked robustness under varying imaging conditions. Recent deep learning advances have enabled automated quality assessment of blood specimens. An Inception-ResNet-V2 architecture was applied to serum quality classification, achieving an area under the curve exceeding 0.98 for distinguishing adequate versus inadequate serum samples [1]. However, this approach provided only sample-level classification without spatial localization of stratified layers. Bacea et al. [2] addressed liquid level detection in plasma tubes using a modified YOLOv4-tiny model, employing vertical image stitching to capture complete tube views and formulating the task as one-dimensional regression. Their method achieved 98.09% mean average precision for plasma interface detection but was limited to identifying a single boundary rather than simultaneously localizing multiple stratified components. Commercial systems such as the PerkinElmer JANUS Blood iQ workstation have incorporated dual-illumination imaging combining blue-light and white-light modalities to automate plasma and buffy coat localization [3]. While these systems demonstrate clinical utility, published technical details regarding cross-modal feature fusion mechanisms remain unavailable. Existing blood analysis methods thus exhibit two critical limitations: reliance on single-modality imaging that may not adequately reveal all stratified layers, and focus on single-interface detection rather than comprehensive multi-layer localization required for clinical assessment.

B. Multimodal Medical Image Fusion

Multimodal fusion in medical imaging has demonstrated substantial benefits for diagnostic tasks by integrating complementary information from different imaging modalities. Wang et al. [4] proposed MicFormer, employing deformable cross-attention Transformers to fuse CT and MRI modalities for cardiac segmentation, achieving improved delineation of anatomical structures through spatial alignment mechanisms. A2FSeg [5] introduced a two-stage fusion strategy combining initial averaging with adaptive weighted fusion to address brain tumor segmentation in scenarios with missing modalities, demonstrating state-of-the-art performance on the BraTS2020 benchmark. MACTFusion [6] developed a cross-modal attention mechanism with multi-axis design to integrate global and local interactions between medical image modalities, effectively capturing both intra-modal and inter-modal contextual relationships. MCTHNet [7] presented a modality-collaborative CNN-Transformer hybrid architecture designed for unpaired multi-modal segmentation tasks with limited annotations, leveraging complementary representations from convolutional and self-attention modules. Despite these advances, existing multimodal fusion methods predominantly target segmentation tasks rather than object detection, and most approaches assume pre-aligned or registered input modalities. Blood tube imaging with dual-illumination systems presents distinct challenges: spatial correspondence between modalities is not guaranteed due to slight variations in illumination geometry, stratified layers exhibit different visibility characteristics across modalities, and the detection task requires precise bounding box localization rather than dense pixel-wise segmentation. Furthermore, concatenation-based fusion strategies commonly employed in these methods do not adequately address fine-grained spatial alignment necessary for detecting thin stratified boundaries.

C. YOLO Architectures for Medical Imaging

YOLO-based object detection frameworks have been increasingly adapted for medical imaging applications due to their real-time inference capabilities and end-to-end trainable architectures. BGF-YOLO [8] integrated Bi-level Routing Attention and Generalized Feature Pyramid Networks with a fourth detection head specifically designed for brain tumor detection, achieving 4.7% mean average precision improvement over YOLOv8x baseline. RCS-YOLO [9] employed RepConv ShuffleNet-based lightweight architectures to accelerate brain tumor detection by 60% while maintaining detection accuracy, demonstrating the feasibility of efficient deployment. The evolution from YOLOv8 to YOLO11 [10] introduced Position-sensitive Spatial Attention and improved downsampling strategies that enhance single-modality RGB detection performance. However, these architectures inherently operate on three-channel RGB inputs and lack mechanisms for processing multi-spectral or dual-modality data. Recent efforts in multispectral object detection have explored fusion strategies for complementary imaging modalities. YOLO-Phantom [11] combined RGB and infrared imaging through Phantom Convolution modules for low-light IoT detection, employing simple feature concatenation without explicit cross-modal attention. Li et al. [12] developed ADMF-Net with Convolutional Block Attention Modules to fuse forward and backward illumination for aircraft glass defect detection, demonstrating benefits of attention-guided fusion. Nevertheless, these approaches rely on either early-stage concatenation or channel-wise attention mechanisms that do not explicitly model spatial correspondences between modalities. For blood stratification detection, where blue-light illumination enhances buffy coat visibility while white-light provides overall structural context, refined cross-modal alignment at spatial token level becomes essential to leverage complementary information effectively while addressing potential geometric inconsistencies between imaging modalities.
