\subsection{Baseline Comparisons and Ablation Studies}

To systematically validate our architectural design choices and quantify the contribution of each component, we conducted comprehensive baseline comparisons and ablation experiments along four dimensions:

\textbf{Modality Contribution Analysis.} Single-modality YOLO11x-seg models trained exclusively on blue-light (id-blue) or white-light (id-white) images establish uni-modal performance upper bounds. Comparison against dual-modality variants quantifies the complementary information gain from cross-spectral fusion. These baselines directly address whether dual-illumination imaging provides measurable advantages over conventional single-source acquisition.

\textbf{Fusion Strategy Ablation.} We systematically compare three fusion mechanisms of increasing sophistication: (\textit{i}) \textit{Channel Concatenation} (concat-compress): direct channel-wise concatenation followed by 1$\times$1 convolutional compression, representing minimal inter-modal interaction; (\textit{ii}) \textit{Adaptive Weighted Fusion} (weighted-fusion): learnable spatial and global attention weights for modality-specific importance estimation; (\textit{iii}) \textit{Cross-Modal Attention} (crossattn): unidirectional token-level attention enabling explicit semantic correspondence modeling. An additional hyperparameter-optimized variant (crossattn-precise) targets DSR maximization through refined token size and neighborhood configurations. This progression isolates the incremental benefit of attention-based alignment over naive feature combination.

\textbf{Initialization Strategy Ablation.} Each fusion variant undergoes evaluation under three initialization regimes: (\textit{i}) \textit{scratch}: training from COCO weights without modality-specific pretraining; (\textit{ii}) \textit{pretrained}: leveraging Stage I/II transferred weights; (\textit{iii}) \textit{freeze\_backbone}: pretrained with backbone freezing during initial epochs. Comparing scratch versus pretrained variants quantifies the value of hierarchical transfer learning, while freeze\_backbone assesses the necessity of early-stage fusion stabilization. Note that certain fusion methods (particularly weighted-fusion) exhibit severe training instabilities under scratch initialization, failing to converge within allocated epochs.

\textbf{Attention Mechanism Design Choices.} For cross-modal attention variants, we ablate key architectural decisions: (\textit{a}) \textit{attention locality}: global versus localized spatial neighborhoods; (\textit{b}) \textit{query directionality}: unidirectional (blue$\rightarrow$white) versus bidirectional attention; (\textit{c}) \textit{token granularity}: varying token size $t \in \{2, 3, 4\}$ and neighborhood size $k \in \{3, 5, 7\}$ across pyramid levels. These ablations validate design choices motivated by computational efficiency and clinical detection requirements.

All experiments employ identical training protocols (optimizer, learning rate schedule, augmentation) and evaluation metrics (DSR, mAP, IoU) to ensure controlled comparisons. Hyperparameters were tuned via validation set grid search (3--5 configurations per method), with test evaluation conducted exclusively on optimal validation configurations.