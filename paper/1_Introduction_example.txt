**1. CAISeg: A Clustering-Aided Interactive Network for Lesion Segmentation in 3D Medical Imaging**

**Introduction**
Medical image segmentation stands as a pivotal task at the intersection of computer vision and medical research, aiming to accurately extract regions of interest from medical images. This provides a foundation for physicians in their detailed analysis and quantitative assessment of physiological structures and tissues. With advancements in deep learning and computer vision, fully automatic medical image segmentation, such as [1], [2], and [3], has achieved expert-level performance in many tasks. However, these high-performing models predominantly rely on fully supervised training strategies, which means they are heavily dependent on extensive, high-quality annotated datasets. Particularly in the task of lesion segmentation, the immense variability in lesion shapes and locations intensifies the challenge of creating a thorough dataset. Consequently, these automatic models might struggle when encountering lesion features not well-represented within their training datasets—essentially the tail-end features. Interactive segmentation is a promising avenue for tackling this challenge as it allows the segmentation network to capitalize on the doctor’s expertise to more effectively identify these tail features.

Interactive segmentation techniques enable users to convey prior knowledge to the network by drawing scribbles, outlining bounding boxes, or clicking on points. Considering the three-dimensional structure of most medical images in clinical settings, the method of interaction through point clicking stands out as the most straightforward and intuitive strategy. In prior research on point-based interactive segmentation of medical images, most approaches drew inspiration from interactive segmentation techniques in the realm of natural images. These methods initially encode user interaction points, employing methods like Euclidean distance encoding [4], disk encoding [5], [6], Gaussian distance encoding [7], geodesic distance encoding [8], [9], etc., and utilize the resulting encoded output as a cue map to guide the network in segmentation. However, the semantic information conveyed by these simple distance or gradient encodings tends to be shallow and simplistic. Therefore, when it comes to tail features in medical images, these methods often fail to make accurate predictions, even when precise interactive points and their associated label information are provided by physicians.

Fortunately, recent works [10], [11], [12], [13] that revisit Mask Transformers [14], [15] from the perspective of clustering have inspired us to develop a novel point-based interactive segmentation network, named **Clustering-Aided Interactive Segmentation Network (CAISeg)**. This framework advances interactive segmentation by introducing a two-stage process: cluster assignment and head feature refinement. Initially, we perform cluster assignment to extract semantic feature vectors that closely resemble user interaction points through clustering within the feature map. Following this, head feature refinement is applied to adjust interactive feature vectors along with those highly similar vectors within their clusters, enabling them to be mapped to the head distribution of the prompted label categories. This interaction-guided approach ensures that CAISeg retains its effectiveness when addressing tail features.

Previous work on point-based interactive segmentation commonly relied on loss functions tailored for general segmentation tasks. These include Binary Cross Entropy Loss (BCE Loss), Dice Loss [16] designed for sparse sample scenarios, and Focal Loss [17] aimed at hard cases. Yet, all of these loss functions were originally conceived for fully automatic segmentation, and they may not take into full account the context of user interactions. This oversight can prevent models from effectively emphasizing areas pointed out by users. In the realm of interactive segmentation, it’s crucial for the model to hone in on user-specified regions and achieve heightened accuracy therein.

To bridge this gap, we introduced **Focus Guided Loss (FG Loss)**. Distinct from conventional loss functions, FG Loss is intricately crafted to resonate with the specifics of user interactions. By applying a higher penalty to the regions where users click, it bolsters the model’s capacity to learn effectively from those interactions, ensuring a profound comprehension and response to such cues. Leveraging this approach, our network not only delineates user-designated structures with enhanced precision but also swiftly recalibrates in light of user feedback.

**We summarize our main contributions as follows:**

* To the best of our knowledge, we are the first to apply the concept of clustering to deep learning-based interactive segmentation networks. The clustering method is effective in aiding CAISeg to seize the regions of interest based on the interaction points. Moreover, aligning features within these regions with the head features of the prompted categories enables CAISeg to maintain a high recognition capability for tail features.
* We propose a novel loss function, Focus Guided Loss, a tailored loss function that emphasizes the predictive accuracy around the proximity of user interaction points. By assigning higher weights to voxels closer to these interaction points, we encourage CAISeg to focus more on these critical regions, enhancing its ability to deduce and prioritize user intent.
* We compared CAISeg with state-of-the-art segmentation methods on three distinct lesion segmentation tasks: brain tumors, colon cancer, and lung cancer. CAISeg exhibits superior segmentation quality compared to existing fully automated segmentation models and achieves more precise segmentation with fewer interaction points.

---

**2. MACTFusion**

Multimodal medical imaging plays a vital role in disease diagnosis and provides valuable assistance to medical practitioners. According to imaging mechanisms, medical images are usually divided into two categories, i.e., structural and functional information images [1]. The structural images have high spatial resolution and can depict anatomical structures of organs. For instance, Computed Tomography (CT) can display high-density structures like bones, and Magnetic Resonance Imaging (MRI) provides valuable information on soft tissues like adipose and muscular tissue [2]. However, CT images lack detailed information on structural organization, and MRI images lack high-contrast information. Functional images such as Positron Emission Tomography (PET) and Single Positron Emission Tomography (SPECT) can reflect the blood flow and metabolic activity of biomolecules. However, the spatial resolution of functional images is low, which limits the sharpness and detailed descriptions. The purpose of multimodal image fusion is to fuse complementary information from different images to enhance visual perception [3], [4]. Illustrations of CT/PET/SPECT images, MRI images, and fused results are shown in Fig. 1.

In recent years, various medical image fusion methods have been developed. These methods can be divided into two categories: classical methods and deep learning methods. The classical fusion methods include multiscale transform (MST) [5], [6], [7], pulse coupled neural network (PCNN) [8], [9], and sparse representation [10], [11]. For instance, shearlet [5], discrete wavelet [6], and low-rank representation [7] have been successfully applied in image fusion. Pulse coupled neural network is used to design fusion rules, but its performance is limited by parameter selection [8]. Sparse representation (SR) simulates the sparse coding principle of visual system in image fusion [10], but dictionary learning is time-consuming. Although these classical methods have demonstrated favorable performance, they still face drawbacks such as irreversible data loss and detail distortion [12]. In addition, the design of fusion rules is laborious and complex.

Recently, deep learning has been employed to address the aforementioned limitations [13]. These deep learning methods mainly include convolutional neural networks (CNNs) [14], generative adversarial networks (GANs) [20], [21], and Transformers [22], [23]. Generally, the fusion process consists of three parts, namely feature extraction, feature fusion strategy, and image reconstruction [14]. The key to the fusion performance lies in the first two parts [16]. Although deep learning methods have shown advantages and addressed some shortcomings of traditional algorithms, there are still several challenges that need to be addressed. In particular, CNN-based frameworks capture local features through convolution operations while ignoring long-range interactions, resulting in the loss of global contextual features.

The recent Transformers have overcome the limitations of global feature extraction. Initially used in natural language processing (NLP) [24], they have achieved excellent results in image processing such as image recognition [25] and image segmentation [26], [27]. Transformers have also promoted the development of image fusion [22], [23]. However, current Transformer-based fusion methods still have limitations. Firstly, most networks adopt simple fusion strategies such as concatenation or addition, resulting in low efficiency in utilizing multimodal image information. These networks usually mine intra-domain interactions but cannot fully mine and integrate the inter-domain information from multimodal images. Moreover, the problem of inconsistent data distribution in multimodal images has not been effectively solved, and direct cross-fusion operations may cause information interference and redundancy. In addition, Transformer-based methods are computationally expensive. Due to the computation of multi-head attention and self-attention, they require high computational cost and long running time. Therefore, it is necessary to explore the combination of local and global methods to enhance flexibility. Recently, MaxVit introduced a multi-axis attention mechanism, attempting to learn interactions between global and local spaces, and has achieved satisfactory results [28].

Inspired by the successful application of vision Transformers, in this work, we develop an adaptive cross-modality fusion strategy for unsupervised medical image fusion. The proposed cross-modality fusion Transformer consists of cross multi-axis attention, which includes cross-window attention and cross-grid attention. It aims to mine and integrate both local and global interactions of multimodal features at lower computational costs. Moreover, the cross Transformer is guided by a novel spatial adaptation fusion module that can adaptively remap feature distributions. This design allows it to learn rich textures and appropriate intensity information.

**The main contributions of this work are summarized as follows:**

* We propose an adaptive cross-modality fusion framework for unsupervised multimodal medical image fusion. It can effectively mine and integrate meaningful information, while improving fusion performance and reducing computational costs.
* We design a novel lightweight cross Transformer with a multi-axis attention mechanism. It includes cross-window attention and cross-grid attention, aiming to integrate local and global interactions of multimodal features.
* We introduce a novel spatial adaptation fusion module to adaptively remap feature distributions. It guides the cross Transformer to the most relevant information.
* We design a special feature extraction module that combines multiple residual dense gradient convolutional and Transformer layers to obtain local features from coarse to fine and effectively capture global features.

In Section II, we review the relevant literatures. Section III discusses the proposed framework, emphasizing the key design. Section IV presents extensive qualitative and quantitative experiments. In Section V, the conclusion and future research are discussed.