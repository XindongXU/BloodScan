{"custom_id": "request-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nMULTIMODAL INFORMATION INTERACTION FOR MEDICAL IMAGE SEGMENTATION.\nXinxin Fan1,2,†, Lin Liu1,2,†,Haoran Zhang1,\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China\n2University of Chinese Academy of Sciences, Beijing, China\nxx.fan@siat.ac.cn\nABSTRACT\nThe use of multimodal data in assisted diagnosis and segmen-\ntation has emerged as a prominent area of interest in current\nresearch. However, one of the primary challenges is how\nto effectively fuse multimodal features. Most of the current\napproaches focus on the integration of multimodal features\nwhile ignoring the correlation and consistency between dif-\nferent modal features, leading to the inclusion of potentially\nirrelevant information. To address this issue, we introduce an\ninnovative Multimodal Information Cross Transformer (Mic-\nFormer), which employs a dual-stream architecture to simul-\ntaneously extract features from each modality. Leveraging\nthe Cross Transformer, it queries features from one modal-\nity and retrieves corresponding responses from another, fa-\ncilitating effective communication between bimodal features.\nAdditionally, we incorporate a deformable Transformer ar-\nchitecture to expand the search space. We conducted exper-\niments on the MM-WHS dataset, and in the CT-MRI mul-\ntimodal image segmentation task, we successfully improved\nthe whole-heart segmentation DICE score to 85.57 and MIoU\nto 75.51. Compared to other multimodal segmentation tech-\nniques, our method outperforms by margins of 2.83 and 4.23,\nrespectively. This demonstrates the efficacy of MicFormer in\nintegrating relevant information between different modalities\nin multimodal tasks. These findings hold significant implica-\ntions for multimodal image tasks, and we believe that Mic-\nFormer possesses extensive potential for broader applications\nacross various domains. Access to our method is available at\nhttps://github.com/fxxJuses/MICFormer.\nIndex Terms —cardiac segmentation, multi modal, cross\nattention\n1. INTRODUCTION\nMultimodal medical image segmentation capitalizes on the\nintegration of information from diverse imaging modalities\nto achieve superior segmentation precision[1, 2, 3]. For ex-\nample, in the context of whole-heart segmentation, augment-\ning CT-based segmentation with MRI data can significantly\n*Corresponding author.\n†First Author and Second Author contribute equally to this work.\n(a)Hybrid region fusion accompanied\nby feature distortion(b)Cross-querying to preserve the original distribution of\nfeatureswhile enhancing unimodal feature representationSearch Feature\nTarget Feature\nMixed\nF\nFeature Extraction and Fusion\nSearch Feature Target Feature\nQuery Key\nCross\nTransformer\nSearch Feature Target FeatureFig. 1 . Limitations of the Current Method: (a) The prior-\nitization of image fusion using unimodal network form for\nmultimodal image segmentation can result in inaccurate fea-\nture representation in the target feature region. (b) Modal fea-\nture fusion is restricted to a two-stream cross-attention fusion\nnetwork. Query and Key matching is used to enhance the uni-\nmodal feature representation without incorporating additional\ninformation. Additionally, technical term abbreviations will\nbe explained when first used.\nimprove the accuracy of the CT segmentation[4, 5]. This\nimprovement can be attributed to the enhanced contrast and\nsuperior soft tissue imaging characteristics inherent in MRI.\nState-of-the-art multimodal segmentation networks[6, 7, 8]\nare designed to proficiently assimilate information from these\ndisparate modalities. They accomplish this by extracting\nsalient features from two modalities and subsequently merg-\ning the relevant features to produce segmentations of excep-\ntional precision.\nWhile current deep learning networks exhibit commend-\nable performance on multimodal segmentation tasks, a ma-\njority of the techniques still predominantly employ a single-\nstream approach for feature alignment and fusion. As de-\npicted in Figure 1, several methods (including TransUnet[9],\nSwin-Unet[10], and MedNeXt[11]) treat the multimodal im-\nage pairs as a unified image input. Consequently, these\nmethods place complete reliance on the network’s internal\nmechanisms, such as feature extraction, to execute feature\nfusion. This methodology fails to establish an explicit cor-\nrespondence between different modalities, inadvertently in-\ntroducing redundant information into the primary features.\nSuch perturbations pose challenges in refining these featuresarXiv:2404.16371v1  [cs.CV]  25 Apr 2024\n\nin subsequent feature extraction stages.\nTransformer and its derivatives[9, 10, 12, 13, 14] are more\nconcerned with self-attention and lack of correlation attention\nbetween image pairs. Furthermore, the Swin Transformer[15]\ndesign is constrained by its necessity to search within a prede-\ntermined corresponding window. Given that multimodal fea-\ntures often exhibit a greater degree of positional variability,\nSwin Transformer architectures may not be well-suited for ex-\ntensive cross-window searches. Thus, the development of an\nattention mechanism tailored for the matching of multimodal\nimage features could substantially enhance the efficiency of\nmultimodal segmentation tasks.\nIn this paper, we propose a novel multimodal transformer,\nMicFormer. For handling feature fusion and matching in\nmultimodal tasks. By leveraging a dual-stream model, Mic-\nFormer adeptly abstracts from both modalities. Central to its\nefficacy is the Deformable Cross Attention module, which\nadvances the querying and communication between the two\nmodal features. The architecture that emerges ensures a com-\npetent fusion of multi-level, multimodal semantic features,\nproviding significant advances in multimodal segmentation\naccuracy. In short, the pivotal contributions of our work\nare:1) We propose a novel two-stream multimodal feature\nfusion transformer backbone network. The multimodal fea-\ntures are continuously matched by using a deformable cross-\nattention fusion module.2) We propose a novel deformable\ncross-attention module for the automatic adaptation of the\nsearch space.\n2. METHOD\nAs depicted in Figure 2, the architecture of MicFormer is\nrooted in a Swin-Unet deep learning segmentation network,\nfurther enhanced by the incorporation of a parallel two-stream\narchitecture. MicFormer itself comprises a Transformer\narchitecture alongside a parallel sub-network featuring U-\nshaped feature extraction, thereby facilitating feature fusion\nand enabling a continuous module exchange between modal-\nities through the utilization of a deformable cross-attention\nmechanism. Within this framework, the Cross Transformer\nmodule employs deformable sampling to compute the struc-\ntural relationship between both modalities, thus reshaping\none modality’s structural information to align with the corre-\nsponding structures of both modalities within the same local\nwindow of the Swin Transformer. This alignment leads to an\nimproved calculation of similarity. The Cross Transformer\nmodule serves to uphold the original feature distribution of\nthe modalities and accentuates the pertinent segmentation\nfeatures while mitigating the influence of extraneous fea-\ntures by integrating both modalities. Specifically, it augments\nthe segmentation-relevant features while diminishing the\nsignificance of non-segmentation-related ones through their\nreciprocal interaction.2.1. U-shaped parallel feature network\nWe introduce a U-shaped network designed for concurrent\nfeature extraction, thereby facilitating the seamless integra-\ntion of information from two distinct modalities. As illus-\ntrated in Figure 2, the two parallel networks adhere to the ar-\nchitectural structure reminiscent of U-net, encompassing both\nencoding and decoding components. Notably, conventional\nconvolution operations are replaced with a composite block\ncomprising Swin Transformer and Cross Transformer, which\nare alternately applied. The Swin Transformer’s intrinsic ca-\npacity to capture both global and local information is har-\nnessed for feature extraction from each modality, followed\nby cross-modal feature querying in the Cross Transformer.\nThis query process serves to enhance the feature representa-\ntion within a single modality. MicFormer employs a parallel\ncommunication network to continually leverage information\nfrom the complementary modality, thereby augmenting the\nfeature expression capabilities within its own modality. Con-\nsequently, the resulting output features exhibit multimodal en-\nhancement.\n2.2. Cross Transformer block\nAs depicted in Figure 2(b), the features denoted as Feature a\nandFeature b, emanating from the parallel sub-network,\nengage in a bidirectional querying process within the Cross\nTransformer block. This process entails the exchange of se-\nquential inputs, thereby mutually enhancing their respective\nfeatures, which are subsequently reintegrated into the origi-\nnal input pipeline to undergo further rounds of deep feature\nextraction and interaction. Given the inherent limitations of\nthe receptive field, attaining a sufficient number of matches\nwithin a single instance proves unfeasible. Consequently,\nthe Cross Transformer block adopts a sequential execution\nstrategy involving two distinct communication phases. This\napproach ensures that our MicFormer not only facilitates sub-\nstantial unimodal feature extraction but also supports iterative\nfeature refinement through numerous cross-queries.\n2.3. Cross Attention\nThe primary objective of the cross-attention mechanism is\nto establish correlations between the two modalities by har-\nnessing the power of the attention mechanism. This is real-\nized by computing the correlation between the features within\nFeature aand those within Feature b. Please refer to Figure\n2 for a visual representation of this process. Specifically, we\nleverage the encoded vectors of Feature bas both Query and\nValue, while the encoded vectors of Feature aserve as Key.\nThe Query operation facilitates the mapping of correlations\nbetween individual points in Feature band the correspond-\ning points in Feature athrough correlation calculations with\nthe Key. Subsequently, we apply a Softmax calculation to\nthe correlation map, denoted as Attn , which assigns higher\n\nPatch Partition\nLinear EmbeddingPatch Partition\nLinear Embedding\nSwin Transformer\nBlock x2Swin Transformer\nBlock x2\nCross Transformer\nBlock x2Cross Transformer\nBlock x2\nPatch Merging Patch Merging\nSwin Transformer\nBlock x2Swin Transformer\nBlock x2\nCross Transformer\nBlock x2Cross Transformer\nBlock x2\nPatch Merging Patch Merging\nSwin Transformer\nBlock x2Swin Transformer\nBlock x2\nCross Transformer\nBlock x2Cross Transformer\nBlock x2Swin Transformer\nBlock x2Swin Transformer\nBlock x2Cross Transformer\nBlock x2Cross Transformer\nBlock x2\nPatch Expanding Patch ExpandingSwin Transformer\nBlock x2Swin Transformer\nBlock x2Cross Transformer\nBlock x2Cross Transformer\nBlock x2Swin Transformer\nBlock x2Swin Transformer\nBlock x2Cross Transformer\nBlock x2Cross Transformer\nBlock x2\nPatch Expanding Patch Expanding\nPatch Merging Patch Merging\nSwin Transformer\nBlock x1Swin Transformer\nBlock x1Patch Expanding Patch Expanding+Segmentation Head\nSkip Connection\nSkip Connection\nSkip Connection\nCT MRI\nencoder decoder\nconcat\nDepth-wise\nConvolutionoffset\nReference Point\n+\nWindow\nparition \nWindow\nparition Resample\nLinear Network\nLinear Network\noffset....\n....\n(a)MicFormer Network(b)Cross T ransformer Block\n(c)Deformable  Operator W-MCA+ Linear Network MLP +Fig. 2 . Our MicFormer architecture, which consists of (a) U-shaped parallel feature network.(b) Cross Transformer\nBlock.(c)Deformable Operator. We utilize deep separable convolution to split Feature aandFeature bin the channel di-\nrection and compute the positional differences at the corresponding positions.\nweights to features that may be challenging to capture within\nFeature bbut are more readily discernible in Feature a. As\nan example, in the context of CT features, the delineation of\ntissue edges can often be indistinct and challenging, whereas\nMRI features tend to offer clearer distinctions in tissue edges.\nConsequently, when calculating the similarity between CT\nfeatures and MRI tissue edges, higher similarity values are\nallocated to the edge regions of CT features. This strategic\nemphasis enables the model to devote more attention to the\noriginal, less distinct tissue edges within CT scans.\nUltimately, the Key is subjected to a matrix multiplica-\ntion with the Softmax correlation mapping Attn , ensuring\nthat features within Feature athat are more challenging to\ndiscern within Feature breceive heightened focus and atten-\ntion from the model. Our computation of Cross Attention is\ndetailed as follows:\nW−MCA =Softmax (QbKT\na√\nd)Vb (1)\nWhere Qb,KaandVbare Query, Key, Value matrices, re-\nspectively, QbandVbcome from the encoding of Feature b\nandkacomes from the encoding of Feature a.where drepre-\nsents the feature dimension of the Key.\n2.4. Deformable Operator\nGiven that the Swin Transformer relies on a fixed window\nshape and requires a broader and scattered exploration for\ncross-modal feature querying, we choose to avoid compu-\ntationally intensive global searches similar to those in theVision Transformer model[16]. Instead, we introduce a de-\nformable operation, as shown in Figure 2, which uses deep\nconvolution to establish relationships between voxel positions\nin the local neighborhood. This operation calculates a unified\ndeformation trend called offset , which dynamically adjusts\nto accommodate all features. The offset is a tensor with\nthree channels, representing the displacement of a point on\nthe feature map in the x,y, andzdirections. Importantly, the\nshape of the offset tensor match those of the feature map.\nIn the final step, we resample Feature ato obtain the de-\nformed feature map, adjusting its position accordingly. This\nprocess is similar to applying the same deformable opera-\ntion to the original queried window. By introducing the De-\nformable Operator, we effectively transform the Swin Trans-\nformer with a fixed receptive field into an equivalent model\nwith a deformable receptive field.\n3. EXPERIMENTS AND RESULTS\n3.1. Dataset and Evaluation Metrics\nThe MMWHS dataset[17] consists of 20 cardiac MRI sam-\nples with expertannotations for seven structures: left and\nright ventricles, left and right atrium, pulmonary artery, my-\nocardium, and aorta. pulmonary artery, myocardium, and\naorta. In this experiment, we used the SyN algorithm[18] CT-\nMRI image pairs for alignment and cropped the correspond-\ning ROI regions. We randomly cut the entire dataset, with 16\npairs in the training set and 4 cases in the test set.Moreover,\nthe Dice similarity coefficient (Dice), mean intersection over\n\nunion (MIoU), and 95% Hausdorff distance (HD95) are em-\nployed to evaluate the model performance.\n3.2. Implementation Details\nWe use Pytorch to train our MicFormer on NVIDIA RTX\n2080 GPU. During the training process, we optimize Mic-\nFormer using Adam with a learning rate of 0.0001. The batch\nsize is set as 1, and the maximum epoch is 1000.\n3.3. Compare with the State-of-the-art Methods\nTable 1 . Comparison with different network architectures\nMethod Single Modal-/Multi Modal- Dice ↑MIoU ↑HD95 ↓\nnnFormer[14] Single Modal 73.67 59.73 29.12\nVT-Unet [12] Multi Modal 77.67 64.36 14.18\nSwin-Unet [10] - 80.22 67.59 26.92\nSwinUneter [13] - 81.49 69.55 14.64\nnnFormer [14] - 82.33 70.57 19.04\nMedNeXt [11] - 82.74 71.48 10.10\nMicFormer(ours) - 85.57 75.71 10.35\nCT\nGround T ruth\nMicFormer\n（ours ）\nVT-Unet\nSwin-Unet\nSwinUneter\nnnformer\nMedNeXt\nMRI\nFig. 3 . Qualitative Results Analysis: The first row shows\nMRI image slices, followed by CT image slices in the sec-\nond row. The third row displays CT’s Ground Truth label.\nThe next six rows exhibit the qualitative segmentation results\nof VT-UNet, Swin-Unet, SwinUneter, nnformer, MedNeXt,\nand MicFormer.We conducted a comprehensive comparison between Mic-\nFormer and five state-of-the-art multimodal segmentation al-\ngorithms, namely VT-Unet[12], Swin-Unet[10], SwinUneter\n[13], nnFormer[14], and MedNeXt[11], with the specific re-\nsults presented in Table 1 and Figure 3. According to the\ndata presented in Table 1, MicFormer surpasses all other\nalgorithms in terms of both the Dice coefficient and MIoU.\nNotably, our results exhibit an improvement of 2.83 and 4.23\npoints over MedNeXt, respectively, underscoring the superior\nperformance of our segmentation model. However, it is worth\nmentioning that our method slightly lags behind MedNeXt in\nterms of the HD95 metric. This discrepancy can be attributed\nto MedNeXt’s utilization of the ConvNeXt architecture[19],\nwhich exhibits a more robust inductive bias for small datasets\nand greater sensitivity to boundary information compared\nto the Transformer architecture. Nevertheless, our method\noutperforms all other algorithms, with the exception of Med-\nNeXt, on the HD95 metric. This study substantiates that Mic-\nFormer excels in capturing segmented surface information\nacross both modalities when compared to the same Trans-\nformer architecture. Additionally, our experiments involving\nSingle Modal and Multi Modal scenarios, utilizing nnFormer\non the dataset, provide further evidence that the incorporation\nof MRI data into the CT segmentation process significantly\nenhances the accuracy of CT segmentation.\nAs demonstrated in Figure 3, the performance of various\nmethods on the test set is visualized. Our MicFormer exhibits\nsuperior discrimination at tissue edges and tissue-tissue junc-\ntions. In the connected domain of the segmentation graph, our\nmethod does not display discrete points like VT-Unet, Swin-\nUnet, SwinUneter and nnFormer.\n4. CONCLUSION\nIn this study, we introduce a novel dual-stream cross-network,\nMicFormer, composed of three main components: a U-shaped\nparallel feature network, Swin Transformer, and Cross Trans-\nformer. The U-shaped parallel feature network prevents\nuncontrolled fusion by isolating the two modalities. The\nSwin Transformer captures both global and local information\nfor unimodal feature extraction, while the Cross Transformer\nemploys a deformable cross-attention mechanism to regu-\nlate the interaction between the two modalities and enhance\nthe unimodal feature representation. Finally, on the MM-\nWHS dataset, our approach significantly outperformed other\nalgorithms.\n5. REFERENCES\n[1] Hong Liu, Yuzhou Zhuang, Enmin Song, Xiangyang\nXu, Guangzhi Ma, Coskun Cetinkaya, and Chih-Cheng\nHung, “A modality-collaborative convolution and trans-\nformer hybrid network for unpaired multi-modal med-\n\nical image segmentation with limited annotations.,”\nMedical physics , 2023.\n[2] Jie Yang, Ruimao Zhang, Chao Wang, Zhuguo Li,\nXiang Wan, and Lingyan Zhang, “Toward unpaired\nmulti-modal medical image segmentation via learn-\ning structured semantic consistency,” ArXiv , vol.\nabs/2206.10571, 2022.\n[3] José Dolz, Karthik Gopinath, Jing Yuan, H. Lom-\nbaert, Christian Desrosiers, and Ismail ben Ayed,\n“Hyperdense-net: A hyper-densely connected cnn for\nmulti-modal image segmentation,” IEEE Transactions\non Medical Imaging , vol. 38, pp. 1116–1126, 2018.\n[4] Hengfei Cui, Y . Wang, Y . Li, Di Xu, Lei Jiang, Yong\nXia, and Yanning Zhang, “An improved combination\nof faster r-cnn and u-net network for accurate multi-\nmodality whole heart segmentation,” IEEE Journal of\nBiomedical and Health Informatics , vol. 27, pp. 3408–\n3419, 2023.\n[5] Zhanwei Xu, Ziyi Wu, and Jianjiang Feng, “Cfun: Com-\nbining faster r-cnn and u-net network for efficient whole\nheart segmentation,” ArXiv , vol. abs/1812.04914, 2018.\n[6] Jianwei Lin, Jiatai Lin, Chenghao Lu, Hao Chen,\nHuan Lin, Bingchao Zhao, Zhenwei Shi, Bingjiang\nQiu, Xipeng Pan, Zeyan Xu, Biao Huang, Changhong\nLiang, Guoqiang Han, Zaiyi Liu, and Chu Han, “Ckd-\ntransbts: Clinical knowledge-driven hybrid transformer\nwith modality-correlated cross-attention for brain tumor\nsegmentation,” IEEE Transactions on Medical Imaging ,\nvol. 42, pp. 2451–2461, 2022.\n[7] Hengyi Yang, Tao Zhou, Yi Zhou, Yizhe Zhang, and\nH. Fu, “Flexible fusion network for multi-modal brain\ntumor segmentation,” IEEE Journal of Biomedical and\nHealth Informatics , vol. 27, pp. 3349–3359, 2023.\n[8] Shun Chen, Ang Li, Jianxin Chen, Xuguang Zhang,\nChong Jiang, and Jingyan Xu, “Hybrid attention fu-\nsion segmentation network for diffuse large b-cell lym-\nphoma in pet-ct,” in 2022 14th International Confer-\nence on Wireless Communications and Signal Process-\ning (WCSP) , 2022, pp. 72–76.\n[9] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo,\nEhsan Adeli, Yan Wang, Le Lu, Alan Loddon Yuille,\nand Yuyin Zhou, “Transunet: Transformers make strong\nencoders for medical image segmentation,” ArXiv , vol.\nabs/2102.04306, 2021.\n[10] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi-\naopeng Zhang, Qi Tian, and Manning Wang, “Swin-\nunet: Unet-like pure transformer for medical image seg-\nmentation,” in ECCV Workshops , 2021.[11] Saikat Roy, Gregor Koehler, Constantin Ulrich, Michael\nBaumgartner, Jens Petersen, Fabian Isensee, Paul F.\nJaeger, and Klaus H. Maier-Hein, “Mednext:\nTransformer-driven scaling of convnets for medical im-\nage segmentation,” ArXiv , vol. abs/2303.09975, 2023.\n[12] Himashi Peiris, Munawar Hayat, Zhaolin Chen, Gary\nEgan, and Mehrtash Harandi, “A robust volumetric\ntransformer for accurate 3d tumor segmentation,” pp.\n162–172, 2022.\n[13] Ali Hatamizadeh, V . Nath, Yucheng Tang, Dong Yang,\nHolger R. Roth, and Daguang Xu, “Swin unetr: Swin\ntransformers for semantic segmentation of brain tumors\nin mri images,” in BrainLes@MICCAI , 2022.\n[14] Hong-Yu Zhou, J. Guo, Yinghao Zhang, Lequan Yu,\nLiansheng Wang, and Yizhou Yu, “nnformer: Inter-\nleaved transformer for volumetric segmentation,” ArXiv ,\nvol. abs/2109.03201, 2021.\n[15] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo, “Swin\ntransformer: Hierarchical vision transformer using\nshifted windows,” 2021 IEEE/CVF International Con-\nference on Computer Vision (ICCV) , pp. 9992–10002,\n2021.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” ArXiv ,\nvol. abs/2010.11929, 2020.\n[17] Xiahai Zhuang, “Multivariate mixture model for my-\nocardial segmentation combining multi-source images,”\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence , vol. 41, pp. 2933–2946, 2016.\n[18] Brian B. Avants, N. Tustison, and Hans J. Johnson, “Ad-\nvanced normalization tools (ants),” 2020.\n[19] Zhuang Liu, Hanzi Mao, Chaozheng Wu, Christoph Fe-\nichtenhofer, Trevor Darrell, and Saining Xie, “A con-\nvnet for the 2020s,” 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , pp.\n11966–11976, 2022.\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nSAM 2: Segment Anything in Images and Videos\nNikhila Ravi∗,†,Valentin Gabeur∗,Yuan-Ting Hu∗,Ronghang Hu∗,Chaitanya Ryali∗,Tengyu Ma∗,\nHaitham Khedr∗,Roman Rädle∗,Chloe Rolland ,Laura Gustafson ,Eric Mintun ,Junting Pan ,Kalyan\nVasudev Alwala ,Nicolas Carion ,Chao-Yuan Wu ,Ross Girshick ,Piotr Dollár†,Christoph Feichtenhofer∗,†\nMeta FAIR\n∗core contributor ,†project lead\nWe present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable\nvisual segmentation in images and videos. We build a data engine, which improves model and data\nvia user interaction, to collect the largest video segmentation dataset to date. Our model is a simple\ntransformer architecture with streaming memory for real-time video processing. SAM 2 trained on\nour data provides strong performance across a wide range of tasks. In video segmentation, we observe\nbetter accuracy, using 3 ×fewer interactions than prior approaches. In image segmentation, our model\nis more accurate and 6 ×faster than the Segment Anything Model (SAM). We believe that our data,\nmodel, and insights will serve as a significant milestone for video segmentation and related perception\ntasks. We are releasing a version of our model, the dataset and an interactive demo.\nDemo: https://sam2.metademolab.com\nCode: https://github.com/facebookresearch/segment-anything-2\nWebsite: https://ai.meta.com/sam2\n1 Introduction\nSegment Anything (SA) introduced a foundation model for promptable segmentation inimages(Kirillov et al.,\n2023). However an image is only a static snapshot of the real world in which visual segments can exhibit\ncomplex motion, and with the rapid growth of multimedia content, a significant portion is now recorded\nwith a temporal dimension, particularly in videodata. Many important applications in AR/VR, robotics,\nautonomous vehicles, and video editing require temporal localization beyond image-level segmentation. We\nbelieve a universal visual segmentation system should be applicable to both images andvideos.\nSegmentation in video aims to determine the spatio-temporal extent of entities, which presents unique\nchallenges beyond those in images. Entities can undergo significant changes in appearance due to motion,\ndeformation, occlusion, lighting changes, and other factors. Videos often have lower quality than images due\nto camera motion, blur, and lower resolution. Further, efficient processing of a large number of frames is a\nkey challenge. While SA successfully addresses segmentation in images, existing video segmentation models\nand datasets fall short in providing a comparable capability to “segment anything in videos”.\nWe introduce the Segment Anything Model 2 (SAM 2), a unifiedmodel for video and image segmentation (we\nconsider an image as a single-frame video). Our work includes a task, model, and dataset (see Fig. 1).\nWe focus on the Promptable Visual Segmentation (PVS) taskthat generalizes image segmentation to the\nvideo domain. The task takes as input points, boxes, or masks on any frame of the video to define a segment of\ninterest for which the spatio-temporal mask (i.e., a ‘ masklet’) is to be predicted. Once a masklet is predicted,\nit can be iteratively refined by providing prompts in additional frames.\nOurmodel(§4) produces segmentation masks of the object of interest, in single images andacross video\nframes. SAM 2 is equipped with a memory that stores information about the object and previous interactions,\nwhich allows it to generate masklet predictions throughout the video, and also effectively correct these based\non the stored memory context of the object from previously observed frames. Our streaming architecture is a\nnatural generalization of SAM to the video domain, processing video frames one at a time, equipped with a\nmemory attention module to attend to the previous memories of the target object. When applied to images,\nthe memory is empty and the model behaves like SAM.\n1\n\nFigure 1We introduce the Segment Anything Model 2 (SAM 2), towards solving the promptable visual segmentation\ntask (a) with our foundation model (b), trained on our large-scale SA-V dataset collected through our data engine (c).\nSAM 2 is capable of interactively segmenting regions through prompts (clicks, boxes, or masks) on one or multiple\nvideo frames by utilizing a streaming memory that stores previous prompts and predictions.\nWe employ a data engine (§5) to generate training data by using our model in the loop with annotators to\ninteractively annotate new and challenging data. Different from most existing video segmentation datasets,\nour data engine is not restricted to objects of specific categories, but instead targeted to provide training\ndata for segmenting anyobject with a valid boundary, including parts and subparts. Compared to existing\nmodel-assisted approaches, our data engine with SAM 2 in the loop is 8.4 ×faster at comparable quality. Our\nfinal Segment Anything Video (SA-V) dataset (§5.2) consists of 35.5M masks across 50.9K videos, 53 ×more\nmasks than any existing video segmentation dataset. SA-V is challenging with small objects and parts that\nget occluded and re-appear throughout the video. Our SA-V dataset is geographically diverse, and a fairness\nevaluation of SAM 2 indicates minimal performance discrepancy in video segmentation based on perceived\ngender, and little variance among the three perceived age groups we evaluated.\nOur experiments (§6) show that SAM 2 delivers a step-change in the videosegmentation experience. SAM 2\ncan produce bettersegmentation accuracy while using 3 ×fewerinteractions than prior approaches. Further,\nSAM 2 outperforms prior work in established videoobject segmentation benchmarks, under multiple evaluation\nsettings, anddelivers better performance compared to SAM on imagesegmentation benchmarks, while being\n6×faster. SAM 2 is shown to be effective across a variety of video and image distributions as observed through\nnumerous zero-shot benchmarks including 17 for video segmentation and 37 for single-image segmentation.\nWe are releasing our work under permissive open licences, including the SA-V dataset (CC by 4.0) a version of\nthe model SAM 2 (Apache 2.0), along with an interactive online demo at https://sam2.metademolab.com .\n2 Related work\nImage segmentation. Segment Anything (Kirillov et al., 2023) introduces a promptable image segmentation\ntask where the goal is to output a valid segmentation mask given an input prompt such as a bounding box\nor a point that refers to the object of interest. SAM trained on the SA-1B dataset allows for zero-shot\nsegmentation with flexible prompting, which enabled its adoption to a wide range of downstream applications.\nRecent work has extended SAM by improving its quality. For example, HQ-SAM (Ke et al., 2024) enhances\nSAM by introducing a High-Quality output token and training the model on fine-grained masks. Another\nline of work focuses on SAM’s efficiency to enable wider use in real-world and mobile applications, such as\nEfficientSAM (Xiong et al., 2023), MobileSAM (Zhang et al., 2023a), and FastSAM (Zhao et al., 2023). The\nsuccess of SAM led to its adoption in a wide range of applications, such as medical imaging (Ma et al., 2024;\nDeng et al., 2023; Mazurowski et al., 2023; Wu et al., 2023a), remote sensing (Chen et al., 2024; Ren et al.,\n2024), motion segmentation (Xie et al., 2024), and camouflaged object detection (Tang et al., 2023).\n2\n\nInteractive Video Object Segmentation (iVOS). Interactive video object segmentation has emerged as a crucial\ntask to efficiently obtain object segmentations in videos (masklets) with user guidance, often in the form of\nscribbles, clicks, or bounding boxes. A few early approaches (Wang et al., 2005; Bai & Sapiro, 2007; Fan\net al., 2015) deploy graph-based optimization to guide the segmentation annotation process. More recent\napproaches (Heo et al., 2020; Cheng et al., 2021b; Delatolas et al., 2024) often adopt a modular design,\nconverting the user inputs into a mask representation on a single frame and then propagating it to other\nframes. Our work shares a similar goal to these works to segment objects across videos with a good interactive\nexperience, and we build a strong model along with a large and diverse dataset in pursuit of this goal.\nIn particular, the DAVIS interactive benchmark (Caelles et al., 2018) allows interactively segmenting an\nobject via scribble inputs on multiple frames. Inspired by the DAVIS interactive benchmark, we also adopt an\ninteractive evaluation setting for the promptable video segmentation task in §6.1.\nClick-based input is easier to collect (Homayounfar et al., 2021) for interactive video segmentation. Recent\nworks have used a combination of SAM on images with video trackers based on masks (Cheng et al., 2023b;\nYang et al., 2023; Cheng et al., 2023c) or points (Rajič et al., 2023). However, these approaches have\nlimitations: the tracker may not work for all objects, SAM may not perform well for image frames from videos,\nand there is no mechanism to interactively refine a model’s mistakes, other than re-annotating using SAM\nfrom scratch on the erroneous frame and restarting the tracking from there.\nSemi-supervised Video Object Segmentation (VOS). Semi-supervised VOS usually begins with an object mask as\ninput in the first frame, which must be accurately tracked throughout the video (Pont-Tuset et al., 2017). It\nis called “semi-supervised” since the input mask can be seen as a supervision signal of the object appearance\nthat is available only for the first frame. This task has drawn significant attention due to its relevance in\nvarious applications, including video editing, robotics, and automatic background removal.\nEarly neural network-based approaches have often used online fine-tuning on the first video frame (Caelles\net al., 2016; Perazzi et al., 2016; Yoon et al., 2017; Maninis et al., 2017; Hu et al., 2018a; Bhat et al., 2020;\nRobinson et al., 2020) or on all frames (Voigtlaender & Leibe, 2017) to adapt the model to the target object.\nFaster inference has been achieved with offline-trained models, conditioned either only on the first frame (Hu\net al., 2018b; Chen et al., 2018), or also integrating the previous frame (Oh et al., 2018; Yang et al., 2018, 2020).\nThis multi-conditioning has been extended to all frames with RNNs (Xu et al., 2018a) and cross-attention (Oh\net al., 2019; Cheng et al., 2021a; Li et al., 2022a; Yang et al., 2021b, 2024; Cheng & Schwing, 2022; Yang\n& Yang, 2022; Wang et al., 2022; Cheng et al., 2023a; Goyal et al., 2023). Recent approaches (Zhang et al.,\n2023b; Wu et al., 2023b) extend a single vision transformer to jointly process the current frame along with all\nprevious frames and associated predictions, resulting in a simple architecture but at a prohibitive inference\ncost. Semi-supervised VOS can be seen as a special case of our Promptable Visual Segmentation (PVS) task,\nas it is equivalent to only providing a mask prompt in the first video frame. Nevertheless, annotating the\nrequired high-quality object mask in the first frame is practically challenging and time-consuming.\nVideo segmentation datasets. Many datasets have been proposed to support the VOS task. Early VOS\ndatasets (Prest et al., 2012; Li et al., 2013; Ochs et al., 2014; Fan et al., 2015), such as DAVIS (Pont-Tuset\net al., 2017; Caelles et al., 2019), include high-quality annotations but their limited size does not allow training\ndeep-learning based approaches. Covering 94 object categories over 4 thousand videos, YouTube-VOS (Xu\net al., 2018b) is the first large-scale dataset for the VOS task. As algorithms became better and benchmark\nperformance started to saturate, researchers have looked at increasing the difficulty of the VOS task by\nspecifically focusing on occlusions (Qi et al., 2022; Ding et al., 2023), long videos (Hong et al., 2023, 2024),\nextreme transformations (Tokmakov et al., 2022), object diversity (Wang et al., 2021b, 2023) or scene\ndiversity (Athar et al., 2022).\nWefindthatcurrentvideosegmentationdatasetslacksufficientcoveragetoachievethecapabilityof“segmenting\nanything in videos”. Their annotations typically cover entire objects (not parts) and datasets are often centered\naround specific object classes, such as people, vehicles, and animals. In comparison to these datasets, our\nreleased SA-V dataset not only focuses on whole objects but also extensively covers object parts and contains\nover an order of magnitude more masks.\n3\n\nf r a m e  1s t e p  1s t e p  2f r a m e  2f r a m e  3f r a m e  4\nFigure 2 Interactive segmentation with SAM 2. Step 1 (selection): we prompt SAM 2 in frame 1 to obtain the segment\nof the target object (the tongue). Green/red dots indicate positive/negative prompts respectively. SAM 2 automatically\npropagates the segment to the following frames (blue arrows) to form a masklet. If SAM 2 loses the object (after frame\n2), we can correct the masklet by providing an additional prompt in a new frame (red arrow). Step 2 (refinement): a\nsingle click in frame 3 is sufficient to recover the object and propagate it to obtain the correct masklet. A decoupled\nSAM + video tracker approach would require several clicks in frame 3 (as in frame 1) to correctly re-annotate the\nobject as the segmentation is restarted from scratch. With SAM 2’s memory, a single click can recover the tongue.\n3 Task: promptable visual segmentation\nThePVStaskallowsprovidingpromptstothemodelon anyframeofavideo. Promptscanbepositive/negative\nclicks, bounding boxes, or masks, either to define an object to segment or to refine a model-predicted one. To\nprovide an interactive experience, upon receiving a prompt on a specific frame, the model should immediately\nrespond with a valid segmentation mask of the object on this frame. After receiving initial (one or multiple)\nprompts (either on the same frame or different frames), the model should propagate these prompts to obtain\nthe masklet of the object across the entire video , which contains the segmentation mask of the target object\non every video frame. Additional prompts can be provided to the model on anyframe to refine the segment\nthroughout the video (example in Fig. 2). For details on the task, see §A.\nSAM 2, introduced in the next section (§4), is applied as a data collection tool to the PVS task for building our\nSA-V dataset (§5). The model is evaluated (§6) in an online and offline setting by simulating interactive video\nsegmentation scenarios involving annotations across multiple frames, in the conventional semi-supervised VOS\nsetting where annotations are limited to the first frame, and for image segmentation on the SA benchmarks.\n4 Model\nOur model can be seen as a generalization of SAM to the video (and image) domain. SAM 2 (Fig. 3) supports\npoint, box, and mask prompts on individual frames to define the spatial extent of the object to be segmented\nacross the video. For image input, the model behaves similarly to SAM. A promptable and light-weight mask\ndecoder accepts a frame embedding and prompts (if any) on the current frame and outputs a segmentation\nmask for the frame. Prompts can be iteratively added on a frame in order to refinethe masks.\nUnlike SAM, the frame embedding used by the SAM 2 decoder is not directly from an image encoder and is\ninsteadconditioned onmemories of past predictions and prompted frames . It is possible for prompted frames\nto also come “from the future” relative to the current frame. Memories of frames are created by the memory\nencoder based on the current prediction and placed in a memory bank for use in subsequent frames. The\nmemory attention operation takes the per-frame embedding from the image encoder and conditions it on the\nmemory bank to produce an embedding that is then passed to the mask decoder.\nWe describe individual components and training below and provide more details in Appendix C.\n4\n\nPrompt encoderMasktimePointsBoxmemory\n attentionmemory\nbankmask decoder\nimage\nencodermemory\nencoderFigure 3 The SAM 2 architecture. For a given frame, the segmentation prediction is conditioned on the current prompt\nand/oron previously observed memories. Videos are processed in a streaming fashion with frames being consumed one\nat a time by the image encoder, and cross-attended to memories of the target object from previous frames. The mask\ndecoder, which optionally also takes input prompts, predicts the segmentation mask for that frame. Finally, a memory\nencoder transforms the prediction and image encoder embeddings (not shown in the figure) for use in future frames.\nImage encoder. For real-time processing of arbitrarily long videos, we take a streaming approach, consuming\nvideo frames as they become available. The image encoder is only run oncefor the entire interaction and its\nrole is to provide unconditioned tokens (feature embeddings) representing each frame. We use an MAE (He\net al., 2022) pre-trained Hiera (Ryali et al., 2023; Bolya et al., 2023) image encoder, which is hierarchical ,\nallowing us to use multiscale features during decoding.\nMemory attention. The role of memory attention is to condition the current frame features on the past\nframes features and predictions as well as on any new prompts. We stack Ltransformer blocks, the first one\ntaking the image encoding from the current frame as input. Each block performs self-attention, followed by\ncross-attention to memories of (prompted/unprompted) frames and object pointers (see below), stored in a\nmemory bank (see below), followed by an MLP. We use vanillaattention operations for self- and cross-attention,\nallowing us to benefit from recent developments in efficient attention kernels (Dao, 2023).\nPrompt encoder and mask decoder. Our prompt encoder is identical to SAM’s and can be prompted by clicks\n(positive or negative), bounding boxes, or masks to define the extent of the object in a given frame. Sparse\nprompts are represented by positional encodings summed with learned embeddings for each prompt type,\nwhile masks are embedded using convolutions and summed with the frame embedding.\nOur decoder design largely follows SAM. We stack “two-way” transformer blocks that update prompt and\nframe embeddings. As in SAM, for ambiguous prompts (i.e., a single click) where there may be multiple\ncompatible target masks, we predict multiple masks. This design is important to ensure that the model\noutputsvalidmasks. In video, where ambiguity can extend acrossvideo frames, the model predicts multiple\nmasks on each frame. If no follow-up prompts resolve the ambiguity, the model only propagates the mask\nwith the highest predicted IoU for the current frame.\nUnlike SAM where there is alwaysa valid object to segment given a positive prompt, in the PVS task it is\npossible for novalid object to exist on some frames (e.g. due to occlusion). To account for this new output\nmode, we add an additional head that predicts whether the object of interest is present on the current frame.\nAnother difference from SAM is that we use skip connections from the hierarchical image encoder (bypassing\nthe memory attention) to incorporate high-resolution information for mask decoding (see §C).\nMemory encoder. The memory encoder generates a memory by downsampling the output mask using\na convolutional module and summing it element-wise with the unconditioned frame embedding from the\nimage-encoder (not shown in Fig. 3), followed by light-weight convolutional layers to fuse the information.\nMemory bank. The memory bank retains information about past predictions for the target object in the video\nby maintaining a FIFO queue of memories of up to Nrecent frames and stores information from prompts\nin a FIFO queue of up to Mprompted frames. For instance, in the VOS task where the initial mask is the\nonly prompt, the memory bank consistently retains the first frame’s memory along with memories of up to N\nrecent (unprompted) frames. Both sets of memories are stored as spatialfeature maps.\nIn addition to the spatial memory, we store a list of object pointers as lightweight vectors for high-level\nsemantic information of the object to segment, based on mask decoder output tokens of each frame (Meinhardt\net al., 2022). Our memory attention cross-attends to both spatial memory features and these object pointers.\n5\n\nWe embed temporal position information into the memories of Nrecent frames, allowing the model to represent\nshort-term object motion, but not into those of prompted frames, because the training signal from prompted\nframes is sparser and it is more difficult to generalize to the inference setting where prompted frames may\ncome from a very different temporal range than seen during training.\nTraining. The model is trained jointlyon image and video data. Similar to previous work (Kirillov et al., 2023;\nSofiiuk et al., 2022), we simulate interactive prompting of the model. We sample sequences of 8 frames and\nrandomly select up to 2 frames to prompt and probabilistically receive corrective clicks which are sampled\nusing the ground-truth masklet and model predictions during training. The training task is to sequentially\n(and “interactively”) predict the ground-truth masklet. Initial prompts to the model can be the ground-truth\nmask with probability 0.5, a positive click sampled from the ground-truth mask with probability 0.25, or a\nbounding box input with probability 0.25. See §C for more details.\n5 Data\nTo develop the capability to “segment anything” in video, we built a data engine to collect a large and diverse\nvideo segmentation dataset. We employ an interactive model in the loop setup with human annotators.\nSimilar to Kirillov et al. (2023), we do not impose semantic constraints on the annotated masklets, and focus\non both whole objects (e.g., a person) and parts (e.g., a person’s hat). Our data engine went through three\nphases, each categorized based on the level of model assistance provided to annotators. Next, we describe\neach data engine phase and our SA-V dataset.\n5.1 Data engine\nPhase 1: SAM per frame. The initial phase used the image-based interactive SAM (Kirillov et al., 2023) to assist\nhuman annotation. Annotators are tasked with annotating the mask of a target object in every frame of the\nvideo at 6 frames per second (FPS) using SAM, and pixel-precise manual editing tools such as a “brush” and\n“eraser”. There is no tracking model involved to assist with the temporal propagation of masks to other frames.\nAs this is a per-frame method, and all frames require mask annotation from scratch, the process is slow, with\nan average annotation time of 37.8 seconds per frame in our experiment. However, this yields high-quality\nspatialannotations per frame. In this phase, we collected 16K masklets across 1.4K videos. We further use\nthis approach to annotate our SA-V val and test sets to mitigate potential biases of SAM 2 during evaluation.\nPhase 2: SAM + SAM 2 Mask. The second phase added SAM 2 into the loop, where SAM 2 only accepted masks\nas prompts. We refer to this version as SAM 2 Mask. Annotators used SAM and other tools as in Phase 1 to\ngenerate spatialmasks in the first frame, and then use SAM 2 Mask to temporally propagate the annotated\nmask to other frames to get the full spatio-temporal masklets. At any subsequent video frame, annotators\ncanspatially modify the predictions made by SAM 2 Mask by annotating a mask from scratch with SAM,\na “brush” and/or “eraser”, and re-propagate with SAM 2 Mask, repeating this process until the masklet is\ncorrect. SAM 2 Mask was initially trained on the Phase 1 data and publicly available datasets. During Phase\n2, we re-trained and updated SAM 2 Mask in the annotation loop twice using the collected data. In Phase 2,\nwe collected 63.5K masklets. The annotation time went down to 7.4 s/frame, a ∼5.1xspeed up over Phase 1.\nDespite an improvement in annotation time, this decoupled approach requires annotating masks in intermediate\nframes from scratch, without previous memory. We then advanced to develop the fully-featured SAM 2,\ncapable of performing both interactive image segmentation and mask propagation in a unifiedmodel.\nPhase 3: SAM 2. In the final phase, we utilize the fully-featured SAM 2, which accepts various types of\nprompts, including points and masks. SAM 2 benefits from memories of objects across the temporal dimension\nto generate mask predictions. This means annotators only need to provide occasional refinement clicks to\nSAM 2 to edit the predicted masklets in intermediate frames, as opposed to annotating from scratch with a\nspatial SAM which has no such memory context. During Phase 3, we re-trained and updated SAM 2 using\nthe collected annotations five times. With SAM 2 in the loop, the annotation time per frame went down to\n4.5 seconds, a ∼8.4xspeed up over Phase 1. In Phase 3, we collected 197.0K masklets.\n6\n\nModel in the LoopTime per\nFrameEdited\nFramesClicks per\nClicked\nFramePhase 1 Mask Alignment Score (IoU>0.75)\nAll Small Medium Large\nPhase 1 SAM only 37.8 s 100.00 % 4.80 - - - -\nPhase 2 SAM + SAM 2 Mask 7.4 s 23.25 % 3.61 86.4 %71.3 %80.4 % 97.9 %\nPhase 3 SAM 2 4.5 s 19.04 % 2.68 89.1 %72.8 %81.8 % 100.0 %\nTable 1Evolution of data engine phases showing the average annotation time per frame, the average percent of edited\nframes per masklet, the number of manual clicks per clicked frame, and Mask Alignment to Phase 1 by mask size.\nQuality verification. To uphold a high standard for annotation, we introduce a verification step. A separate set\nof annotators are tasked with verifying the quality of each annotated masklet as “satisfactory” (correctly and\nconsistently tracking the target object across all frames) or “unsatisfactory” (target object is well defined with\na clear boundary but the masklet is not correct or consistent). Unsatisfactory masklets were sent back to the\nannotation pipeline for refinement. Any masklets tracking not well defined objects were rejected entirely.\nAuto masklet generation. Ensuring diversity in annotation is important to enable the anything capability of\nour model. As human annotators might typically focus more on salient objects, we augment the annotations\nwith automatically generated masklets (referred to as “Auto”). This serves a dual purpose of increasing the\ncoverage of annotations and helping identify model failure cases. To generate auto masklets, we prompt\nSAM 2 with a regular grid of points in the first frame and generate candidate masklets. These are then sent\nto the masklet verification step for filtering. Automatic masklets tagged as “satisfactory” are added to the\nSA-V dataset. Masklets identified as “unsatisfactory” (i.e., model failure cases) are sampled and presented to\nannotators to refine with SAM 2 in the loop (Phase 3 of the data engine). These automatic masklets cover\nlarge salient central objects but also objects of varying sizes and positions in the background.\nAnalysis. Table 1 shows a comparison of the annotation protocol in each data engine phase through a controlled\nexperiment (details in §D.2.2). We compare the average annotation time per frame, the average percentage\nof manually edited frames per masklet, and the average number of clicks per clicked frame. For quality\nevaluation, we define the Phase 1 Mask Alignment Score as the percentage of masks whose IoU compared to\nthe corresponding masks in Phase 1 exceeds 0.75. Phase 1 data is chosen as a reference as it has per-frame\nhigh quality manual annotations. Phase 3 with SAM 2 in the loop leads to increased efficiency and comparable\nquality: it is 8.4 ×faster than Phase 1, has the lowest edited frame percentage and clicks per frame, and\nresults in better alignment.\nTraining data SA-V val 9 zero-shot\nVOS + SA-1B 50.0 62.5\n+ Phase 1 53.0 66.9\n+ Phase 2 58.8 70.9\n+ Phase 3 62.5 71.2\n+ Auto 63.2 71.5\nTable 2Segmentation accuracy ( J&Fmetric)\nimprovement from adding data from each data\nengine phase. “VOS” is a set of video object\nsegmentation datasets. Details are in §E.In Table 2, we show the performance comparison of SAM\n2 trained on the available data at the end of each phase\nkeeping the number of iterations fixed, therefore measuring\nsolely the impact of the additional data. We evaluate on our\nownSA-V val set and also on 9 zero-shot benchmarks (see\n§E.1 for details) using the standard J&Faccuracy metric\n(the higher the better) when prompting with 3-clicks on the\nfirst frame. We note a consistent improvement after iteratively\nincluding the data from each phase, not only on the in-domain\nSA-V val set, but also on the 9 zero-shot benchmarks.\n5.2 SA-V dataset\nThe SA-V dataset collected with our data engine comprises 50.9K videos with 642.6K masklets. In Table 3 we\ncompare the SA-V composition to common VOS datasets across the number of videos, masklets, and masks.\nNotably, the number of annotated masks is 53 ×(15×without auto) larger than any existing VOS dataset,\nproviding a substantial resource for future work. We are releasing SA-V under a permissive license.\nVideos.We collected a new set of 50.9K videos captured by crowdworkers. Videos comprise 54% indoor and\n46% outdoor scenes with an average duration of 14 seconds. Videos feature “ in-the-wild ” diverse environments,\nand cover various everyday scenarios. Our dataset has more videos than existing VOS datasets, and as shown\nin Fig. 5, videos span 47 countries and were captured by diverse participants (self-reported demographics).\n7\n\nFigure 4 Example videos from the SA-V dataset with masklets overlaid (manual and automatic). Each masklet has a\nunique color, and each row represents frames from one video, with 1 second between them.\n8\n\n#Videos Duration #Masklets #Masks #Frames Disapp. Rate\nDAVIS 2017 (Pont-Tuset et al., 2017) 0.2K 0.1 hr 0.4K 27.1K 10.7K 16.1 %\nYouTube-VOS (Xu et al., 2018b) 4.5K 5.6 hr 8.6K 197.3K 123.3K 13.0 %\nUVO-dense (Wang et al., 2021b) 1.0K 0.9 hr 10.2K 667.1K 68.3K 9.2 %\nVOST(Tokmakov et al., 2022) 0.7K 4.2 hr 1.5K 175.0K 75.5K 41.7 %\nBURST (Athar et al., 2022) 2.9K 28.9 hr 16.1K 600.2K 195.7K 37.7 %\nMOSE (Ding et al., 2023) 2.1K 7.4 hr 5.2K 431.7K 638.8K 41.5 %\nInternal 62.9K 281.8 hr 69.6K 5.4M 6.0M 36.4 %\nSA-V Manual 50.9K 196.0 hr 190.9K 10.0M 4.2M 42.5 %\nSA-V Manual+Auto 50.9K 196.0 hr 642.6K 35.5M 4.2M 27.7 %\nTable 3Comparison of our datasets with open source VOS datasets in terms of number of videos, duration, number\nof masklets, masks, frames, and disappearance rate. SA-V Manual contains only manually annotated labels. SA-V\nManual+Auto combines manually annotated labels with automatically generated masklets.\n0-0.1 0.1-0.2 0.2-0.3 >0.3\nNormalized mask area0.00.20.40.60.81.0Percentage of masksSA-V\nDAVIS\nMOSE\nYouTubeVOS\n(a)Size\n(b)GeographyMale\n274\nFemale\n236\n(i)Gender18-24\n109\n25-40\n30541-64\n88\n(ii)Age\n(c)Crowdworker Demographics\nFigure 5 Dataset distribution: (a) masklets size distribution (normalized by video resolution), (b) geographic diversity\nof the videos, and (c) self-reported demographics of the crowdworkers who recorded the videos.\nMasklets. The annotations comprise 190.9K manual masklet annotations and 451.7K automatic masklets\ncollected using our data engine. Example videos with masklets overlaid (manual and automatic) are shown\nin Fig. 4. SA-V has 53 ×(15×without auto annotations) more masks than the largest VOS dataset. The\ndisappearance rate (Ding et al., 2023) in SA-V Manual (the percentage of annotated masklets that disappear\nin at least one frame and then re-appear) is 42.5 %, competitive among existing datasets. Fig. 5a shows a\ncomparison of mask size distribution (normalized by video resolution) with DAVIS, MOSE, and YouTubeVOS.\nMore than 88%of SA-V masks have a normalized mask area less than 0.1.\nSA-V training, validation and test splits. We split SA-V based on the video authors (and their geographic\nlocations) to ensure minimal overlap of similar objects. To create SA-V val and SA-V test sets, we focus\non challenging scenarios in selecting videos, and ask annotators to identify challenging targets that are\nfast-moving, have complex occlusions by other objects as well as disappearance/re-appearance patterns. These\ntargets were annotated at 6 FPS using the data engine Phase 1 setup in §5.1. There are 293 masklets and 155\nvideos in the SA-V val split, and 278 masklets and 150 videos in the SA-V test split.\nInternal dataset. We also used internally available licensed video data to further augment our training set.\nOur internal dataset is comprised of 62.9K videos and 69.6K masklets annotated in Phase 2 and Phase 3 (see\n§5.1) for training, and 96 videos and 189 masklets annotated using Phase 1 for testing (Internal-test).\nSee Appendix D for more details on the data engine and SA-V dataset.\n6 Zero-shot experiments\nHere, we compare SAM 2 with previous work on zero-shot video tasks (§6.1) and image tasks (§6.2). We\nreport the standard J&Fmetric (Pont-Tuset et al., 2017) for video and mIoU metric for image tasks. Unless\notherwise mentioned, the results reported in this section follow our default setup using Hiera-B+ image\nencoder with a resolution of 1024 and trained on the full combination of datasets, i.e., SAM 2 (Hiera-B+) in\nTable 7 (see also §C.2 for details).\n9\n\n6.1 Video tasks\n6.1.1 Promptable video segmentation\n1 2 3 4 5 6 7 8\nnumber/uni00A0of/uni00A0annotated/uni00A0frames/uni00A0with/uni00A03/uni00ADclick6570758085average/uni00A0J&F/uni00A0over/uni00A0datasets\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n1 2 3 4 5 6 7 8\nnumber/uni00A0of/uni00A0annotated/uni00A0frames/uni00A0with/uni00A03/uni00ADclick6570758085average/uni00A0J&F/uni00A0over/uni00A0datasets\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n(a)offlineaverage J&Facross datasets (3-click) (b) onlineaverage J&Facross datasets (3-click)\nFigure 6 Zero-shot accuracy over 9 datasets in interactive offline and online evaluation settings.\nWe first evaluate promptable video segmentation, which involves simulating an interactive setting that\nresembles the user experience. We have two settings, offlineevaluation, where multiple passes are made\nthrough a video to select frames to interact with based on the largest model error, and onlineevaluation,\nwhere the frames are annotated in a single forward pass through the video. These evaluations are conducted\non 9 densely annotated zero-shot video datasets using Nclick= 3clicks per frame (see §E.1 for details).\nWe create two strong baselines, SAM+XMem++ and SAM+Cutie, based on two state-of-the-art models\nfor video object segmentation, XMem++ (Bekuzarov et al., 2023) and Cutie (Cheng et al., 2023a). We use\nXMem++ to generate a video segmentation based on mask inputs on one or multiple frames. SAM is used\nto provide an initial mask or to refine an output (by feeding the current segmentation as a mask prompt to\nSAM). For the SAM+Cutie baseline, we modify Cutie to allow taking mask inputs on multiple frames.\nIn Fig. 6, we report the average J&Faccuracy over Nframe = 1, .. . , 8interacted frames. SAM 2 outperforms\nSAM+XMem++ and SAM+Cutie for both offline and online evaluation settings. Across all 9 datasets (see\nper-dataset results in §E.1), SAM 2 dominates both methods, confirming that SAM 2 is able to generate\nhigh-quality video segmentation from a few clicks while also allowing continued refinement of the results with\nfurther prompts. Overall, SAM 2 can generate better segmentation accuracy, with >3×fewer interactions.\n6.1.2 Semi-supervised video object segmentation\nMethod 1-click 3-click 5-click bounding box ground-truth mask‡\nSAM+XMem++ 56.9 68.4 70.6 67.6 72.7\nSAM+Cutie 56.7 70.1 72.2 69.4 74.1\nSAM 2 64.3 73.2 75.4 72.9 77.6\nTable 4Zero-shot accuracy across 17 video datasets under semi-supervised VOS evaluation using different prompts.\nThe table shows the averaged J&Ffor each type of prompt (1, 3 or 5 clicks, bounding boxes, or ground-truth masks)\nin the first video frame (‡: in this case we directly use masks as inputs into XMem++ or Cutie without using SAM).\nWe next evaluate the semi-supervised video object segmentation (VOS) setting (Pont-Tuset et al., 2017) with\nclick, box, or mask prompts onlyon thefirst frame of the video. When using click prompts, we interactively\nsample either 1, 3 or 5 clicks on the first video frame, and then track the object based on these clicks.\nSimilar to the interactive setting in §6.1.1, we compare to XMem++ and Cutie, using SAM for click and\nbox prompts, and in their default setting when using mask prompts. We report the standard J&Faccuracy\n(Pont-Tuset et al., 2017), except for on VOST (Tokmakov et al., 2022), where we report the Jmetric following\nits protocol. The results are in Table 4. SAM 2 outperforms both baselines on the 17 datasets, using various\ninput prompts. The results underline that SAM 2 also excels at the conventional, non-interactive VOS task\nwith mask input, for which these other works are specifically designed. More details are in §E.1.3.\n10\n\n6.1.3 Fairness evaluation1-click 3-click mask\ngender\nmale 81.9 95.1 95.9\nfemale 75.1 94.1 95.2\nage\n18-26 77.2 95.0 95.7\n26-50 76.7 94.7 95.8\n50+ 81.4 95.1 96.2\nTable 5Fairness evaluation of SAM 2 (under\nJ&Fmetric) on protected demographic groups.We evaluate SAM 2 for fairness across demographic groups.\nWe collect annotations for the people category in the Ego-\nExo4D (Grauman et al., 2023) dataset, which contains self-\nreported demographic information supplied by the subject of\nthe video. We employ the same annotation setup as for SA-V\nval and test sets and apply this to 20-second clips from the\nthird-person (exo) videos. We evaluate SAM 2 on this data\nusing 1-, 3-clicks, and ground-truth mask on the first frame.\nTable 5 shows the comparison in J&Faccuracy of SAM 2 for segmenting people across gender and age. At\n3 clicks and with ground-truth mask prompts there is minimal discrepancy. We manually inspect 1 click\npredictions, and find the model frequently predicts the mask for a part instead of the person. When limiting\nthe comparison to clips where the person is correctly segmented, the gap in 1 click shrinks substantially ( J&F\nmale 94.3, female 92.7), suggesting the discrepancy can be partially attributed to ambiguity in the prompt.\nIn Appendix G, we provide model, data and annotation cards for SA-V.\n6.2 Image tasks\nWe evaluate SAM 2 on the Segment Anything task across 37 zero-shot datasets, including 23 datasets\npreviously used by SAM for evaluation. 1-click and 5-click mIoUs are reported in Table 6 and we show the\naverage mIoU by dataset domain and model speed in frames per second (FPS) on a single A100 GPU.\nThe first column (SA-23 All) shows accuracy on the 23 datasets from SAM. SAM 2 achieves higher accuracy\n(58.9 mIoU with 1 click) than SAM (58.1 mIoU with 1 click), withoutusing any extra data and while being\n6×faster. This can be mainly attributed to the smaller but more effective Hiera image encoder in SAM 2.\nThe bottom row shows how training on our SA-1B and video data mix can further improve accuracy to 61.4%\non average on the 23 datasets. We also see exceptional gains on the video benchmarks from SA-23 (video\ndatasets are evaluated as images, identical to Kirillov et al. (2023)), and the 14 new video datasets we added.\n1 (5) click mIoU\nModel Data SA-23 All SA-23 Image SA-23 Video 14 new Video FPS\nSAM SA-1B 58.1 (81.3) 60.8 (82.1) 54.5 (80.3) 59.1 (83.4) 21.7\nSAM 2 SA-1B 58.9 (81.7) 60.8 (82.1) 56.4 (81.2) 56.6 (83.7) 130.1\nSAM 2 our mix 61.4 (83.7) 63.1 (83.9) 59.1 (83.3) 69.6 (86.0) 130.1\nTable 6Zero-shot accuracy on the Segment Anything (SA) task across 37 datasets. The table shows the average 1-\nand 5-click mIoU of SAM 2 compared to SAM by domains (image/video). We report the average metrics on the 23\ndatasets used by SAM (SA-23) and the average across 14 additional zero-shot video datasets (as detailed in §E.3).\nOverall, the findings underscore SAM 2’s dual capability in interactive video and image segmentation, a\nstrength derived from our diverse training data that encompasses videos and static images across visual\ndomains. More detailed results including a breakdown by dataset are in §E.3.\n7 Comparison to state-of-the-art in semi-supervised VOS\nOur primary focus is on the general, interactive PVS task, but we also address the specific semi-supervised\nVOS setting (where the prompt is a ground-truth mask on the first frame), as it is a historically common\nprotocol. We evaluate two versions of SAM 2 with varying image encoder sizes (Hiera-B+/-L) with different\nspeed-vs-accuracy tradeoffs. We measure framesper second (FPS) on a single A100 GPU using a batch-size of\none. SAM 2 based on Hiera-B+ and Hiera-L runs at real-time speeds of 43.8 and 30.2 FPS, respectively.\nWe present a comparison with existing state-of-the-art in Table 7, reporting accuracy using standard protocols.\nSAM 2 shows significant improvement over the best existing methods. We observe that using a larger image\nencoder brings significant accuracy gains across the board.\n11\n\nJ&F G\nMethod MOSE\nvalDAVIS\n2017 valLVOS\nvalSA-V\nvalSA-V\ntestYTVOS\n2019 val\nSTCN(Cheng et al., 2021a) 52.5 85.4 - 61.0 62.5 82.7\nSwinB-AOT (Yang et al., 2021b) 59.4 85.4 - 51.1 50.3 84.5\nSwinB-DeAOT (Yang & Yang, 2022) 59.9 86.2 - 61.4 61.8 86.1\nRDE(Li et al., 2022a) 46.8 84.2 - 51.8 53.9 81.9\nXMem(Cheng & Schwing, 2022) 59.6 86.0 - 60.1 62.3 85.6\nSimVOS-B (Wu et al., 2023b) - 88.0 - 44.2 44.1 84.2\nJointFormer (Zhang et al., 2023b) - 90.1 - - - 87.4\nISVOS(Wang et al., 2022) - 88.2 - - - 86.3\nDEVA(Cheng et al., 2023b) 66.0 87.0 55.9 55.4 56.2 85.4\nCutie-base (Cheng et al., 2023a) 69.9 87.9 66.0 60.7 62.7 87.0\nCutie-base+ (Cheng et al., 2023a) 71.7 88.1 - 61.3 62.8 87.5\nSAM 2 (Hiera-B+) 75.8 90.9 74.9 73.6 74.1 88.4\nSAM 2 (Hiera-L) 77.2 91.6 76.1 75.6 77.6 89.1\nTable 7VOS comparison to prior work. SAM 2 performs well in accuracy ( J&F,G) for video segmentation based on\nfirst-frame ground-truth mask prompts. SAM 2 performs significantly better on SA-V val/test.\nWe also evaluate existing work on the SA-V val and test sets which measure performance for open-world\nsegments of “any” object class. When comparing on this benchmark, we see that most previous methods peak\nat around the same accuracy. The best performance on SA-V val and SA-V test for prior work is significantly\nlower demonstrating the gap to a “segment anything in videos” capability. Finally, we see that SAM 2 also\nbrings notable gains in long-term video object segmentation as observed in the LVOS benchmark result.\n8 Data and model ablations\nThis section presents ablations that informed the design decisions for SAM 2. We evaluate on our MOSE\ndevelopment set (“MOSE dev”) which contains 200 randomly-sampled videos from the MOSE training split\nand excluded from the training data in our ablations, SA-V val, and the average over 9 zero-shot video\ndatasets. As the metric for comparison, we report J&Funder 3-click input on the first frame as a balance\nbetween the 1-click regime and the VOS-style mask prompts. Additionally, we report the average 1-click mIoU\non the 23-dataset benchmark used by SAM for the SA task on images. Unless otherwise specified, we run our\nablations at 512 resolution and with SA-V manual and a 10% subset of SA-1B. Additional details are in §C.2.\n8.1 Data ablations\nData mix ablation. In Table 8, we compare the accuracy of SAM-2 when trained on different data mixtures.\nWe pre-train on SA-1B and then train a separate model for each setting. We fix the number of iterations\n(200k) and batch size (128) with onlythe training data changing between experiments. We report accuracy\non our SA-V val set, MOSE, 9 zero-shot video benchmarks, and the SA-23 tasks (§6.2). Row 1 shows that a\nmodel purely trained on VOS datasets (Davis, MOSE, YouTubeVOS) performs well on the in-domain MOSE\ndev, but poorly on all the others including the 9 zero-shot VOS datasets (59.7 J&F).\nWe observe tremendous benefit from adding our data engine data into the training mix, including +12.1%\naverage performance improvement on 9 zero-shot datasets (row 11 vs 1). This can be attributed to the\nlimited coverage and size of VOS datasets. Adding SA-1B images improves the performance on the image\nsegmentation task (rows 3 vs 4, 5 vs 6, 9 vs 10, 11 vs 12) without degrading the VOS capability. Training only\non SA-V and SA-1B (row 4) is enough to obtain strong performance on all benchmarks except for MOSE.\nOverall, we obtain the best results when mixing all datasets: VOS, SA-1B, and our data engine data (row 12).\nData quantity ablation. Next, we study the effect of scaling training data. SAM 2 is pre-trained on SA-1B\nbefore training on varying sizes of SA-V. We report average J&Fscore (when prompted with 3 clicks in the\n12\n\nTraining data J&F mIoU\nVOSInternal SA-V SA-1B SA-V val Internal-test MOSE dev 9 zero-shot SA-23\n1 ✓ 48.1 60.2 76.9 59.7 45.4\n2 ✓ 57.0 72.2 70.6 70.0 54.4\n3 ✓ 63.0 72.6 72.8 69.7 53.0\n4 ✓ ✓ 62.9 73.2 73.6 69.7 58.6\n5 ✓ ✓ 63.0 73.2 73.3 70.9 55.8\n6 ✓ ✓ ✓ 63.6 75.0 74.4 71.6 58.6\n7 ✓ ✓ 50.0 63.2 77.6 62.5 54.8\n8 ✓ ✓ 54.9 71.5 77.9 70.6 55.1\n9 ✓ ✓ 61.6 72.8 78.3 69.9 51.0\n10 ✓ ✓ ✓ 62.2 74.1 78.5 70.3 57.3\n11 ✓ ✓ ✓ 61.8 74.4 78.5 71.8 55.7\n12 ✓ ✓ ✓ ✓ 63.1 73.7 79.0 71.6 58.9\nTable 8We train our model on different data mixtures including VOS (Davis, MOSE, YouTubeVOS), and subsets of\nInternal-train, SA-V, and SA-1B. We report the J&Faccuracy when prompted with 3 clicks in the first frame on\nSA-V val and Internal-test, MOSE, and 9 zero-shot datasets, and the average 1-click mIoU on SA-23 datasets.\nfirst frame) over 3 benchmarks: SA-V val, zero-shot, and MOSE dev. Fig. 7 shows a consistent power law\nrelationship between the quantity of training data and the video segmentation accuracy on all benchmarks.\n5 10 20 50 100 200\n#masklets/uni00A0×/uni00A01000586062646668SA/uni00ADV/uni00A0val/uni00A0J&F\n5 10 20 50 100 200\n#masklets/uni00A0×/uni00A010006667686970717273zero/uni00ADshot/uni00A0J&F\n5 10 20 50 100 200\n#masklets/uni00A0×/uni00A0100069707172737475MOSE/uni00A0dev/uni00A0J&F\nFigure 7 Performance of SAM 2 as a function of the SA-V quantity. We report J&Faccuracy for 3-click prompts in\nthe first frame on SA-V val (left), 9 zero-shot datasets (center), and MOSE dev (right).\nData quality ablation. In Table 9, we experiment with filtering strategies for quality. We subsample 50k\nmasklets from SA-V, either randomly or by taking the masklets that have been edited the most by annotators.\nFiltering based on the number of edited frames leads to strong performance using just 25% of the data, and\noutperforms random sampling. However, it is worse than using all 190k SA-V masklets.\nJ&F mIoU\nSetting SA-V val Intern-test MOSE dev 9 zero-shot SA-23\nSA-1B + SA-V 50k random 63.7 70.3 72.3 68.7 59.1\nSA-1B + SA-V 50k most edited 66.2 73.0 72.5 69.2 58.6\nSA-1B + SA-V 69.9 73.8 73.9 70.8 59.8\nTable 9We train our model on different subsets of our SA-V Manual data: 50k randomly sampled masklets, 50k\nmasklets with the most edited frames, and the full SA-V dataset (190k masklets).\n8.2 Model architecture ablations\nIn this section, we present model ablations that guided design decisions, conducted under a smaller model\nsetup with 512 input resolution by default. For each ablation setting, we report segmentation accuracy for\nvideo ( J&F) and image (mIoU) tasks, and its relative video segmentation speed (the maximum inference\nthroughput relative to the ablation default setup in gray). We find design choices for image and video\ncomponents to be largely decoupled – this can be attributed to our modular design and training strategy.\n13\n\n8.2.1 Capacity ablations\nJ&F mIoU\nres.MOSE dev SA-V val 9 zero-shot speed SA-23\n512 73.0 68.3 70.7 1.00×59.7\n768 76.1 71.1 72.5 0.43×61.0\n1024 77.0 70.1 72.3 0.22 × 61.5\n(a) Resolution.J&F mIoU\n#frames MOSE dev SA-V val 9 zero-shot speed SA-23\n4 71.1 60.0 67.7 1.00 × 60.1\n8 73.0 68.3 70.7 1.00 ×59.7\n10 74.5 68.1 71.1 1.00×59.9\n(b) #Frames .\nJ&F mIoU\n#mem. MOSE dev SA-V val 9 zero-shot speed SA-23\n4 73.5 68.6 70.5 1.01× 59.9\n6 73.0 68.3 70.7 1.00×59.7\n8 73.2 69.0 70.7 0.93× 59.9\n(c) #Memories.J&F mIoU\nchan. dim. MOSE dev SA-V val 9 zero-shot speed SA-23\n64 73.0 68.3 70.7 1.00 ×59.7\n256 73.4 66.4 70.0 0.92 × 60.0\n(d) Memory channels.\nJ&F mIoU\n(#sa, #ca) MOSE dev SA-V val 9 zero-shot speed SA-23\n(2, 2) 73.3 67.3 70.2 1.13×59.9\n(3, 2) 72.7 64.1 69.5 1.08 × 60.0\n(4, 4) 73.0 68.3 70.7 1.00×59.7\n(e) Memory attention.J&F mIoU\nimg. enc. MOSE dev SA-V val 9 zero-shot speed SA-23\nS 70.9 65.5 69.4 1.33×57.8\nB+ 73.0 68.3 70.7 1.00 ×59.7\nL 75.0 66.3 71.9 0.60× 61.1\n(f) Image encoder size .\nTable 10 Capacity ablations . We ablate modeling capacity along input size (resolution, #frames), memory size\n(#memories, memory channel dim) and model size (memory attention, image encoder). Ablation defaults in gray .\nInput size. During training, we sample sequences of frames of fixed resolution and fixed length (here denoted by\n# frames). We ablate their impact in Tables 10a, 10b. A higher resolution leads to significant improvements\nacross image and video tasks, and we use an input resolution of 1024 in our final model. Increasing the number\nof frames brings notable gains on video benchmarks and we use a default of 8 to balance speed and accuracy.\nMemory size. Increasing the (maximum) number of memories, N, generally helps the performance although\nthere could be some variance, as in Table 10c. We use a default value of 6 past frames to strike a balance\nbetween temporal context length and computational cost. Using fewer channels for memories does not cause\nmuch performance regression as in Table 10d, while making the memory required for storage 4 ×smaller.\nModel size. More capacity in the image encoder or memory-attention (#self-/#cross-attention blocks) generally\nleads to improved results, as shown in Tables 10e, 10f. Scaling the image encoder brings gains on both image\nand video metrics, while scaling the memory-attention only improves video metrics. We default to using a B+\nimage encoder, which provides a reasonable balance between speed and accuracy.\n8.2.2 Relative positional encoding\nJ&F mIoU\nRPB in img. enc. 2d-RoPE in mem. attn. MOSE dev SA-V val LVOSv2 val 9 zero-shot speed SA-23\n✓ 73.0 68.3 71.6 70.7 1.00 ×59.7\n✓ ✓ 73.6 67.9 71.0 71.5 0.93× 60.0\n72.8 67.1 70.3 70.3 1.04×59.9\nTable 11 Relative positional encoding . We use 2d-RoPE in memory attention while removing RPB from the image\nencoder by default ( gray). Removing RPB also allows us to enable FlashAttention-2 (Dao, 2023), which gives a\nsignificant speed boost at 1024 resolution. At the higher resolution of 1024, the speed gap between 2d-RoPE (1st row)\nand the no RoPE baseline (3rd row) becomes much smaller.\nBy default, we always use absolute positional encoding in both the image encoder as well as memory attention.\nIn Table 11, we study relative positional encoding design choices. Here we also evaluate on LVOSv2 (Hong\net al., 2024) with 3 clicks on the 1st frame as a benchmark for long-term video object segmentation.\n14\n\nWhile SAM (Kirillov et al., 2023) follows Li et al. (2022b) in adding relative positional biases (RPB) to all\nimage encoder layers, Bolya et al. (2023) improve upon this by removing RPB in all but the global attention\nlayers while adopting “absolute-win” positional encoding which brings large speed gains. We improve upon this\nfurther by removing all RPB from the image encoder, with no performance regression on SA-23 and minimal\nregression on video benchmarks (see Table 11), while giving a significant speed boost at 1024 resolution. We\nalso find it is beneficial to use 2d-RoPE (Su et al., 2021; Heo et al., 2024) in the memory attention.\n8.2.3 Memory architecture ablations\nRecurrent memory. We investigate the effectiveness of feeding the memory features to a GRU before adding\nthem to the memory bank. Similar to §8.2.2, we also evaluate on LVOSv2 as an additional benchmark for\nlong-term object segmentation. While prior works have commonly employed GRU (Cho et al., 2014) states\nas a means of incorporating memory into the tracking process, our findings in Table 12 suggest that this\napproach does not provide an improvement (except slightly on LVOSv2). Instead, we find it sufficient to\ndirectly store the memory features in the memory bank, which is both simpler and more efficient.\nObject pointers. We ablate the impact of cross-attending to the object pointer vectors from the mask decoder\noutput in other frames (see §4). The results presented in Table 12 show that while cross-attending to\nobject pointers does not enhance average performance across the 9 zero-shot datasets, it significantly boosts\nperformance on SA-V val dataset as well as on the challenging LVOSv2 benchmark (validation split). Hence,\nwe default to cross-attending to object pointers together with the memory bank.\nJ&F mIoU\nObject Pointers GRU MOSE dev SA-V val LVOSv2 val 9 zero-shot speed SA-23\n73.1 64.5 67.0 70.9 1.00 ×59.9\n✓ 72.3 65.3 68.9 70.5 0.97 × 60.0\n✓ 73.0 68.3 71.6 70.7 1.00×59.7\nTable 12 Ablations on memory design. We use object pointers by default ( gray) and also study recurrent GRU memory.\n9 Conclusion\nWe present a natural evolution of Segment Anything into the video domain, based on three key aspects:\n(i) extending the promptable segmentation task to video, (ii) equipping the SAM architecture to use memory\nwhen applied to video, and (iii) the diverse SA-V dataset for training and benchmarking video segmentation.\nWe believe SAM 2 marks a significant advancement in visual perception, positioning our contributions as\nmilestones that will propel further research and applications in the field.\n10 Acknowledgements\nWe thank Alexander Kirillov and Jitendra Malik for discussions on project direction. Thanks to Andrew\nHuang, Sahir Gomez, Miguel Martin, Devansh Kukreja, and Somya Jain for work on the demo, and to Aohan\nLin and Meng Wang for creating the dataset visualizer. We thank Shoubhik Debnath and Sagar Vaze for their\nwork on dataset preparation. Thanks also to William Ngan and Sasha Mitts for their design expertise and to\nGrant Gardner and George Orlin for leading product management. We are grateful to Joelle Pineau, Daniel\nBolya, Kate Saenko, Pengchuan Zhang, and Christopher Chedeau, for valuable discussions. Thanks to Rene\nMartinez Doehner and Baishan Guo for data support, and to our annotation engineering and management\npartners: Robert Kuo, Rishi Godugu, Bob Kamma, Ida Cheng, Claudette Ward, Kai Brown, Jake Kinney,\nJenny Truong, and Karen Bergan. Thanks to Vispi Cassod, Parth Malani, Shiva Koduvayur, Alexander\nMiller, and Caleb Ho for their support with compute and infra. Finally, we thank Azita Shokrpour, Mallika\nMalhotra, Rodrick Shepard, Jonathan Torres, Luc Dahlin, David Soofian, Alex Bosenberg, and Amanda\nKallet for project-level support.\n15\n\nAppendix\nTable of contents:\n•§A: Task Details\n•§B: Limitations\n•§C: Model Details\n•§D: Dataset Details\n•§E: Zero-shot Experiments Details\n•§G: Dataset, Annotation, and Model Cards\n•§D.2.1: Annotation Guidelines\nA Details on the PVS Task\nThe Promptable Visual Segmentation (PVS) task can be seen as an extension of the Segment Anything (SA)\ntask from static images to videos. In the PVS setting, given an input video, the model can be interactively\nprompted with different types of inputs (including clicks, boxes, or masks) on any frame in the video, with\nthe goal of segmenting (and tracking) a valid object throughout the video. When interacting with a video,\nthe model provides an instant response on the frame being prompted (similar to the interactive experience\nof SAM on images), and also returns the segmentation of the object throughout the entire video in near\nreal-time. Similar to SAM the focus is on validobjects which have a clearly defined boundary, and we do not\nconsider regions without visual boundaries (e.g. Bekuzarov et al. (2023)). Fig. 8 illustrates the task.\nSegment\nAnything\n(SA)Promptable Visual\nSegmentation\n(PVS)Semi-supervised Video\nObject Segmentation\n(VOS)\nFigure 8 An illustration of the Promptable Visual Segmentation task (PVS). Previously studied tasks such as Segment\nAnything (SA) and semi-supervised Video Object Segmentation (VOS) can be seen as special cases of the PVS task.\nPVS is related to several tasks in both the static image and video domains. On images, the SA task can be\nconsidered a subset of PVS with the video reduced to a single frame. Similarly, traditional semi-supervised and\ninteractive VOS (Pont-Tuset et al., 2017) tasks are special cases of PVS, limited to mask prompts provided\nonly on the firstframe and scribbles on multiple frames to segment objects throughout a video, respectively.\nIn PVS, prompts can either be clicks, masks, or boxes, and the focus is on enhancing the interactive experience,\nenabling easy refinement of an object’s segmentation with minimal interaction.\nB Limitations\nSAM 2 demonstrates strong performance in both static image and video domains, yet it encounters difficulties\nin certain scenarios. The model may fail to segment objects across shot changes and can lose track of or\nconfuse objects in crowded scenes, after long occlusions or in extended videos. To alleviate this issue, we\n16\n\ndesigned the ability to prompt SAM 2 in anyframe: if the model loses the object or makes an error, refinement\nclicks on additional frames can quickly recover the correct prediction in most cases. SAM 2 also struggles\nwith accurately tracking objects with very thin or fine details especially when they are fast-moving. Another\nchallenging scenario occurs when there are nearby objects with similar appearance (e.g., multiple identical\njuggling balls). Incorporating more explicit motion modeling into SAM 2 could mitigate errors in such cases.\nWhile SAM 2 can track multiple objects in a video simultaneously, SAM 2 processes each object separately,\nutilizing only shared per-frame embeddings without inter-object communication. While this approach is\nsimple, incorporating shared object-level contextual information could aid in improving efficiency.\nOur data engine relies on human annotators to verify masklet quality and select frames that require correction.\nFuture developments could include automating this process to enhance efficiency.\nC SAM 2 details\nC.1 Architecture\nHere we discuss further architecture details, expanding on the model description in §4.\nImage encoder. We use a feature pyramid network (Lin et al., 2017) to fuse the stride 16 and 32 features from\nStages 3 and 4 of the Hiera image encoder respectively to produce the image embeddings for each frame. In\naddition, the stride 4 and 8 features from Stages 1 and 2 are not used in the memory attention but are added\nto the upsampling layers in the mask decoder as shown in Figure 9, which helps produce high-resolution\nsegmentation details. We follow Bolya et al. (2023) in using windowed absolute positional embeddings in the\nHiera image encoder. In Bolya et al. (2023), RPB provided positional information spanning acrosswindows\nin the image encoder, in lieu of which we adopt a simpler approach of interpolating the global positional\nembedding instead to span across windows. We do not use any relative positional encoding. We train models\nwith varying image encoder sizes – T, S, B+ and L. We follow Li et al. (2022b) and use global attention in\nonly a subset of the image encoder layers (see Table 13).\nMemoryattention. In addition to sinusoidal absolute positional embeddings, we use 2d spatial Rotary Positional\nEmbedding (RoPE) (Su et al., 2021; Heo et al., 2024) in self-attention and cross-attention layers. The object\npointer tokens are excluded from RoPE as they do not have specific spatial correspondence. By default, the\nmemory attention uses L= 4layers.\nimage\nembedding\n(256x64x64)x2\ntoken\nto image\nattn.2x\nconv.\ntrans.\nIoU\nscoresmlpmasksdot product\nper mask\nprompt tokens\n(Ntokensx256)output tokens\n+output\ntoken\nper mask\nIoU\noutput tokenmlpmask decoder\nself attn.token to image attn.mlpimage to token attn.stride 4, 8 feats. from img. enc.\nobj ptr\nmlpocclusion \ntokenocclusion \nscore\nFigure 9 Mask decoder architecture. The design largely follows SAM, and we additionally include the stride 4 and 8\nfeatures from the image encoder during upsampling. We also use the mask token corresponding to the output mask as\nan object pointer and generate an occlusion score which indicates if the object of interest is visible in the current frame.\nPrompt encoder and mask decoder. The prompt encoder design follows SAM, and we next discuss additional\ndetails on design changes in the mask decoder. We use the mask token corresponding to the output mask\nas the object pointer token for the frame, which is placed in the memory bank. As discussed in §4, we also\nintroduce an occlusion prediction head. This is accomplished by including an additional token along with\nthe mask and IoU output tokens. An additional MLP head is applied to this new token to produce a score\nindicating the likelihood of the object of interest being visible in the current frame (as shown in Figure 9).\n17\n\nSAM introduced the ability to output multiple valid masks when faced with ambiguity about the object being\nsegmented in an image. For example, when a person clicks on the tire of a bike, the model can interpret this\nclick as referring to only the tire or the entire bike and output multiple predictions. In videos, this ambiguity\ncan extend across video frames. For example, if in one frame only the tire is visible, a click on the tire might\nrelate to just the tire, or as more of the bike becomes visible in subsequent frames, this click could have been\nintended for the entire bike. To handle this ambiguity, SAM 2 predicts multiple masks at each step of the\nvideo. If further prompts do not resolve the ambiguity, the model selects the mask with the highest predicted\nIoU for the current frame for further propagation in the video.\nMemory encoder and memory bank. Our memory encoder does not use an additional image encoder and\ninstead reuses the image embeddings produced by the Hiera encoder, which are fused with the predicted mask\ninformation to produce memory features (as discussed in §4). This design allows the memory features to\nbenefit from the strong representations produced by the image encoder (especially when we scale the image\nencoder to a larger size). Further, we project the memory features in our memory bank to a dimension of 64,\nand split the 256-dim object pointer into 4 tokens of 64-dim for cross-attention to the memory bank.\nC.2 Training\nC.2.1 Pre-training\nWe first pre-train SAM 2 on static images on the SA-1B dataset (Kirillov et al., 2023). Table 13a details the\nsettings used during pre-training on SA-1B – other settings not mentioned here follow Kirillov et al. (2023).\nThe image encoder is initialized from MAE pre-trained Hiera (Ryali et al., 2023). Similar to SAM, we filter\nmasks covering more than 90% of the image and restricted training to 64 randomly sampled masks per image.\nUnlike SAM, we found it beneficial to use an ℓ1loss to more aggressively supervise the IoU predictions and\nto apply a sigmoid activation to the IoU logits to restrict the output into the range between 0 and 1. For\nmulti-mask predictions (on the first click), we supervise the IoU predictions of allmasks to encourage better\nlearning of when a mask might be bad, but only supervise the mask logits with the lowest segmentation\nloss (linear combination of focal and dice loss). In SAM, during iterative sampling of points, two iterations\nwere inserted with no additional prompts (only feeding the previous mask logits) – we do not add such\niterations during our training and use 7 correction clicks (instead of 8 in SAM). We also employ horizontal\nflip augmentation during training and resize the image to a square size of 1024 ×1024.\nWe use AdamW (Loshchilov & Hutter, 2019) and apply layer decay (Clark et al., 2020) on the image encoder\nand follow a reciprocal square-root schedule (Zhai et al., 2022). See Table 13 (a) for the hyperparameters in\nour pre-training stage.\nC.2.2 Full training\nAfter pre-training, we train SAM 2 on our introduced datasets SA-V + Internal (section §5.2), a 10% subset\nof SA-1B, and a mixture of open-source video datasets including DAVIS (Pont-Tuset et al., 2017; Caelles\net al., 2019), MOSE (Ding et al., 2023), and YouTubeVOS (Xu et al., 2018b). Our released model is trained\non SA-V manual + Internal and SA-1B.\nSAM 2 is designed for two tasks; the PVS task (on videos) and the SA task (on images). Training is done\njointlyon image and video data. To optimize our data usage and computational resources during training, we\nadopt an alternating training strategy between video data (multiple frames) and static images (one single\nframe). Specifically, in each training iteration, we sample a full batch either from the image or video dataset,\nwith their sampling probabilities proportional to the size of each data source. This approach allows for\na balanced exposure to both tasks and a different batch size for each data source to maximize compute\nutilization. Settings not explicitly mentioned here for the image task follow settings from the pre-training\nphase. See Table 13 (b) for the hyperparameters in our full training stage. The training data mixture consists\nof∼15.2% SA-1B, ∼70% SA-V and ∼14.8% Internal. The same settings are used when open-source datasets\nare included, with the change that the additional data is included ( ∼1.3% DAVIS, ∼9.4% MOSE, ∼9.2%\nYouTubeVOS, ∼15.5% SA-1B, ∼49.5% SA-V, ∼15.1% Internal).\n18\n\nconfig value\ndata SA-1B\nsteps ∼90k\nresolution 1024\nprecision bfloat16\noptimizer AdamW\noptimizer momentum β1, β2=0.9,0.999\ngradient clipping type: ℓ2, max: 0.1\nweight decay 0.1\nlearning rate (lr) 4e-4\nlr schedule reciprocal sqrt, timescale=1000\nwarmup linear, 1k iters\ncooldown linear, 5k iters\nlayer-wise decay 0.8 (T, S), 0.9 (B+), 0.925 (L)\naugmentation hflip, resize to 1024 (square)\nbatch size 256\ndrop path 0.1 (T, S), 0.2 (B+), 0.3 (L)\nmask losses (weight) focal (20), dice (1)\nIoU loss (weight) ℓ1(1)\nmax. masks per img. 64\n# correction points 7\nglobal attn. blocks 5-7-9 (T), 7-10-13 (S), 12-16-20\n(B+), 23-33-43 (L)\n(a)Pre-trainingconfig value\ndata SA-1B, Internal, SA-V\nsteps ∼300k\nresolution 1024\nprecision bfloat16\noptimizer AdamW\noptimizer momentum β1, β2=0.9,0.999\ngradient clipping type: ℓ2, max: 0.1\nweight decay 0.1\nlearning rate (lr) img. enc.: 6e-5, other: 3.0e-4\nlr schedule cosine\nwarmup linear, 15k iters\nlayer-wise decay 0.8 (T, S), 0.9 (B+), 0.925 (L)\nimg. augmentation hflip, resize to 1024 (square)\nvid. aug. hflip, affine (deg: 25, shear: 20),\ncolorjitter (b: 0.1, c: 0.03, s:\n0.03, h: null), grayscale (0.05),\nper frame colorjitter (b: 0.1, c:\n0.05, s: 0.05, h: null)\nbatch size 256\ndrop path 0.1 (T, S), 0.2 (B+), 0.3 (L)\nmask losses (weight) focal (20), dice (1)\nIoU loss (weight) ℓ1(1)\nocclusion loss (weight) cross-entropy (1)\nmax. masks per frame. image: 64, video: 3\n# correction points 7\nglobal attn. blocks 5-7-9 (T), 7-10-13 (S), 12-16-20\n(B+), 23-33-43 (L)\n(b)Full training\nTable 13 Hyperparameters and details of SAM 2 pre-training and full training. Note that some settings vary with\nimage encoder size (T, S, B+, L).\nWe train by simulating an interactive setting, sampling 8-frame sequences and randomly selecting up to 2\nframes (including the first) for corrective clicks. During training, we use ground-truth masklets and model\npredictions to sample prompts, with initial prompts being the ground-truth mask (50% probability), a positive\nclick from the ground-truth mask (25%), or a bounding box input (25%).\nWe restrict the maximum number of masklets for each sequence of 8 frames to 3 randomly chosen ones. We\nreverse the temporal order with a probability of 50% to help generalization to bi-directional propagation.\nWhen we sample corrective clicks - with a small probability of 10%, we randomly sample clicks from the\nground truth mask, irrespective of the model prediction, to allow additional flexibility in mask refinement.\nLosses and optimization. We supervise the model’s predictions using a linear combination of focal and dice\nlosses for the mask prediction, mean-absolute-error (MAE) loss for the IoU prediction, and cross-entropy loss\nfor object prediction with a ratio of 20:1:1:1 respectively. As during pre-training, for multi-mask predictions,\nwe only supervise the mask with the lowest segmentation loss. If the ground-truth does not contain a mask\nfor a frame, we do not supervise any of the mask outputs (but always supervise the occlusion prediction head\nthat predicts whether there should exist a mask in the frame).\nC.3 Speed benchmarking\nWe conduct all benchmarking experiments on a single A100 GPU using PyTorch 2.3.1 and CUDA 12.1, under\nautomatic mixed precision with bfloat16. We compile the image encoder with torch.compile for all SAM 2\nmodels and do the same for SAM and HQ-SAM for direct comparison on the SA task (Tables 6 and 15). The\nFPS measurements for the SA task were conducted using a batch size of 10 images, which was found to yield\nthe highest FPS across all three model types. For video tasks, we use a batch size of 1 following the common\nprotocol in video segmentation.\n19\n\nD Data details\nD.1 SA-V dataset details\n(a) ML only (b) With Auto\nFigure 10 Annotations overlaid on the first\nframe: (a) manual labels (ML) only, (b)\nwith automatic labels (Auto). Automatic\nlabels increase diversity and coverage.Videos.Resolutions range from 240p to 4K with 1,401 ×1,037 on\naverage. Duration ranges from 4 seconds to 2.3 minutes, with an\naverage of 13.8 seconds, totaling 4.2M frames and 196 hours.\nAutomatic masklets. Similar to the approach described by Kirillov\net al. (2023), automatic masklets are generated by prompting the\nmodel with regular grids. We prompt the model with a 32×32\ngrid on the first frame, and additionally we use a 16×16grid\non 4 zoomed image crops of the first frame (derived from a 2×2\noverlapped window) and a 4×4grid on 16 zoomed image crops of\nthe first frame (derived from a 4×4overlapped window). We apply\ntwo post-processing steps across all frames. First, we remove tiny\ndisconnected components with areas smaller than 200 pixels. Second, we fill in holes in segmentation masks\nif the area of the hole is less than 200 pixels. By combining these automatically generated masklets with\nmanually created ones, we enhance the coverage of annotations in the SA-V dataset, as illustrated in Fig. 10.\nD.2 Data engine details\nD.2.1 Annotation protocol\nA diagram of the annotation protocol used in our data engine is shown in Fig. 11. The annotation task was\nseparated into steps each carried out by a different annotator: Steps 1 and 2 focus on object selection , Steps 3\nand 4 on masklet tracking , and Step 5 on quality verification . SAM 2 was deployed on GPU as an API and\nbuilt into the annotation tool to enable interactive use.\nCompared to image segmentation annotation, large-scale video segmentation annotation presents unique\nchallenges which require innovations in the annotation task design and protocol. To improve our model’s\nability to “segment anything”, it was important to focus annotation on challenging objects where SAM 2\nstruggled. We leveraged our online model in the loop setup to enable this, requesting annotators to use SAM\n2 interactively to identify failure modes and then correct them.\nWe found the number of edited frames to be a proxy to the “challengingness” of an object as shown in Table 9.\nTherefore, we asked annotators to annotate objects that required at least 2 edited frames with SAM 2 in the\nloop. To focus annotation on less prominent and more challenging cases, annotators were presented with videos\npre-filled with verified satisfactory automatic masklets and asked to find un-annotated challenging objects.\nWe further decouple the object selection task from the annotation task: in the selection task annotators focus\non choosing the challenging objects in one frame, while in the annotation task annotators are presented with\na challenging target object and requested to annotate the masklet consistently throughout the video.\nD.2.2 Data engine phase comparison\nThe comparison of data engine phases shown in Table 1 was conducted as a controlled experiment using 169\nvideos and 452 masklets. We ask three subsets of annotators to annotate the same set of objects with the\nannotation protocol from each phase. We categorize masklets into three buckets based on the mask area in the\nfirst frame (small: 1 to 322, medium: 322to962, and large: equal or greater than 962). Phase 1 data is used\nas the quality reference, due to the high quality masks from frame-by-frame manual annotation with SAM.\nE Details on zero-shot transfer experiments\nIn this section, we describe further details of our zero-shot experiments (§6). Unless otherwise noted, the\nresults reported in this section follow our default setup using Hiera-B+ image encoder with a resolution of\n1024 and trained on the full combination of datasets, i.e., SAM 2 (Hiera-B+) in Table 7.\n20\n\nFigure 11 Annotation guideline overview. There are 3 main annotation tasks: masklet selection, masklet tracking, and\nmasklet verification. Each task has a different set of annotators working on it.\nE.1 Zero-shot video tasks\nE.1.1 Video dataset details\nWe evaluate SAM 2 on a diverse benchmark of 17 zero-shot datasets: EndoVis 2018 (Allan et al., 2020)\ncontains medical surgery videos with robotic instruments. ESD (Huang et al., 2023) contains videos from a\nrobot manipulator camera often with motion blur. LVOSv2 (Hong et al., 2024) is a benchmark for long-term\nvideo object segmentation. LV-VIS (Wang et al., 2023) contains videos from a diverse set of open-vocabulary\nobject categories. UVO (Wang et al., 2021b) contains videos for open-world object segmentation, and\nVOST (Tokmakov et al., 2022) contains videos with objects undergoing large transformations such as egg\nbroken or paper torn. PUMaVOS (Bekuzarov et al., 2023) contains videos with segments around object parts\nsuch as a person’s cheek. Virtual KITTI 2 (Cabon et al., 2020) is a synthetic video dataset with driving scenes.\nVIPSeg (Miao et al., 2022) provides object segmentation in panoptic videos. Wildfires (Toulouse et al., 2017)\ncontains wildfire videos under different conditions from the Corsican Fire Database. VISOR (Darkhalil et al.,\n2022) contains egocentric videos in kitchen scenes with segments around hands and active objects. FBMS (Brox\net al., 2010) provides motion segmentation over moving objects in videos. Ego-Exo4D (Grauman et al., 2023)\n21\n\nis a large dataset with egocentric videos around various human activities. Cityscapes (Cordts et al., 2016)\ncontains videos for urban driving scenes. Lindenthal Camera (Haucke & Steinhage, 2021) contains videos in a\nwildlife park with segments around observed animals such as birds and mammals. HT1080WT Cells (Gómez-de\nMariscal et al., 2021) contains microscopy videos with cell segments. Drosophila Heart (Fishman et al., 2023)\ncontains microscopy videos for the heart of fruit flies.\nAmong these 17 zero-shot video datasets above, 9 of them (EndoVis, ESD, LVOSv2, LV-VIS, UVO, VOST,\nPUMaVOS, Virtual KITTI 2, and VIPSeg) have dense object segments annotated for every video frame. In\nthe remaining 8 datasets (Wildfires, VISOR, FBMS, Ego-Exo4D, Cityscapes, Lindenthal Camera, HT1080WT\nCells, and Drosophila Heart), the object segments are sparsely annotated over only a subset of video frames,\nand we compute the metrics on those frames where the ground-truth segmentation masks are available. In\nmost evaluations of the paper, we only evaluate zero-shot performance on the 9 densely annotated datasets,\nwhile in our semi-supervised VOS evaluation (§6.1.2), we evaluate on all these 17 datasets listed above.\nE.1.2 Interactive offline and online evaluation details\nOffline evaluation involvesmultiple passes over the entire video. We start with click prompts on the first\nframe, segment the object throughout the entire video, and then in the next pass, we select the frame with the\nlowest segmentation IoUw.r.t. the ground-truth as the new frame for prompting. The model then segments\nthe object again throughout the video based on allprompts received previously, until reaching a maximum of\nNframepasses (with one new prompted frame in each pass).\nOnline evaluation involves only one pass over the entire video. We start with click prompts on the first\nframe and propagate the prompts across the video, pausing propagation when encountering a frame with a\nlow-quality prediction ( IoU<0.75with ground-truth). We then add additional click prompts on the paused\nframe to correct the segment on this frame and resume the propagation forwarduntil reaching another low\nquality frame with IoU<0.75. This is repeated while the number of prompted frames is less than the\nmaximum Nframe. Unlike the previous offline evaluation, in this setting, the new prompts only affect the\nframesafterthe current paused frame but not the frames before it.\nIn both settings, we evaluate on 9 densely annotated datasets in §E.1.1 (EndoVis, ESD, LVOSv2, LV-VIS,\nUVO, VOST, PUMaVOS, Virtual KITTI 2, and VIPSeg). If a video contains multiple objects to segment in\nits ground-truth annotations, we perform inference on each object independently. We simulate interactive\nvideo segmentation with Nclick= 3clicks per frame, assuming that the user would visually locate the object\nto label it (with initial clicks) or to refine the current segmentation prediction of it (with correction clicks).\nSpecifically, when starting the first pass (where there are not any existing predictions yet), we place an initial\nclick on the first frame at the center1of the object’s ground-truth mask and then interactively add two more\nclicks based on the center of the error region (between the ground-truth mask and the predicted segments on\nthe first frame). Then in subsequent passes (where there are already predicted segments), we interactively\nadd three clicks based on the center of the error region (between the ground-truth mask and the predicted\nsegments on the frame being prompted).\nWe report the average J&Fmetric over Nframe = 1, .. . , 8interacted frames and the J&Fmetrics under\ndifferent annotation time on a video based on the following assumptions:\n•On each frame, it takes Tloc= 1sec for the annotator to visually locate an object in the frame, and\nTclick= 1.5sec to add each click, following Delatolas et al. (2024).\n•Inofflinemode, it takes Texam = 30sec on a 300-frame video to examine the results throughout the\nvideo in each round, including finding the frame with the worst segmentation quality to add corrections\n(and for longer or shorter videos, this time is proportional to the video length L, assuming the annotator\ncould examine the results at 10 FPS).\n•Inonlinemode, it takes Texam = 30sec on a 300-frame video to follow the results throughout the\nvideo in total, including pausing at a frame with low quality for further corrections (and this time is\nproportional to the video length Lsimilar to the offline mode).\n1The center of a mask is defined as the mask pixel that has the largest Euclidean distance to the mask boundary.\n22\n\n20 40 60 80 100 120\nEndoVis/uni00A02018/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n606570758085J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n20 40 60 80 100 120 140 160\nESD/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n848586878889909192J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n50 100 150 200 250 300 350 400\nLVOSv2/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n6065707580859095J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n10 20 30 40 50 60\nLV/uni00ADVIS/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n8082848688909294J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n100 200 300 400 500 600 700\nPUMaVOS/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n455055606570758085J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n20 40 60 80 100 120\nUVO/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n7072747678808284J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n10 20 30 40 50 60\nVIPSeg/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n7880828486889092J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n20 40 60 80 100 120\nVirtual/uni00A0KITTI/uni00A02/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n556065707580J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n20 40 60 80 100 120\nVOST/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n4050607080J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie(a)J&Fperformance on each dataset with different number of interacted frames (3-click)\nEndoVis Virtual\nMethod 2018 ESD LVOSv2 LV-VIS PUMaVOS UVO VIPSeg KITTI 2 VOST (average)\nSAM + XMem++ 68.9 88.2 72.1 86.4 60.2 74.5 84.2 63.8 46.6 71.7\nSAM + Cutie 71.8 87.6 82.1 87.1 59.4 75.2 84.4 70.3 54.3 74.7\nSAM 2 78.4 89.6 87.3 90.4 70.6 79.8 89.0 74.7 66.6 80.7\n(b) average J&Fon each dataset over 8 interacted frames (3-click)\nFigure 12 Zero-shot performance of SAM 2 vs baselines (SAM+XMem++ and SAM+Cutie) under interactive offline\nevaluation with different numbers of interacted frames, using 3 clicks per interacted frame. See §E.1.2 for details.\n•The annotation time for an object is (Texam·(L/300) + Tloc+Tclick·Nclick)·Nframein offline mode and\nTexam·(L/300) +(Tloc+Tclick·Nclick)·Nframein online mode, where Lis the total frame number in\nthe video, Nframe = 1, .. . , 8is the number of frames annotated (i.e., the number of interactive rounds),\nandNclick= 3is the number of clicks per frame.2\nWe show per-dataset results of SAM 2 and the two baselines (SAM+XMem++ and SAM+Cutie, see their\ndetails below) for interactive offline and online evaluation in Fig. 12 and Fig. 13. SAM 2 outperforms both\nbaselines with a notable margin on all datasets and settings.\nE.1.3 Semi-supervised VOS evaluation details\nIn §6.1.2, we also compare with previous video tracking methods under the semi-supervised VOS setting\n(Pont-Tuset et al., 2017), where prompts (which can be foreground/background clicks, bounding boxes, or\nground-truth object masks) are provided only on the first frame of the video. When using click prompts, we\ninteractively sample either 1, 3 or 5 clicks on the first video frame, and then track the object based on these\nclicks. Following the click-based evaluation in prior work (Kirillov et al., 2023; Sofiiuk et al., 2022), the initial\nclick is placed on the object center and subsequent clicks are obtained from the center of the error region.\nSimilar to the interactive setting, here we also use SAM+XMem++ and SAM+Cutie as two baselines. For\nclick or box prompts, SAM is first used to handle click or bounding box inputs, and its output mask is then\n2We note that this estimation does not account for the model’s tracking FPS. The intuition is that human annotators can\nonly examine the results at a lower speed, and therefore the model’s tracking time is covered by Texam.\n23\n\n15 20 25 30 35 40 45 50 55\nEndoVis/uni00A02018/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n606570758085J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n20 25 30 35 40 45 50 55 60\nESD/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n848586878889909192J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n55 60 65 70 75 80 85 90\nLVOSv2/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n6065707580859095J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n10 15 20 25 30 35 40 45\nLV/uni00ADVIS/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n8082848688909294J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n95 100 105 110 115 120 125 130\nPUMaVOS/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n455055606570758085J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n15 20 25 30 35 40 45 50\nUVO/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n7072747678808284J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n10 15 20 25 30 35 40 45\nVIPSeg/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n7880828486889092J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n15 20 25 30 35 40 45 50 55\nVirtual/uni00A0KITTI/uni00A02/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n556065707580J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie\n15 20 25 30 35 40 45 50 55\nVOST/uni00A0 /uni00A0annotation/uni00A0time/uni00A0(sec)/uni00A0with/uni00A03/uni00ADclick\n4050607080J&F\nSAM/uni00A02\nSAM/uni00A0+/uni00A0XMem++\nSAM/uni00A0+/uni00A0Cutie(a)J&Fperformance on each dataset with different number of interacted frames (3-click)\nEndoVis Virtual\nMethod 2018 ESD LVOSv2 LV-VIS PUMaVOS UVO VIPSeg KITTI 2 VOST (average)\nSAM + XMem++ 71.4 87.8 72.9 85.2 63.7 74.7 82.5 63.9 52.7 72.8\nSAM + Cutie 70.5 87.3 80.6 86.0 58.9 75.2 82.1 70.4 54.6 74.0\nSAM 2 77.8 88.5 85.8 88.7 74.2 79.0 86.1 74.3 63.3 79.7\n(b) average J&Fon each dataset over 8 interacted frames (3-click)\nFigure 13 Zero-shot performance of SAM 2 vs baselines (SAM+XMem++ and SAM+Cutie) under interactive online\nevaluation with different numbers of interacted frames, using 3 clicks per interacted frame. See §E.1.2 for details.\nused as input to XMem++ or Cutie. For mask prompts, the ground-truth object masks on the first frame\nare directly used as input to XMem++ and Cutie – this is the standard semi-supervised VOS setting and\nevaluates XMem++ and Cutie without using SAM.\nIn this setting, we evaluate on all 17 zero-shot video datasets in §E.1.1. If a dataset does not follow the\nstandard VOS format, we preprocess it into a format similar to MOSE (Ding et al., 2023). During processing,\nwe ensure that all objects in each video have a valid non-empty segmentation mask on the first frame to be\ncompatible with semi-supervised VOS evaluation. In case an object doesn’t appear in the first frame, we\ncreate a separate video for it starting from the first frame where the object appears.\nWe report the standard J&Fmetric (Pont-Tuset et al., 2017) for this evaluation. If a dataset provides an\nofficial evaluation toolkit, we use it for evaluation (on the VOST dataset, we report the Jmetric instead,\nfollowing its official protocol (Tokmakov et al., 2022)). The results are shown in Table 4, where SAM 2\nperforms better than both baselines on the majority of the 17 datasets across different types of prompts.\nWe show per-dataset results of SAM 2 and the two baselines (SAM+XMem++ and SAM+Cutie, see their\ndetails below) for semi-supervised VOS evaluation in Fig. 14. SAM 2 outperforms both baselines on the\nmajority of these datasets across different types of prompts.\nE.1.4 SAM+XMem++ and SAM+Cutie baseline details\nWe adopt SAM+XMem++ and SAM+Cutie as two baselines for promptable video segmentation, where the\nclick (or box) prompts are first processed by SAM to obtain an object mask, and then XMem++ / Cutie\n24\n\n20406080100\nEndoVis 2018 ESD LVOSv2 LV-VIS PUMaVOS UVO VIPSeg Virtual    \nKITTI 2VOST Wildfires Cityscapes Ego-Exo4D FBMS HT1080WT \nCellsLindenthal \nCameraDrosophila \nHeartVISORSAM + XMem++ (3-click on 1st frame) SAM + Cutie (3-click on 1st frame) SAM 2 (3-click on 1st frame)\n20406080100\nEndoVis 2018 ESD LVOSv2 LV-VIS PUMaVOS UVO VIPSeg Virtual    \nKITTI 2VOST Wildfires Cityscapes Ego-Exo4D FBMS HT1080WT \nCellsLindenthal \nCameraDrosophila \nHeartVISORSAM + XMem++ (5-click on 1st frame) SAM + Cutie (5-click on 1st frame) SAM 2 (5-click on 1st frame)\n20406080100\nEndoVis 2018 ESD LVOSv2 LV-VIS PUMaVOS UVO VIPSeg Virtual    \nKITTI 2VOST Wildfires Cityscapes Ego-Exo4D FBMS HT1080WT \nCellsLindenthal \nCameraDrosophila \nHeartVISORSAM + XMem++ (bounding box on 1st frame) SAM + Cutie (bounding box on 1st frame) SAM 2 (bounding box on 1st frame)\n20406080100\nEndoVis 2018 ESD LVOSv2 LV-VIS PUMaVOS UVO VIPSeg Virtual    \nKITTI 2VOST Wildfires Cityscapes Ego-Exo4D FBMS HT1080WT \nCellsLindenthal \nCameraDrosophila \nHeartVISORXMem++ (ground-truth mask on 1st frame) Cutie (ground-truth mask on 1st frame) SAM 2 (ground-truth mask on 1st frame)\n20406080100\nEndoVis 2018 ESD LVOSv2 LV-VIS PUMaVOS UVO VIPSeg Virtual    \nKITTI 2VOST Wildfires Cityscapes Ego-Exo4D FBMS HT1080WT \nCellsLindenthal \nCameraDrosophila \nHeartVISORSAM + XMem++ (1-click on 1st frame) SAM + Cutie (1-click on 1st frame) SAM 2 (1-click on 1st frame)Figure 14 Zero-shot performance on 17 video datasets of SAM 2 vs two baselines (SAM+XMem++ and SAM+Cutie)\nunder semi-supervised VOS evaluation using different prompts (1, 3 or 5 clicks, bounding boxes, or ground-truth masks\non the first video frame), with the averaged performance across datasets for each type of prompt shown in Table 4 in\nthe main text. See §E.1.3 for details.\nmodels track this SAM mask across the video to obtain the final masklet. In these two baselines, SAM can be\nused to provide both an initialobject mask on the first frame, or to correctan existing object mask output\nby XMem++ or Cutie. This is used for subsequent interacted frames during interactive offline and online\nevaluation, where new positive and negative clicks are provided as corrections over an existing mask.\nWhen using SAM to apply a correction over an existing mask prediction in a given frame, we follow the\nstrategy in EVA-VOS (Delatolas et al., 2024) to first initialize SAM with the XMem++ or Cutie output mask\nbefore incorporating the new correction clicks. Specifically, we first reconstruct the XMem++ or Cutie output\nmask in SAM by sampling clicks from them and feeding them as inputs to SAM until the reconstructed mask\nin SAM reaches IoU>0.8with the XMem++ or Cutie output mask. Then, to incorporate new positive\nand negative clicks for correction, we concatenate these additional correction clicks with the initial clicks\nsampled during mask construction, and feed the joint concatenated list as input into SAM to obtain the\n25\n\nfinal corrected masks. We find that this strategy works better than several alternatives (such as feeding the\nXMem++ or Cutie output mask as a mask prompt together with new correction clicks into SAM, or taking\nonly the correction clicks as inputs to SAM while ignoring the XMem++ or Cutie output mask).\nE.2 DAVIS interactive benchmark\nWe also evaluate on the DAVIS interactive benchmark (Caelles et al., 2018), which resembles our interactive\noffline evaluation in §6.1.1, where in each round of interaction, the evaluation server would provide new\nannotations on frames with the worst segmentation performance. The official DAVIS eval toolkit provides\nscribble prompts during interactions, while other work such as CiVOS (Vujasinović et al., 2022) has also\nextended this to cover click prompts.\nHere we follow CiVOS to use positive and negative clicks as input prompts and adopt the same strategy\nfor click sampling. We report the J&F@60s and AUC- J&Fmetrics on this benchmark as provided by its\nevaluator, and compare to two baselines: MiVOS (Cheng et al., 2021b), which directly uses the provided\nscribbles via a scribble-to-mask module (and is also extended to click prompts in Vujasinović et al. (2022)),\nand CiVOS, which samples click from the provided scribbles. The results are shown in Table 14, where SAM\n2 (based on click inputs) outperforms both baselines under click inputs. We note that SAM 2 often tends\nto segment object parts (e.g. a person’s arm) on the first click while the DAVIS dataset mainly contains\nwhole objects (e.g. an entire person), which could penalize SAM 2’s J&Fperformance on this benchmark.\nWe verified this by observing better accuracy (0.86 AUC- J&Fand 0.89 J&F@60s with click input) for an\nearlier model trained on fewer part annotations.\nMethod input type AUC- J&F J&F@60s\nMiVOS (Cheng et al., 2021b) scribbles 0.87 0.88\nMiVOS (Cheng et al., 2021b)‡clicks 0.75 0.75\nCiVOS (Vujasinović et al., 2022) clicks 0.83 0.84\nSAM 2 clicks 0.84 0.87\nTable 14 Performance of SAM 2 and other models on the DAVIS interactive benchmark. For SAM 2, we use clicks as\ninputs following the click sampling strategy from CiVOS (Vujasinović et al., 2022). See §E.2 for details (‡: performance\nreported in Vujasinović et al. (2022)).\nE.3 Zero-shot image tasks\nE.3.1 Dataset details\nFor the interactive segmentation task, we evaluated SAM 2 on a comprehensive suite of 37 datasets. This\nsuite includes the 23 datasets previously used by SAM for zero-shot evaluation. For completeness, we list\nthe 23 datasets: LVIS (Gupta et al., 2019), ADE20K (Zhou et al., 2019), Hypersim (Roberts et al., 2021),\nCityscapes (Cordts et al., 2016), BBBC038v1 (Caicedo et al., 2019), DOORS (Pugliatti & Topputo, 2022),\nDRAM (Cohen et al., 2022), EgoHOS (Zhang et al., 2022), GTEA (Fathi et al., 2011; Li et al., 2015), iShape\n(Yang et al., 2021a), NDD20 (Trotter et al., 2020), NDISPark (Ciampi et al., 2021, 2022), OVIS (Qi et al.,\n2022), PPDLS (Minervini et al., 2016), Plittersdorf (Haucke et al., 2022), STREETS (Snyder & Do, 2019),\nTimberSeg (Fortin et al., 2022), TrashCan (Hong et al., 2020), VISOR (Darkhalil et al., 2022; Damen et al.,\n2022), WoodScape (Yogamani et al., 2019), PIDRay (Wang et al., 2021a), ZeroWaste-f (Bashkirova et al.,\n2022), and IBD (Chen et al., 2022). For more detailed information about these datasets, we refer the reader\nto Kirillov et al. (2023). In addition to these 23 datasets, we evaluated on frames sampled from 14 video\ndatasets to assess SAM 2’s performance on images from the video domain. The video datasets used are listed\nas follows: Lindenthal Camera Traps (LCT) (Haucke & Steinhage, 2021), VOST (Tokmakov et al., 2022),\nLV-VIS (Wang et al., 2023), FBMS (Brox et al., 2010), Virtual KITTI 2 (Cabon et al., 2020), Corsican Fire\nDatabase (CFD) (Toulouse et al., 2017), VIPSeg (Miao et al., 2022), Drosophila Heart OCM (DH OCM)\n(Fishman et al., 2023), EndoVis 2018 (Allan et al., 2020), ESD (Huang et al., 2023), UVO (Wang et al.,\n2021b), Ego-Exo4d (Grauman et al., 2023), LVOSv2 (Hong et al., 2024), and HT1080WT (Gómez-de Mariscal\net al., 2021). Table 16 has a more detailed description of these datasets. (Some of these datasets are obtained\nfrom the same data source as the zero-shot video datasets in §E.1.1.)\n26\n\nE.3.2 Detailed zero-shot experiments\nIn this section, we include a more detailed version of the experiments in §6.2. We compare SAM 2 to SAM\nand HQ-SAM with different model sizes in Table 15. The main metrics we use for evaluation are the 1- and\n5-click mIoU and we categorize the results by the dataset domain.\nTable 15 first shows a comparison of the models trained only on images (for the SA task) with different\nimage encoder sizes on both the SA-23 benchmark as well as the 14 newly introduced video datasets. SAM 2\n(Hiera-B+) trained only on SA-1B outperforms SAM (ViT-H) on 1-click accuracy, and both SAM (ViT-H)\nand HQ-SAM (ViT-H) on 5-click accuracy while being 6x faster. SAM 2 (Hiera-L) further improves the 1-click\naccuracy by 1 point on average, but trading off speed. Despite being slower than Hiera-B+, it is still 3.4x\nfaster than SAM (ViT-H) and 1.5x faster than SAM (ViT-B).\nThe last two rows in Table 15 illustrate the benefits of training with our mix of image and video data, which\nboosts the average accuracy to 61.4% across the 23 datasets with the Hirea-B+ image encoder. Additionally,\nwe observe substantial improvements on the video benchmarks of SA-23 as well as the 14 newly introduced\nvideo datasets. We note that we do not scale beyond Hiera-L, but expect better performance for a larger\nmodel.\n1 (5) click mIoU\nModel Data SA-23 All SA-23 Image SA-23 Video 14 new Video FPS\nSAM (ViT-B) SA-1B 55.9 (80.9) 57.4 (81.3) 54.0 (80.4) 54.5 (82.6) 76.7\nSAM (ViT-H) SA-1B 58.1 (81.3) 60.8 (82.1) 54.5 (80.3) 59.1 (83.4) 21.7\nHQ-SAM (ViT-B) HQSEG-44k 53.9 (72.1) 56.3 (73.9) 50.7 (69.9) 54.5 (75.0) 73.5\nHQ-SAM (ViT-H) HQSEG-44k 59.1 (79.8) 61.8 (80.5) 55.7 (78.9) 58.9 (81.6) 21.4\nSAM 2(Hiera-B+) SA-1B 58.9 (81.7) 60.8 (82.1) 56.4 (81.2) 56.6 (83.7) 130.1\nSAM 2(Hiera-L) SA-1B 60.0 (81.8) 62.0 (82.2) 57.4 (81.2) 58.5 (83.8) 61.4\nSAM 2(Hiera-B+) our mix 61.9 (83.6) 63.2 (83.8) 60.3 (83.3) 69.9 (85.9) 130.1\nSAM 2(Hiera-L) our mix 63.5(83.5) 64.3(83.7) 62.4(83.2) 71.2(85.6) 61.4\nTable 15 Zero-shot performance on the Segment Anything (SA) task across a suite of 37 datasets. The table shows\nthe average 1- and 5- click mIoU of SAM 2 compared to two baselines, categorized by dataset domain. We report\nthe average metrics on the 23 datasets used by SAM for zero-shot evaluation, as well as the average across 14 newly\nintroduced zero-shot video benchmarks.\nA breakdown of the accuracy across datasets is presented in Fig. 15, where the per-dataset delta in 1-click\nmIoU relative to SAM is color-coded to indicate the data type (image or video). Notably, SAM 2 (Hiera-B+)\nsurpasses SAM on 29 datasets3by up to 53.9 mIoU, despite using a smaller Hiera-B+ image encoder.\nF Details on comparison to state-of-the-art in semi-supervised VOS\nWe provide additional details on the comparison to the previous state-of-the-art in semi-supervised VOS (§7).\nWe include results from SAM 2 trained only on SA-1B, SA-V and Internal data, for different encoder sizes.\nQualitative comparison: In Fig. 16, we show a comparison between our baseline (Cutie-base+, top row) and\nour model (SAM 2, bottom row) when prompted with a mask in the first frame. While the mask prompt in\nthe first frame only covers the shirt of the person, the masklet predicted by the baseline wrongfully propagates\nto the whole person. Our model, however, is able to restrict the masklet to the target object.\nQuantitative comparison: In Table 17, we compare the performance of our model to previous approaches on\nadditional semi-supervised VOS metrics. SAM 2 outperforms prior work on all evaluated benchmarks, in all\nmetrics. Note that unlike these previous approaches, SAM 2 is not specialized in the semi-supervised VOS\ntask but is capable of more general promptable segmentation. SAM 2 is also not restricted to a specific set of\nobject classes. The performance of our model on the SA-V benchmark (Table 17a) demonstrates its capability\nto segment anything in a video.\n3We note that OVIS is not strictly zero-shot for SAM 2 since its videos are used in MOSE (part of our training data mix).\n27\n\n0 10 20 30 40 50\n1 click mIoU deltaNDISParkHT108WTPPDLSCityscapesTimberSegWoodScapeLVISHypersimADE20KBBBC038v1NDD20IBDEgo-Exo4DVOSTVirtual KITTISTREETSPlittersdorfCFDiShapeLCTZeroWaste-fVISORLV-VISDOORSVIPSegPIDRayDRAMESDUVOEgoHOSGTEATrashCanLVOSv2OVISFBMSEndoVisDH OCM\n-6.2-4.2-2.2-2.1-1.6-1.4-1.2-1.11.02.12.62.72.72.82.93.03.84.44.74.85.25.35.95.95.97.18.49.19.210.111.814.316.017.118.318.753.9\nVideo domain\nImage domain\n1Figure 15 Zero-shot performance of SAM 2 vs SAM on a suite of 37 datasets. The figure shows the center 1 click mIoU\ndelta between SAM 2 and SAM. Datasets derived from video distribution are highlighted in red, while those from\nimage distribution are highlighted in blue.\noursbaseline\nFigure 16 Comparison between our baseline (Cutie-base+, top row) and our model (SAM 2, bottom row) when\nprompted with a mask in the first frame.\n28\n\ndatasetabbreviation\n& linkvideo\ntypedescriptionannotation\ntypesource split# videos\nsampled# masklets\nsampled# frames\nsampled# masks\nsampled\nLV-VIS (Wang et al., 2023) LV-VIS Open VocabularyLarge scale open vocabulary video instance\nsegmentationDense Validation 690 2536 15,604 54,077\nA Drosophila heart optical\ncoherence microscopy\ndatabase for automatic\nvideo\nsegmentation (Fishman\net al., 2023)DH OCM Microscopy; heartSegmentation of a fruit fly heart in optical coherence\nmicroscopy videosSparse All 213 213 608,000 607,158\nVideo Object\nSegmentation under Trans-\nformations (Tokmakov\net al., 2022)VOST DeformationVideo object segmentation with emphasis on shape\ntransformationsDense Validation 635 882 67,004 89,722\nCityscapes-VPS (Cordts\net al., 2016; Kim et al.,\n2020)Cityscapes-VPS Driving Panoptic segmentation for Cityscapes driving dataset Sparse Validation Instance 209 1372 4,259 4,579\nCorsican Fire\nDatabase (Toulouse et al.,\n2017)CFDWildfire Segmentation of wildfires Sparse All 5 5 541 541\nPartial and Unusual Masks\nfor Video Object\nSegmentation (Bekuzarov\net al., 2023)PUMaVOS PartsVideo object segmentation with a focus on parts and\npractical use casesDense All 24 26 21,187 21,485\nEPIC-KITCHENS\nVISOR (Darkhalil et al.,\n2022)VISOR EgocentricVideo object segmentation benchmark containing\negocentric videos of cooking with an emphasis on\nsegmenting active objects.Sparse Validation 921 921 736,030 4,426\nHT1080WT cells\nembedded in 3D collagen\ntype I matrices (Gómez-de\nMariscal et al., 2021)HT1080WT Microscopy; cells Timelapse videos of HT1080WT cell movement Sparse All 60 150 1,010 2,694\nFreiburg-Berkeley Motion\nSegmentation\nDataset (Brox et al., 2010)FBMS Moving Object Precise segmentation of moving objects Sparse All 45 70 9734 755\nVirtual KITTI 2 (Cabon\net al., 2020)Virtual KITTI 2 Synthetic; DrivingSynthetic driving videos generated by a game engine\nthat recreate real world KITTI videos.DenseAll from video angle\nCamera_0996 1,638 109,368 162,708\nEndoVis 2018 (Allan et al.,\n2020)EndoVis 2018 Endoscopic video; surgery Segmentation of medical tools in endoscopic videos Dense All 15 29 2,325 4,314\nLindenthal Camera\nTraps (Haucke &\nSteinhage, 2021)LCTStereo Wildlife videos captured using stereo cameras. Sparse All 12 12 4,012 412\nLVOSv2 (Hong et al.,\n2024)LVOSv2 Long videosLong-term video object segmentation benchmark, on\naverage 1.14 minutesDense Validation 136 225 64,523 91,510\nUVO (Wang et al., 2021b) UVO Open WorldOpen World instance segmentation of all objects in a\nvideoDense Validation 54 311 4,860 26,747\nEgoExo4d (Grauman et al.,\n2023)EgoExo4d EgocentricEgocentric videos of participants completing skilled\nactivities.SparseValidation videos on\negocentric cameras1185 1185 327,080 9,035\nVIPSeg (Miao et al., 2022) VIPSeg PanopticLarge scale and real world scenarios for video panoptic\nsegmentationDense Validation 152 1,457 3,416 30,408\nEvent-based Segmentation\nDataset (Huang et al.,\n2023)ESDClutterTabletop object segmentation in an indoor cluttered\nenvironmentDense All 135 814 13,325 78,243\nTable 16 Video segmentation datasets used for zero-shot evaluation.\nUVO Ego-Exo4d LVOSv2 EndoVis 2018 LCT VIPSeg\nVirtual KITTI 2 ESD FBMS HT1080WT VISOR\nPUMaVOS CFD Cityscapes-VPS VOST DH OCM LV-VIS\nFigure 17 Examples from SAM 2 zero-shot video benchmark suite.\n29\n\nSA-V val SA-V test\nMethod J&F J F J&F J F\nSTCN (Cheng et al., 2021a) 61.057.464.562.559.066.0\nSwinB-AOT-L (Yang et al., 2021b) 51.146.455.750.346.054.6\nSwinB-DeAOT-L (Yang & Yang, 2022) 61.456.666.261.857.266.3\nRDE (Li et al., 2022a) 51.848.455.253.950.557.3\nXMem (Cheng & Schwing, 2022) 60.156.363.962.358.965.8\nSimVOS-B (Wu et al., 2023b) 44.240.048.344.140.547.7\nDEVA (Cheng et al., 2023b) 55.451.559.256.252.460.1\nCutie-base (Cheng et al., 2023a) 60.757.763.762.759.765.7\nCutie-base+ (Cheng et al., 2023a) 61.358.364.462.859.865.8\nSAM 2 (Hiera-B+) 73.670.476.974.170.677.5\nSAM 2 (Hiera-L) 75.672.378.9 77.6 74.0 81.1\nSAM 2 (Hiera-T)‡73.770.377.175.071.578.5\nSAM 2 (Hiera-S)‡72.769.476.074.971.478.4\nSAM 2 (Hiera-B+)‡75.371.878.774.771.278.2\nSAM 2 (Hiera-L)‡76.1 72.9 79.276.072.679.3\n(a)Comparisons between SAM 2 and previous work on our SA-V\nbenchmark for the semi-supervised VOS task. We evaluated prior works\non SA-V using their open-sourced code and checkpoints.\nLVOS val\nMethod J&F J F\nDEVA (Cheng et al., 2023b) 55.951.160.7\nDDMemory (Hong et al., 2023) 60.755.066.3\nCutie-base (Cheng et al., 2023a) 66.061.370.6\nSAM 2 (Hiera-B+) 74.970.279.6\nSAM 2 (Hiera-L) 76.171.680.6\nSAM 2 (Hiera-T)‡73.668.878.3\nSAM 2 (Hiera-S)‡73.568.678.4\nSAM 2 (Hiera-B+)‡76.271.680.7\nSAM 2 (Hiera-L)‡77.9 73.1 82.7\n(b)Comparisons between SAM 2 and previous\nwork on the LVOS (Hong et al., 2023) benchmark.LVOSv2 val\nMethod J&F J sFsJuFu\nSTCN (Cheng et al., 2021a) 60.657.264.057.563.8\nRDE (Li et al., 2022a) 62.256.764.160.867.2\nSwinB-DeAOT (Yang & Yang, 2022) 63.961.569.058.466.6\nXMem (Cheng & Schwing, 2022) 64.562.669.160.665.6\nSAM 2 (Hiera-B+) 75.878.985.365.173.8\nSAM 2 (Hiera-L) 78.178.985.269.778.6\nSAM 2 (Hiera-T)‡75.375.281.867.776.4\nSAM 2 (Hiera-S)‡76.477.184.067.577.0\nSAM 2 (Hiera-B+)‡75.877.083.467.075.6\nSAM 2 (Hiera-L)‡79.8 80.0 86.6 71.6 81.1\n(c)Comparisons between SAM 2 and previous work on the\nLVOSv2 (Hong et al., 2024) benchmark. We report the perfor-\nmance of prior works as evaluated by the LVOSv2 authors.\nMOSE val DAVIS17 val DAVIS17 test YTVOS19 val\nMethod J&F J F J&F J F J&F J F G J sFsJuFu\nSTCN (Cheng et al., 2021a) 52.548.556.685.482.288.676.172.779.682.781.185.478.285.9\nSwinB-AOT-L (Yang et al., 2021b) 59.455.563.285.482.488.481.277.385.184.584.088.878.486.7\nSwinB-DeAOT-L (Yang & Yang, 2022) 59.955.764.086.283.189.282.878.986.786.185.390.280.488.6\nRDE (Li et al., 2022a) 46.842.451.384.280.887.577.473.681.281.981.185.576.284.8\nXMem (Cheng & Schwing, 2022) 59.655.463.786.082.889.279.676.183.085.684.188.581.088.9\nSimVOS-B (Wu et al., 2023b) - - - 88.085.091.080.476.184.684.283.1 - 79.1 -\nJointFormer (Zhang et al., 2023b) - - - 90.187.093.288.184.791.687.486.590.982.090.3\nISVOS (Wang et al., 2022) - - - 88.284.591.984.080.187.886.385.289.781.089.1\nDEVA (Cheng et al., 2023b) 66.061.870.387.083.890.282.678.986.485.484.989.479.687.8\nCutie-base (Cheng et al., 2023a) 69.965.874.187.984.691.186.182.489.987.086.090.582.089.6\nCutie-base+ (Cheng et al., 2023a) 71.767.675.888.185.590.888.184.791.487.586.390.682.790.5\nSAM 2 (Hiera-B+) 75.871.879.990.987.794.188.385.091.588.487.191.783.391.4\nSAM 2 (Hiera-L) 77.2 73.3 81.2 91.6 88.3 94.9 89.0 85.8 92.2 89.1 87.5 92.0 84.5 92.4\nSAM 2 (Hiera-T)‡70.966.775.289.285.792.786.783.390.087.485.590.083.091.2\nSAM 2 (Hiera-S)‡71.567.375.688.885.492.286.382.889.987.585.790.183.291.2\nSAM 2 (Hiera-B+)‡72.868.876.988.985.592.286.783.290.187.986.290.783.291.4\nSAM 2 (Hiera-L)‡74.670.678.689.385.792.888.184.691.688.686.691.384.292.2\n(d)Comparisons between SAM 2 and previous work on the semi-supervised VOS task.\nTable 17 Detailed comparisons between SAM 2 and previous work on various benchmarks (‡: a version of the model\ntrained on SA-1B, SA-V, and our internal dataset as described in §5.2).\n30\n\nG Model, data and annotation cards\nG.1 Model card\nModel Overview\nName SAM 2 (Segment Anything Model 2)\nVersion 1.0\nDate2024\nOrganization Meta FAIR\nMode type Promptable segmentation model\nArchitecture See Section 4\nRepository https://github.com/facebookresearch/segment-anything-2\nLicense Apache 2.0\nIntended Use\nPrimary intended users SAM 2 was designed as a unified model for promptable video and image segmenta-\ntion tasks. The model was primarily developed for research use cases. SAM 2 is\nreleased under an Apache 2.0 license.\nOut-of-scope use cases See Ethical considerations and license for restrictions.\nCaveats and recommendations See Appendix B for limitations.\nRelevant Factors\nGroups SAM 2 is class agnostic and was designed for promptable image and video segmen-\ntation. It can segment and track any object.\nInstrumentation and environment SAM 2 was evaluated across a variety of types of video and image data. The video\nbenchmark suite included domains such as driving data, microscopy, egocentric\nvideo, robotic surgery . See Table 16 for descriptions of the benchmarks and\nFigure 17 for example frames. SAM 2 was evaluated on the same suite of image\nbenchmarks as Kirillov et al. (2023), which covers domains including underwater\nimages, paintings, fish-eye images .\nMetrics\nWe evaluate the performance of SAM 2 using the following metrics:\nJ&F: We evaluate performance using J&F(Pont-Tuset et al., 2017) for the\npromptable video segmentation and semi-supervised VOS tasks.\nG: We use Gfor evaluation on YTVOS 2019 for the semi-supervised VOS task.\nmIoU: We evaluate performance using mIoU for the promptable image segmentation\ntask.\nEvaluation Data\nData sources See Appendix E\nTraining Data\nData source SAM 2 was trained on the SA-V dataset alongside internally available licensed\nvideo data. See Section 5 of the main text for more details and Appendix G.2 for\nthe SA-V dataset data card.\nEthical Considerations\nDataSee Section 5 for more details about the SAM 2 training data. In Section 5.2 we\nshow a geographic distribution of the videos and demographic distribution of the\ncrowdworkers who collected the videos in the SA-V dataset.\nCost and impact of compute The released SAM 2 was trained on 256 A100 GPUs for 108 hours. This corresponds\nto 12165.12 kWH and an estimated emissions of 3.89 metric tons of CO2e (Patterson\net al., 2021; Lacoste et al., 2019). The emissions from training the released SAM\n2 are equivalent to ∼10k miles driven by an average gasoline-powered passenger\nvehicle (Agency, 2022).\nRisks and harms In Section 6.1.3 of the main text we analyze SAM 2 performance on people\nacross demographic groups. When using SAM 2 in new settings, we suggest that\nresearchers perform their own fairness evaluation for SAM 2 specific to their use\ncase.\nUse cases We implore users to use their best judgement.\nTable 18 Model card for SAM 2 following the structure in Mitchell et al. (2019)\n31\n\nG.2 Dataset card for SA-V dataset\nMotivation\n1.For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled?\nPlease provide a description. The dataset was designed for the PVS task. The contributions of our dataset to the vision community\nare: (1) The dataset, composed of 50.9K videos and 642.6K masklets, is the largest video segmentation dataset publicly available\ntoday (see 5.2 for comparisons to current VOS datasets) (2) The dataset is available under a Creative Commons Attribution 4.0\nInternational Public License at https://ai.meta.com/datasets/segment-anything-video/, (3) The data is a more geographically diverse,\npublicly available, video segmentation dataset than its predecessors.\n2.Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\nThe dataset was created by Meta FAIR. The underlying videos were collected via a contracted third party company.\n3.Who funded the creation of the dataset? The dataset was funded by Meta FAIR.\n4.Any other comments? No.\nComposition\n1.What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of\ninstances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\nAll of the instances in the dataset are videos. Subject matter diversity was encouraged and no specific themes were applied during\nvideo collection. Common themes of the video include: locations, objects, scenes. All the videos are distinct, however there are some\nsets of videos that were taken of the same subject matter.\n2.How many instances are there in total (of each type, if appropriate)? There are 50.9K videos.\n3.Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the\ndataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so,\nplease describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why\nnot (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). While the dataset contains\nall possible instances, reviewers were advised to refuse to annotate content containing explicit imagery.\n4.What data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or features? In either case, please provide\na description. Each instance is a video.\n5.Is there a label or target associated with each instance? If so, please provide a description. Each video is annotated with masklets\nthat track objects throughout the video. There are no categories or text associated with the masklets. The data was annotated at 6\nFPS. There are an average of 3.8 manual masklets, and 8.9 auto masklets per video, and there are 642.6K masklets in total.\n6.Is any information missing from individual instances? If so, please provide a description, explaining why this information is\nmissing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g.,\nredacted text. No.\n7.Are relationships between individual instances made explicit (e.g., users’ movie ratings, social network links)? If so, please\ndescribe how these relationships are made explicit. No.\n8.Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. For manual masklets,\nhuman errors may exist; for example, annotators may miss a frame to check or fix when needed. For auto masklets, as SAM 2 is used\nto generates them, model errors such as inconsistencies in the masklets may exist.\n9.Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it\nlinks to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there\nofficial archival versions of the complete dataset (e.g., including the external resources as they existed at the time the dataset was\ncreated); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a\ndataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links\nor other access points, as appropriate. The dataset is self contained.\n10.Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient\nconfidentiality, data that includes the content of individuals’ non-public communications)? If so, please provide a description. No.\n11.Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\nIf so, please describe why. We have three safety measures to prevent objectionable content: (1) The video collecting crowdworkers\nwere provided instructions to not record videos that might contain objectionable content (e.g., graphic, nudity, or inappropriate\ncontent). (2) The expert annotators who annotated the videos were provided instructions to flag and reject videos if objectionable\ncontent was present. (3) reports about video(s) in the dataset can be submitted to segment-anything@meta.com.\n12.Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified\nand provide a description of their respective distributions within the dataset. The dataset does not identify any subpopulations of\nthe people in the videos. The demographics of the crowdworkers who collected the videos in the dataset are presented in 5.2.\n13.Is it possible to identify individuals (i.e, one or more natural persons), either directly or indirectly (i.e., in combination with\nother data) from the dataset? If so, please describe how. Videos were subjected to a face blurring model. Reports about videos in\nthe dataset can be submitted to segment-anything@meta.com.\n14.Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual\norientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic\ndata; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.\nThe dataset is not focused on data that may be considered sensitive. Reports about videos in the dataset can be submitted to\nsegment-anything@meta.com.\n32\n\n15.Any other comments? No.\nCollection Process\n1.How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings),\nreported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based\nguesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data\nvalidated/verified? If so, please describe how. The released masklets associated with each video were collected using two methods.\n(1) SAM 2 assisted manual annotation (2) automatically generated by SAM 2 and verified by annotators.\n2.What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation,\nsoftware programs, software APIs)? How were these mechanisms or procedures validated? The videos in the dataset were collected\nvia a contracted third-party vendor. They are videos taken by crowdworkers with unknown equipment.\n3.If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling\nprobabilities)? N/A.\n4.Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g.,\nhow much were crowdworkers paid)? (1) The videos in the dataset were collected via a contracted third-party vendor. They are\nvideos taken by crowdworkers who were compensated with an hourly wage set by the vendor. (2) The manually collected masklets in\nthe dataset were collected by annotators via another third-party vendor. Annotators were compensated with an hourly wage set by\nthe vendor.\n5.Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the\ninstances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the\ninstances was created. The videos were filmed between November 2023 and March 2024. The masklet annotations were collected\nbetween April 2024 and July 2024.\n6.Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these\nreview processes, including the outcomes, as well as a link or other access point to any supporting documentation. If the dataset\ndoes not relate to people, you may skip the remaining questions in this section. The project underwent an internal review process.\n7.Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g. websites)?\nWe contracted with third-party vendors to collect the videos and to generate or review annotations.\n8.Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other\ninformation) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of\nthe notification itself. The videos were collected by crowdworkers via a contracted third-party vendor. The crowdworkers agreed to\nconsent forms.\n9.Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or\nother information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the\nexact language to which the individuals consented. The videos were collected via a contracted third-party who provided appropriate\nrepresentations regarding the collection of any notices and consents as required from individuals.\n10.If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or\nfor certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).\nPursuant to the contract, the contracted third-party collected consents and provided opportunity for consent revocation.\n11.Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been\nconducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to\nany supporting documentation. See detail in 6.1.3.\n12.Any other comments? No.\nPreprocessing / Cleaning / Labeling\n1.Was any preprocessing / cleaning / labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging,\nSIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may\nskip the remaining questions in this section. The videos were re-sampled to 24 fps and converted to mp4 format.\n2.Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so,\nplease provide a link or other access point to the “raw” data. No.\nUses\n1.Has the dataset been used for any tasks already? If so, please provide a description. The dataset has been used to train and\nevaluate SAM 2.\n2.What (other) tasks could the dataset be used for? The data could be used for VOS, iVOS, or PVS tasks. If frames are sampled from\nthe videos, the dataset can be used for the image segmentation task.\n3.Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might\nimpact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in\nunfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks,\nfinancial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or\nharms? We have an analysis of the geography and crowdworker demographic of our dataset in 5.2. While we believe our dataset to\nbe more representative on these factors than most of the publicly existing datasets of its kind at this time, we acknowledge that we do\nnot have parity across all geographic and demographic groups, and we encourage users of the dataset to be mindful of any potential\nbiases models may learn using this dataset.\n33\n\n4.Are there tasks for which the dataset should not be used? If so, please provide a description. No. Full terms of use for the dataset\ncan be found at https://ai.meta.com/datasets/segment-anything-video-downloads/.\n5.Any other comments? No.\nDistribution\n1.Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which\nthe dataset was created? If so, please provide a description. The dataset will be available under the permissive Creative Commons\nAttribution 4.0 International Public License.\n2.How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier\n(DOI)? The dataset is available at https://ai.meta.com/datasets/segment-anything-video/.\n3.When will the dataset be distributed? The dataset will be distributed in July 2024.\n4.Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use\n(ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any\nrelevant licensing terms or ToU, as well as any fees associated with these restrictions. Yes, the dataset will be available under the\nCreative Commons Attribution 4.0 International Public License. The license agreement and terms of use for the dataset can be found\nat https://ai.meta.com/datasets/segment-anything-video-downloads/. Users must agree to the terms of use before downloading or\nusing the dataset.\n5.Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe\nthese restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well\nas any fees associated with these restrictions. Full terms of use and restrictions on use of the SA-V dataset can be found at\nhttps://ai.meta.com/datasets/segment-anything-video-downloads/.\n6.Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these\nrestrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation. The license and\nrestrictions on use of the SA-V dataset can be found at https://ai.meta.com/datasets/segment-anything-video-downloads/.\n7.Any other comments? No.\nMaintenance\n1.Who will be supporting/hosting/maintaining the dataset? The dataset will be hosted at https://ai.meta.com/datasets/segment-\nanything-video/ and maintained by Meta FAIR.\n2.How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Please email segment-anything@meta.com.\n3.Is there an erratum? If so, please provide a link or other access point. No.\n4.Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how\noften, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)? Updates may be made\npursuant to inbound received at segment-anything@meta.com.\n5.If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the\nindividuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe\nthese limits and explain how they will be enforced. There are no limits on data retention.\n6.Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe\nhow its obsolescence will be communicated to dataset consumers. No. If updates are made to the dataset, previous versions will not\ncontinue to be hosted.\n7.If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide\na description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for\ncommunicating/distributing these contributions to dataset consumers? If so, please provide a description. We encourage further\nannotations for SA-V, but these will not be validated/verified or supported/hosted/maintained by Meta.\n8.Any other comments? No.\nG.3 Data annotation card\nTask Formulation\n1.At a high level, what are the subjective aspects of your task? Selecting objects to mask and track in a video is inherently a subjective\ntask, and annotators might differ in their decision to mask objects.\n2.What assumptions do you make about annotators? We assume our annotators understand the PVS task and are well trained on\nvideo related tasks. Our annotators worked full time on our annotation task. This made it possible to train the annotators by sharing\nfeedback on a regular basis.\n3.How did you choose the specific wording of your task instructions? What steps, if any, were taken to verify the clarity of task\ninstructions and wording for annotators? (1) The task instructions included visual examples (images and videos) to provide clarity.\n(2) Annotators were well trained before working on production queues. (3) The research team shared feedback daily and met with the\nannotators weekly for Q&A sessions.\n4.What, if any, risks did your task pose for annotators and were they informed of the risks prior to engagement with the task?\nAnnotators were informed to reject objectionable videos.\n34\n\n5.What are the precise instructions that were provided to annotators? See detail in 11 for annotation instructions.\nSelecting Annotations\n1.Are there certain perspectives that should be privileged? If so, how did you seek these perspectives out? We chose to work with\nannotators with previous video annotation experience.\n2.Are there certain perspectives that would be harmful to include? If so, how did you screen these perspectives out? No.\n3.Were sociodemographic characteristics used to select annotators for your task? If so, please detail the process. For masklet\nannotations, sociodemographic characteristics were not used to select the annotators. For video collection, we emphasized the\nimportance of diversity among the crowdworkers to our third-party vendor. While it was not a strict requirement, we encouraged the\ninclusion of a diverse group of crowdworkers to enrich the data collection process with a wide range of perspectives. This approach\naimed to naturally incorporate diversity without imposing strict selection based on sociodemographic factors.\n4.If you have any aggregated socio-demographic statistics about your annotator pool, please describe. Do you have reason to\nbelieve that sociodemographic characteristics of annotators may have impacted how they annotated the data? Why or why not?\nAggregated socio-demographic statistics about the crowdworkers who collected the videos are presented in 5.2.\n5.Consider the intended context of use of the dataset and the individuals and communities that may be impacted by a model trained\non this dataset. Are these communities represented in your annotator pool? The SA-V dataset is a geographically diverse, publicly\navailable, video segmentation dataset, as discussed in 5.2. In addition, we analyze the responsible AI axes of a model trained on the\ndataset, as discussed in 6.1.3.\nPlatform and Infrastructure Choices\n1.What annotation platform did you utilize? At a high level, what considerations informed your decision to choose this platform?\nDid the chosen platform sufficiently meet the requirements you outlined for annotator pools? Are any aspects not covered? We\nused an internal annotation platform.\n2.What, if any, communication channels did your chosen platform offer to facilitate communication with annotators? How did this\nchannel of communication influence the annotation process and/or resulting annotations? The research team shared feedback daily\nand met with the annotators weekly to align on the task instructions and expectations and to hold Q&A sessions. Outside of those\nsessions, annotators had access to a spreadsheet and chat group to facilitate communication with the research team.\n3.How much were annotators compensated? Did you consider any particular pay standards, when determining their compensation?\nIf so, please describe. (1) The video collecting crowdworkers were compensated with an hourly wage set by the vendor. (2) Annotators\nwere compensated with an hourly wage set by the vendor.\nDataset Analysis and Evaluation\n1.How do you define the quality of annotations in your context, and how did you assess the quality in the dataset you constructed?\nAnnotators were required to follow a training before moving to production queues. Annotators followed a 2-day training session led by\nthe vendor and then were asked to annotate jobs from a training queue. Annotators were able to move from training to production\nafter the vendor Q&A team or the research team reviewed their work and assessed quality. On average, annotators spent 1 - 2 weeks\nin training before moving to production. Similarly, the vendor and research team Q&A manually reviewed the production queues’\nannotations daily, sharing feedback daily.\n2.Have you conducted any analysis on disagreement patterns? If so, what analyses did you use and what were the major findings?\nDid you analyze potential sources of disagreement? The disagreement patterns were shared daily and weekly during feedback and\nQ&A sessions.\n3.How do the individual annotator responses relate to the final labels released in the dataset? The final labels are after data cleaning\nand post processing from the individual annotator responses.\nDataset Release and Maintenance\n1.Do you have reason to believe the annotations in this dataset may change over time? Do you plan to update your dataset? No.\n2.Are there any conditions or definitions that, if changed, could impact the utility of your dataset? No.\n3.Will you attempt to track, impose limitations on, or otherwise influence how your dataset is used? If so, how? The SA-V dataset\nis released under a permissive CC by 4.0 license.\n4.Were annotators informed about how the data is externalized? If changes to the dataset are made, will they be informed? No.\n5.Is there a process by which annotators can later choose to withdraw their data from the dataset? If so, please detail. No.\n35\n\nReferences\nUnited States Environmental Protection Agency. Greenhouse gas equivalencies calculator.\nhttps://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator, 2022.\nMax Allan, Satoshi Kondo, Sebastian Bodenstedt, Stefan Leger, Rahim Kadkhodamohammadi, Imanol Luengo, Felix\nFuentes, Evangello Flouty, Ahmed Mohammed, Marius Pedersen, et al. 2018 robotic scene segmentation challenge.\narXiv preprint arXiv:2001.11190 , 2020.\nAli Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan.\nBurst: A benchmark for unifying object recognition, segmentation and tracking in video. WACV, pp. 1674–1683,\n2022.\nXue Bai and Guillermo Sapiro. A geodesic framework for fast interactive image and video segmentation and matting.\nInICCV, 2007.\nDina Bashkirova, Mohamed Abdelfattah, Ziliang Zhu, James Akl, Fadi Alladkani, Ping Hu, Vitaly Ablavsky, Berk Calli,\nSarah Adel Bargal, and Kate Saenko. ZeroWaste dataset: Towards deformable object segmentation in cluttered\nscenes.CVPR, 2022.\nMaksym Bekuzarov, Ariana Bermudez, Joon-Young Lee, and Hao Li. Xmem++: Production-level video segmentation\nfrom few annotated frames. In ICCV, pp. 635–644, 2023.\nGoutam Bhat, Felix Järemo Lawin, Martin Danelljan, Andreas Robinson, Michael Felsberg, Luc Van Gool, and Radu\nTimofte. Learning what to learn for video object segmentation. ECCV, abs/2003.11540, 2020.\nDaniel Bolya, Chaitanya Ryali, Judy Hoffman, and Christoph Feichtenhofer. Window attention is bugged: How not to\ninterpolate position embeddings. ICLR, 2023.\nT Brox, J Malik, and P Ochs. Freiburg-berkeley motion segmentation dataset (fbms-59). In ECCV, volume 1, pp. 9,\n2010.\nYohann Cabon, Naila Murray, and Martin Humenberger. Virtual KITTI 2. arXiv preprint arXiv:2001.10773 , 2020.\nSergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixé, Daniel Cremers, and Luc Van Gool. One-shot\nvideo object segmentation. CVPR, pp. 5320–5329, 2016.\nSergi Caelles, Alberto Montes, Kevis-Kokitsi Maninis, Yuhua Chen, Luc Van Gool, Federico Perazzi, and Jordi\nPont-Tuset. The 2018 davis challenge on video object segmentation. arXiv preprint arXiv:1803.00557 , 2018.\nSergi Caelles, Jordi Pont-Tuset, Federico Perazzi, Alberto Montes, Kevis-Kokitsi Maninis, and Luc Van Gool. The\n2019 davis challenge on vos: Unsupervised multi-object segmentation. arXiv preprint arXiv:1905.00737 , 2019.\nJuan C. Caicedo, Allen Goodman, Kyle W. Karhohs, Beth A. Cimini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng\nHeng, Tim Becker, Minh Doan, Claire McQuin, Mohammad Rohban, Shantanu Singh, and Anne E. Carpenter.\nNucleus segmentation across imaging experiments: the 2018 data science bowl. Nature Methods , 2019.\nJiazhou Chen, Yanghui Xu, Shufang Lu, Ronghua Liang, and Liangliang Nan. 3D instance segmentation of MVS\nbuildings. IEEE Transactions on Geoscience and Remote Sensing , 2022.\nKeyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi. Rsprompter:\nLearning to prompt for remote sensing instance segmentation based on visual foundation model. IEEE Transactions\non Geoscience and Remote Sensing , 2024.\nYuhua Chen, Jordi Pont-Tuset, Alberto Montes, and Luc Van Gool. Blazingly fast video object segmentation with\npixel-wise metric learning. CVPR, pp. 1189–1198, 2018.\nHo Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an atkinson-shiffrin\nmemory model. In ECCV, pp. 640–658. Springer, 2022.\nHo Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethinking space-time networks with improved memory coverage\nfor efficient video object segmentation. In NeurIPS , 2021a.\nHo Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modular interactive video object segmentation: Interaction-to-mask,\npropagation and difference-aware fusion. In CVPR, 2021b.\nHo Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, and Alexander Schwing. Putting the object back into\nvideo object segmentation. In arXiv, 2023a.\n36\n\nHo Kei Cheng, Seoung Wug Oh, Brian L. Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with\ndecoupled video segmentation. ICCV, pp. 1316–1326, 2023b.\nYangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track\nanything. arXiv preprint arXiv:2305.06558 , 2023c.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation, 2014.\ncite arxiv:1406.1078Comment: EMNLP 2014.\nLuca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and Giuseppe Amato. Domain adaptation for traffic\ndensity estimation. International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory\nand Applications , 2021.\nLuca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and Giuseppe Amato. Night and day instance\nsegmented park (NDISPark) dataset: a collection of images taken by day and by night for vehicle detection,\nsegmentation and counting in parking areas. Zenodo, 2022.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training text encoders as\ndiscriminators rather than generators. In ICLR, 2020.\nNadav Cohen, Yael Newman, and Ariel Shamir. Semantic segmentation in art paintings. Computer Graphics Forum ,\n2022.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,\nStefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, pp.\n3213–3223, 2016.\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection,\npipeline and challenges for EPIC-KITCHENS-100. IJCV, 2022.\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint\narXiv:2307.08691 , 2023.\nAhmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar, Richard Higgins, Sanja Fidler, David Fouhey, and\nDima Damen. Epic-kitchens visor benchmark: Video segmentations and object relations. NeurIPS , 35:13745–13758,\n2022.\nThanos Delatolas, Vicky Kalogeiton, and Dim P Papadopoulos. Learning the what and how of annotation in video\nobject segmentation. In WACV, pp. 6951–6961, 2024.\nRuining Deng, Can Cui, Quan Liu, Tianyuan Yao, Lucas W Remedios, Shunxing Bao, Bennett A Landman, Lee E\nWheless, Lori A Coburn, Keith T Wilson, et al. Segment anything model (sam) for digital pathology: Assess\nzero-shot segmentation on whole slide imaging. arXiv preprint arXiv:2304.04155 , 2023.\nHenghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip H. S. Torr, and Song Bai. Mose: A new dataset for video\nobject segmentation in complex scenes. ICCV, pp. 20167–20177, 2023.\nQingnan Fan, Fan Zhong, Dani Lischinski, Daniel Cohen-Or, and Baoquan Chen. Jumpcut: non-successive mask\ntransfer and interpolation for video cutout. ACM Transactions on Graphics , 2015.\nAlireza Fathi, Xiaofeng Ren, and James M. Rehg. Learning to recognize objects in egocentric activities. CVPR, 2011.\nMatthew Fishman, Abigail Matt, Fei Wang, Elena Gracheva, Jiantao Zhu, Xiangping Ouyang, Andrey Komarov,\nYuxuan Wang, Hongwu Liang, and Chao Zhou. A drosophila heart optical coherence microscopy dataset for\nautomatic video segmentation. Scientific Data , 10(1):886, 2023.\nJean-Michel Fortin, Olivier Gamache, Vincent Grondin, François Pomerleau, and Philippe Giguère. Instance segmenta-\ntion for autonomous log grasping in forestry operations. IROS, 2022.\nEstibaliz Gómez-de Mariscal, Hasini Jayatilaka, Özgün Çiçek, Thomas Brox, Denis Wirtz, and Arrate Muñoz-\nBarrutia. Search for temporal cell segmentation robustness in phase-contrast microscopy videos. arXiv preprint\narXiv:2112.08817 , 2021.\nRaghav Goyal, Wan-Cyuan Fan, Mennatullah Siam, and Leonid Sigal. Tam-vt: Transformation-aware multi-scale\nvideo transformer for segmentation and tracking. arXiv preprint arXiv:2312.08514 , 2023.\n37\n\nKristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar\nAshutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity\nfrom first-and third-person perspectives. arXiv preprint arXiv:2311.18259 , 2023.\nAgrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance segmentation. CVPR,\n2019.\nTimm Haucke and Volker Steinhage. Exploiting depth information for wildlife monitoring. arXiv preprint\narXiv:2102.05607 , 2021.\nTimm Haucke, Hjalmar S. Kühl, and Volker Steinhage. SOCRATES: Introducing depth in visual wildlife monitoring\nusing stereo vision. Sensors, 2022.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, 2022.\nByeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer.\narXiv preprint arXiv:2403.13298 , 2024.\nYuk Heo, Yeong Jun Koh, and Chang-Su Kim. Interactive video object segmentation using global and local transfer\nmodules. In ECCV, 2020.\nNamdar Homayounfar, Justin Liang, Wei-Chiu Ma, and Raquel Urtasun. Videoclick: Video Object Segmentation with\na Single Click. arXiv preprint arXiv:2101.06545 , 2021.\nJungseok Hong, Michael Fulton, and Junaed Sattar. TrashCan: A semantically-segmented dataset towards visual\ndetection of marine debris. arXiv:2007.08097 , 2020.\nLingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: A\nbenchmark for long-term video object segmentation. In ICCV, pp. 13480–13492, 2023.\nLingyi Hong, Zhongying Liu, Wenchao Chen, Chenzhi Tan, Yuang Feng, Xinyu Zhou, Pinxue Guo, Jinglun Li, Zhaoyu\nChen, Shuyong Gao, et al. Lvos: A benchmark for large-scale long-term video object segmentation. arXiv preprint\narXiv:2404.19326 , 2024.\nYuan-Ting Hu, Jia-Bin Huang, and Alexander G. Schwing. MaskRNN: Instance level video object segmentation. In\nNeurIPS , 2018a.\nYuan-Ting Hu, Jia-Bin Huang, and Alexander G. Schwing. VideoMatch: Matching based video object segmentation.\nECCV, abs/1809.01123, 2018b.\nXiaoqian Huang, Kachole Sanket, Abdulla Ayyad, Fariborz Baghaei Naeini, Dimitrios Makris, and Yahya Zweiri. A\nneuromorphic dataset for object segmentation in indoor cluttered environment. arXiv preprint arXiv:2302.06301 ,\n2023.\nLei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, et al. Segment anything in high\nquality.NeurIPS , 36, 2024.\nDahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Video panoptic segmentation. In CVPR, 2020.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023.\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of\nmachine learning. arXiv preprint arXiv:1910.09700 , 2019.\nFuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and James M. Rehg. Video segmentation by tracking many\nfigure-ground segments. CVPR, pp. 2192–2199, 2013.\nMingxing Li, Liucheng Hu, Zhiwei Xiong, Bang Zhang, Pan Pan, and Dong Liu. Recurrent dynamic embedding for\nvideo object segmentation. CVPR, pp. 1322–1331, 2022a.\nYanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object\ndetection. In ECCV, 2022b.\nYin Li, Zhefan Ye, and James M. Rehg. Delving into egocentric actions. CVPR, 2015.\nTsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In CVPR, 2017.\n38\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019.\nJun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature\nCommunications , 15(1):654, 2024.\nKevis-Kokitsi Maninis, Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset, Laura Leal-Taixé, Daniel Cremers, and Luc Van\nGool. Video object segmentation without temporal information. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 41:1515–1530, 2017.\nMaciej A Mazurowski, Haoyu Dong, Hanxue Gu, Jichen Yang, Nicholas Konz, and Yixin Zhang. Segment anything\nmodel for medical image analysis: an experimental study. Medical Image Analysis , 89:102918, 2023.\nTim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-object tracking\nwith transformers. In CVPR, June 2022.\nJiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang. Large-scale video panoptic\nsegmentation in the wild: A benchmark. In CVPR, pp. 21033–21043, 2022.\nMassimo Minervini, Andreas Fischbach, Hanno Scharr, and Sotirios A. Tsaftaris. Finely-grained annotated datasets\nfor image-based plant phenotyping. Pattern Recognition Letters , 2016.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,\nInioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the conference on\nfairness, accountability, and transparency , pp. 220–229, 2019.\nPeter Ochs, Jitendra Malik, and Thomas Brox. Segmentation of moving objects by long term video analysis. IEEE\nTransactions on Pattern Analysis and Machine Intelligence , 36(6):1187–1200, 2014.\nSeoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, and Seon Joo Kim. Fast video object segmentation by\nreference-guided mask propagation. CVPR, pp. 7376–7385, 2018.\nSeoung Wug Oh, Joon-Young Lee, N. Xu, and Seon Joo Kim. Video object segmentation using space-time memory\nnetworks. ICCV, pp. 9225–9234, 2019.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud\nTexier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 , 2021.\nFederico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and Alexander Sorkine-Hornung. Learning video\nobject segmentation from static images. CVPR, pp. 3491–3500, 2016.\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alexander Sorkine-Hornung, and Luc Van Gool.\nThe 2017 davis challenge on video object segmentation. arXiv:1704.00675 , 2017.\nAlessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid, and Vittorio Ferrari. Learning object class\ndetectors from weakly annotated video. CVPR, pp. 3282–3289, 2012.\nMattia Pugliatti and Francesco Topputo. DOORS: Dataset fOr bOuldeRs Segmentation. Zenodo, 2022.\nJiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip Torr, and\nSong Bai. Occluded video instance segmentation: A benchmark. IJCV, 2022.\nFrano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher Yu. Segment anything meets point\ntracking. arXiv:2307.01197 , 2023.\nSimiao Ren, Francesco Luzi, Saad Lahrichi, Kaleb Kassaw, Leslie M Collins, Kyle Bradbury, and Jordan M Malof.\nSegment anything, from space? In WACV, pp. 8355–8365, 2024.\nMike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,\nand Joshua M. Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding.\nICCV, 2021.\nAndreas Robinson, Felix Järemo Lawin, Martin Danelljan, Fahad Shahbaz Khan, and Michael Felsberg. Learning fast\nand robust target models for video object segmentation. CVPR, pp. 7404–7413, 2020.\nChaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu\nChowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, and Christoph Feichtenhofer. Hiera: A\nhierarchical vision transformer without the bells-and-whistles. ICML, 2023.\nCorey Snyder and Minh Do. STREETS: A novel camera network dataset for traffic flow. NeurIPS , 2019.\n39\n\nKonstantin Sofiiuk, Ilya A Petrov, and Anton Konushin. Reviving iterative training with mask guidance for interactive\nsegmentation. In ICIP, pp. 3141–3145. IEEE, 2022.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position\nembedding. arXiv preprint arXiv:2104.09864 , 2021.\nLv Tang, Haoke Xiao, and Bo Li. Can sam segment anything? when sam meets camouflaged object detection. arXiv\npreprint arXiv:2304.04709 , 2023.\nPavel Tokmakov, Jie Li, and Adrien Gaidon. Breaking the “object” in video object segmentation. CVPR, pp.\n22836–22845, 2022.\nTom Toulouse, Lucile Rossi, Antoine Campana, Turgay Celik, and Moulay A Akhloufi. Computer vision for wildfire\nresearch: An evolving image dataset for processing and analysis. Fire Safety Journal , 92:188–194, 2017.\nCameron Trotter, Georgia Atkinson, Matt Sharpe, Kirsten Richardson, A. Stephen McGough, Nick Wright, Ben\nBurville, and Per Berggren. NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation.\narXiv:2005.13359 , 2020.\nPaul Voigtlaender and B. Leibe. Online adaptation of convolutional neural networks for video object segmentation.\nArXiv, abs/1706.09364, 2017.\nStéphane Vujasinović, Sebastian Bullinger, Stefan Becker, Norbert Scherer-Negenborn, Michael Arens, and Rainer\nStiefelhagen. Revisiting click-based interactive video object segmentation. In ICIP, pp. 2756–2760. IEEE, 2022.\nBoying Wang, Libo Zhang, Longyin Wen, Xianglong Liu, and Yanjun Wu. Towards real-world prohibited item detection:\nA large-scale x-ray benchmark. CVPR, 2021a.\nHaochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, Xu Tang, Yao Hu, Weidi Xie, and Efstratios Gavves. Towards\nopen-vocabulary video instance segmentation. In ICCV, pp. 4057–4066, 2023.\nJue Wang, Pravin Bhat, R Alex Colburn, Maneesh Agrawala, and Michael F Cohen. Interactive video cutout. ACM\nTransactions on Graphics , 2005.\nJunke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Chuanxin Tang, Xiyang Dai, Yucheng Zhao, Yujia Xie,\nLu Yuan, and Yu-Gang Jiang. Look before you match: Instance understanding matters in video object segmentation.\nCVPR, pp. 2268–2278, 2022.\nWeiyao Wang, Matt Feiszli, Heng Wang, and Du Tran. Unidentified video objects: A benchmark for dense, open-world\nsegmentation. In ICCV, pp. 10776–10785, 2021b.\nJunde Wu, Wei Ji, Yuanpei Liu, Huazhu Fu, Min Xu, Yanwu Xu, and Yueming Jin. Medical sam adapter: Adapting\nsegment anything model for medical image segmentation. arXiv preprint arXiv:2304.12620 , 2023a.\nQiangqiang Wu, Tianyu Yang, Wei Wu, and Antoni B. Chan. Scalable video object segmentation with simplified\nframework. In ICCV, 2023b.\nJunyu Xie, Charig Yang, Weidi Xie, and Andrew Zisserman. Moving object segmentation: All you need is sam (and\nflow).arXiv preprint arXiv:2404.12389 , 2024.\nYunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang,\nFei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, and Vikas Chandra. Efficientsam: Leveraged masked image\npretraining for efficient segment anything, 2023.\nN. Xu, L. Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian L. Price, Scott D. Cohen, and\nThomas S. Huang. Youtube-vos: Sequence-to-sequence video object segmentation. In ECCV, 2018a.\nN. Xu, L. Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas S. Huang. Youtube-vos: A\nlarge-scale video object segmentation benchmark. ArXiv, abs/1809.03327, 2018b.\nJinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything\nmeets videos. arXiv preprint arXiv:2304.11968 , 2023.\nL. Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, and Aggelos K. Katsaggelos. Efficient video object segmentation\nvia network modulation. CVPR, pp. 6499–6507, 2018.\nLei Yang, Yan Zi Wei, Yisheng HE, Wei Sun, Zhenhang Huang, Haibin Huang, and Haoqiang Fan. iShape: A first\nstep towards irregular shape instance segmentation. arXiv:2109.15068 , 2021a.\n40\n\nZongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object segmentation. In NeurIPS,\n2022.\nZongxin Yang, Yunchao Wei, and Yi Yang. Collaborative video object segmentation by foreground-background\nintegration. In ECCV, 2020.\nZongxin Yang, Yunchao Wei, and Yi Yang. Associating objects with transformers for video object segmentation. In\nNeurIPS , 2021b.\nZongxin Yang, Jiaxu Miao, Yunchao Wei, Wenguan Wang, Xiaohan Wang, and Yi Yang. Scalable video object\nsegmentation with identification mechanism. TPAMI, 2024.\nSenthil Yogamani, Ciarán Hughes, Jonathan Horgan, Ganesh Sistu, Padraig Varley, Derek O’Dea, Michal Uricár, Stefan\nMilz, Martin Simon, Karl Amende, et al. WoodScape: A multi-task, multi-camera fisheye dataset for autonomous\ndriving.ICCV, 2019.\nJae Shin Yoon, François Rameau, Junsik Kim, Seokju Lee, Seunghak Shin, and In-So Kweon. Pixel-level matching for\nvideo object segmentation using convolutional neural networks. ICCV, pp. 2186–2195, 2017.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In CVPR, pp.\n12104–12113, 2022.\nChaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster\nsegment anything: Towards lightweight sam for mobile applications, 2023a.\nJiaming Zhang, Yutao Cui, Gangshan Wu, and Limin Wang. Joint modeling of feature, correspondence, and a\ncompressed memory for video object segmentation. ArXiv, abs/2308.13505, 2023b.\nLingzhi Zhang, Shenghao Zhou, Simon Stent, and Jianbo Shi. Fine-grained egocentric hand-object segmentation:\nDataset, model, and applications. ECCV, 2022.\nXu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment\nanything, 2023.\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic\nunderstanding of scenes through the ADE20K dataset. IJCV, 2019.\n41\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nMODIPHY: Multimodal Obscured Detection for IoT\nusing PHantom Convolution-Enabled Faster YOLO\nShubhabrata Mukherjee∗, Cory Beard, Zhu Li\nSchool of Science and Engineering ,\nUniversity of Missouri Kansas City ,\nKansas City, USA\nsmpw5@umsystem.edu; beardc@umkc.edu; zhu.li@ieee.org\nAbstract —Low-light conditions and occluded scenarios impede\nobject detection in real-world Internet of Things (IoT) appli-\ncations like autonomous vehicles and security systems. While\nadvanced machine learning models strive for accuracy, their\ncomputational demands clash with the limitations of resource-\nconstrained devices, hampering real-time performance. In our\ncurrent research, we tackle this challenge, by introducing “YOLO\nPhantom\", one of the smallest YOLO models ever conceived.\nYOLO Phantom utilizes the novel Phantom Convolution block,\nachieving comparable accuracy to the latest YOLOv8n model\nwhile simultaneously reducing both parameters and model size\nby 43%, resulting in a significant 19% reduction in Giga Floating-\nPoint Operations (GFLOPs). YOLO Phantom leverages transfer\nlearning on our multimodal RGB-infrared dataset to address\nlow-light and occlusion issues, equipping it with robust vision\nunder adverse conditions. Its real-world efficacy is demonstrated\non an IoT platform with advanced low-light and RGB cameras,\nseamlessly connecting to an A WS-based notification endpoint\nfor efficient real-time object detection. Benchmarks reveal a\nsubstantial boost of 17% and 14% in frames per second (FPS)\nfor thermal and RGB detection, respectively, compared to the\nbaseline YOLOv8n model. For community contribution, both the\ncode and the multimodal dataset are available on GitHub.1\nIndex Terms —Low light object detection, Multimodal fusion,\nIoT, YOLO, Phantom Convolution\nI. I NTRODUCTION\nRobust object detection under low light and occlusion is\ncrucial for various applications, including traffic monitoring,\npublic safety, and environmental monitoring. Instances such as\nTesla’s autopilot oversight and GM’s Cruise pedestrian crash\nunderscore safety concerns in autonomous vehicles [1-2]. The\nchallenges extend to facial recognition systems, facing accu-\nracy drops in inadequate lighting, necessitating advancements\nin object detection technologies [3-4]. AI plays a pivotal role\nin addressing these low-light challenges, employing innovative\napproaches like integrating infrared sensors to perceive ther-\nmal signatures in darkness. The multimodal augmentation [5]\nof thermal and RGB datasets in multimodal imaging emerge\nas a powerful solution, particularly in overcoming occlusion.\nFusing thermal, visible light, LiDAR, and radar distance\nscanning data through machine learning significantly improves\nthe performance of object detection and classification systems.\nRecent strides in low-light object detection, utiliz-\ning deep learning and fusion techniques [6]–[9], exhibit\n1The code and dataset are available at http://tinyurl.com/46y5twkupromise. YOLO-based methods, in particular, have gained\nacclaim [10]–[12]. While deploying neural network models\non edge devices like UA Vs presents challenges [13], our\nwork bridges this gap by enhancing the YOLOv8 nano [14]\narchitecture, giving rise to the “YOLO Phantom.\" Tailored\nfor resource-constrained IoT devices, this adaptation ensures\ncompatibility with small-scale hardware, achieving substantial\nspeed improvements for real-time object detection in low-light\nconditions. Our presentation delves into these enhancements,\nscrutinizes their performance gains, and showcases their ef-\nfectiveness in pertinent low-light object detection tasks.\nIn the context of our current research, we have developed\na compact yet efficient multimodal object detection model\noptimized for effective low-light object detection. Leveraging\nthe FLIRV2 dataset [15], we crafted a multimodal dataset\nto train our own YOLO Phantom model, complemented by\nintegrating a NoIR camera Module [16] with Raspberry Pi for\nexceptional performance in low-light scenarios. The distinctive\ncontributions of this paper include:\n•Creation of a compact YOLO model : Designing one\nof the smallest YOLO models, by using only 50% of\nthe kernels in three of the layers, demonstrating superior\nspeed without compromising accuracy.\n•YOLO Phantom for multimodal detection : Introducing\nYOLO Phantom, capable of detecting multiple modalities\nsuch as RGB and thermal. The model underwent training\nusing various modalities and demonstrates effectiveness\nin scenarios characterized by low light conditions and\nocclusion.\n•Innovative convolution block: Proposing the novel\n“Phantom Convolution\" block, by using Depthwise Sep-\narable Convolution [17] and Group Convolution [18]\nlayers we created a faster and more efficient version of\nGhost Convolution [19].\n•Realization of an end-to-end notification system : Re-\nalizing the new model through a practical end-to-end\nobject detection framework. This involves integrating a\nRaspberry Pi with RGB and NoIR cameras for simultane-\nous detection, along with an AWS cloud-based real-time\nnotification system.\nII. I MPLEMENTATION STRATEGIES\nCustomized detection systems power diverse applications,\nfrom self-driving cars to surveillance, tailoring to specific\nThis paper has been accepted for publication at the IEEE International Conference on Image Processing (ICIP) 2024\n1arXiv:2402.07894v2  [cs.CV]  23 Jun 2024\n\nenvironments and tasks. Key types include:\n•Aerial Detection System (ADS): ADS is utilized in\nair defense, public safety, disaster recovery [20] and\nvegetation monitoring, with ground-based installations\nstrategically placed and air-based systems integrated into\nmanned or unmanned aerial vehicles (UA Vs) for en-\nhanced situational awareness.\n•Marine Detection System (MDS): MDS is designed\nto detect and identify threats to maritime security, using\nradar, sonar, acoustic sensors, and smart cameras to track\nvessels and activities in surrounding waters [21]. AI-\nbased software and ML algorithms analyze sensor data\nfor threat identification, with examples like an Auto-\nmatic Identification System (AIS) for vessel tracking and\nLong-Range Identification and Tracking (LRIT) for long-\ndistance identification.\n•Ground-based Detection System (GDS): GDS focuses\non detecting, classifying, or tracking objects in terrestrial\nenvironments, employing technologies like ground-based\nradar, RGB or infrared-sensor-based cameras, and motion\nsensors to identify people, vehicles, or object move-\nment [22]. AI-based infrastructure analyzes collected data\nfor threat identification, including perimeter intrusion\ndetection systems and object detection with facial recog-\nnition using smart cameras and machine learning.\nFig. 1. Detection on a Rainy, Obscured Evening with Severe Occlusion Using\na Multimodal YOLO Model and a NoIR Camera on a Raspberry Pi Platform\nIII. R OLE OF RESOURCE OPTIMIZATION\nLimited resources in remote areas often constrain resilient\nobject detection in IoT systems. Optimized resource usage\nbecomes vital for their extended, reliable operation in such\nenvironments, enhancing lifespan, accuracy, and overall effi-\ncacy. In particular, resource optimization is focused on the\nefficient utilization of computation resources such as power,\nmemory, and temperature control. Limited power in devices\nlike Unmanned Aerial Vehicle (UA V) or surveillance cars\nhinders training and running computation-intensive AI models,\ncompromising efficiency, longevity, and overall performance.\nThrough significant improvements, particularly the incorpo-\nration of our newly proposed phantom convolution block,\nwe successfully developed one of the most compact YOLO\nmodels to date. As illustrated in Fig. 2, our model boasts the\nfewest parameters, minimal GFLOPs, and the smallest sizewhen compared to some of the most recent compact YOLO\nmodels. Performance evaluation revealed comparable or su-\nperior performance in different scenarios compared to other\nstate-of-the-art (SOTA) smaller models, encompassing both\nRGB and thermal modalities. YOLO Phantom’s lightweight\ndesign makes it highly suitable for real-time object detection\non edge devices.\nFig. 2. Size, Parameters, and GFLOP comparison of smaller YOLO models\nIV. O UR APPROACH : YOLO P HANTOM\nWe have modified the architecture of the smallest version\nof the Ultralytics YOLOv8 model, YOLOv8n, to enable it to\ndetect both RGB and thermal modality images. Furthermore,\nwe strategically restructured and resized select network blocks\nto facilitate expedited model inference. And, finally, we trained\nit using transfer learning with a multimodal RGB and thermal\nimage dataset as described later. This enabled YOLO Phantom\nto detect multimodal objects much faster than the Ultralytics\nYOLOv8, with similar or slightly higher accuracy (mAP), as\nseen in Section V.\nA. Ultralytics YOLOv8 architecture\nThe Ultralytics YOLOv8 architecture is primarily divided\ninto three main parts: the Backbone, Neck, and Head. The\nBackbone is responsible for generating feature maps and in-\ncorporates the following components: C2f, a modified version\nof Darknet53 with Cross-Stage Partial (CSP) connections for\nimproved feature re-usability and gradient flow, convolutional\nlayers with varying kernel sizes. The output of the Backbone\nis feature maps at multiple scales, capturing both low-level\ndetails and high-level semantic information. Fig. 3 shows the\nimplementation and our changes in YOLOv8 backbone, which\nincludes the C2f replaced by improved C2f (“C2fi\") and new\nPhantom Convolution blocks. Ultralytics YOLOv8 backbone\nalso consists of SPPF (Spatial Pyramid Pooling - Fast) which\nenhances the receptive field and captures multi-scale features\nthrough parallel pooling branches with different kernel sizes\nand concatenation of pooled features.\nFig. 4 shows the Ultralytics YOLOv8 implementation of\nthe neck and head, along with our introduced Phantom Con-\nvolution block. Upsample blocks are used to increase the\nfeature resolution and match with C2fi (C2f with ShortCut\nFalse) blocks using Concat blocks. The outputs from C2fi goes\nto Convolution blocks as well as detection heads, used for\nThis paper has been accepted for publication at the IEEE International Conference on Image Processing (ICIP) 2024\n2\n\nFig. 3. Modified YOLOv8 Backbone [23]\ndetecting small, medium, or large objects. It uses a decoupled\nhead to perform various tasks like detection, classification,\nsegmentation separately. YOLOv8 also uses SiLU (Swish\nLinear Unit) as default activation function, Mosaic augmen-\ntation, anchor-free detection, batch normalization for enriched\ntraining, and flexible prediction. Please note that inside the Ul-\ntralytics repository, in the main architecture file for YOLOV8,\nthe head and neck sections have been collectively referred to\nas ‘head’.\nB. Architecture improvement for faster and better inference\nIn the development of YOLO Phantom, we undertook\nmodifications to accommodate both RGB and thermal im-\nage detection, enhancing the model’s versatility. Ultralytics\nYOLOv8 training was conducted on the COCO dataset [24], a\npredominantly RGB dataset. It was necessary for us to make\nadjustments to architectural elements and parameter scaling\nby directing our efforts toward achieving comparable mean\naverage precision (mAP) performance with reduced model\ncomplexity and improved speed. Various novel convolution\nmethods have been adapted by researchers to improve accuracy\nperformance (better mAP) or make the model faster (higher\nFPS) to perform different real-time computer vision tasks.\nWe experimented with quite a few of them for our training;\nwe have mentioned our observations from using them in our\nmodel architecture.\na)Group Convolution :Group convolution [25] divides\nfilters into groups, each operating on a subset of input\nchannels. Unlike traditional convolution connecting to all\ninput channels, group convolution segregates filters, reducing\ncomputational costs and enabling parallelization. It enhances\nefficiency without compromising representational power, espe-\ncially in models with many channels. The group convolution\noperation can be expressed as below:\n•Input tensor: X∈RN×C×H×W\n•Weight tensor: W∈RG×C\nG×K×K\n•Bias term: b∈RG×1\n•Output tensor: Y∈RN×G×C\nG×H′×W′\nYn,g,i,j =C\nGX\nc=1Xn,(g−1)C\nG+c,i′,j′·Wg,c,k′,l′+bg (1)\nFig. 4. Modified YOLOv8 Neck and Decoupled detection Head [23]\nb)Depth-wise separable Convolution :Depth-wise Sep-\narable convolution [17] is a CNN variant that splits a standard\nconvolution into two steps: depth-wise convolution, where\neach input channel is convolved independently, and point-\nwise convolution, combining results across channels with 1x1\nfilters. This separation significantly reduces computational\ncomplexity and parameters compared to traditional convolu-\ntions, making it widely adopted in mobile and edge devices\nfor efficiency without compromising expressiveness. It strikes\na balance between computational efficiency and model perfor-\nmance, making it suitable for resource-constrained environ-\nments.\n•Input tensor: X∈RN×C×H×W\n•Depthwise convolution filter: D∈RC×1×Kd×Kd\n•Pointwise convolution filter: P∈RC′×C×1×1\n•Bias term: b∈RC′×1\n•Output tensor: Y∈RN×C′×H′×W′\nDepthwise Convolution:\nYdw=DepthwiseConv (X, D )//Ydw∈RN×C×H′×W′(2)\nPointwise Convolution:\nY=PointwiseConv (Ydw, P) +b//Y∈RN×C′×H′×W′(3)\nNow, if we combine above two operations:\nYn,c′,i,j=CX\nc=1KdX\nk′=1KdX\nl′=1Xn,c,i×S+k′,j×S+l′·Dc,1,k′,l′\n+CX\nc=1Ydw,n,c,i,j ·Pc′,c,1,1+bc′ (4)\nc)Ghost Convolution :Ghost Convolution [19] enhances\nconvolutional neural networks (CNNs) by introducing a par-\nallel “ghost branch\" alongside the main convolutional layers.\nThe ghost branch, created by subsampling input channels, pro-\ncesses a reduced input version and combines its output with the\nmain branch. This technique balances computational efficiency\nand model expressiveness. By utilizing fewer channels in the\nghost branch, Ghost Convolution achieves resource savings\nThis paper has been accepted for publication at the IEEE International Conference on Image Processing (ICIP) 2024\n3\n\nwhile retaining essential features. It is particularly valuable\nin scenarios with limited computational resources like mobile\ndevices where efficiency is crucial.\nFig. 5. Phantom Convolution and C2fi Block Architecture\nVarious strategies, culminating in adopting Ghost Convolu-\ntion, were employed to enhance feature extraction with fewer\nparameters. As seen in Fig. 5, the Ghost Convolution block\nunderwent re-architecting, with the first layer transformed into\na group convolution (group size 4) for increased operational\nspeed and larger kernel sizes (5x5) to expand the receptive\nfield. The second layer utilized Depth-wise Separable Convo-\nlution for efficiency, leading to a compact block with improved\ncomputational efficiency, named “Phantom Convolution\".\nThe C2f blocks were modified by eliminating forward pass\nconnections, introducing “C2fi\" or “C2f improved\" blocks for\na smaller, faster, and more efficient model. These blocks,\nwith shortcut connections set to false, replaced traditional C2f\nblocks throughout the architecture. These changes allowed a\nreduction in the number of filters in deeper layers, halving\nthe number of filters in layer 7 and in the head of the\nbackbone. Notably, the seventh layer incorporated the Phantom\nConvolution block, and the final convolution block in the head\n(layer 19) was replaced with Phantom Convolution to enhance\nefficiency and expedite the detection process.\nFig. 6. End-to-End Detection and Notification PipelineC. Experimental Setup\nIn this study, we devised an end-to-end detection and no-\ntification system utilizing IoT-based hardware in conjunction\nwith Amazon Web Services (AWS) for cloud-based notifica-\ntions (Fig. 7). Specifically, we employed a Raspberry Pi [26]\nversion 4, model B (CanaKit Extreme, 128 GB, 8GB, BullsEye\nOS), as the hosting and execution platform for the object\ndetection model. An external RGB camera was interfaced\nwith the USB port of the Raspberry Pi to capture relevant\ndata. We also connected a NoIR Camera Module V2 (Fig. 6)\nto the Raspberry Pi using Camera Serial Interface (CSI).\nThis camera module can see in low-light without an infrared\nfilter, making it ideal for low-light detection. To establish\nconnectivity between the Raspberry Pi and the cloud, we\nleveraged an AWS IoT Core, thereby constructing an event\nnotification framework. This integration empowered us to dis-\npatch customized notifications based on detected events; in our\nexperimentation, we validated this functionality by triggering\nnotifications upon the detection of a person. Furthermore, we\nused AWS Simple Notification Service (SNS), to augment the\nAWS IoT Core in disseminating alerts. This feature facilitated\nthe transmission of notifications to designated email addresses,\nphone numbers, or mobile applications, enhancing the versa-\ntility and accessibility of our alerting mechanism.\nFig. 7. Detection Setup Using Raspberry Pi and RGB Camera\nTo bolster the core detection framework, we integrated\nsupplementary safety controls. A temperature monitoring sys-\ntem for the Raspberry Pi automatically halts detection when\nexceeding a conservative 60°C threshold. Additionally, an\naudio alert system triggers upon person detection, offering\nredundancy beyond visual notifications. These enhancements\nincrease responsiveness and provide layered alerts for en-\nhanced situational awareness and threat mitigation.\nD. Dataset description and Training\nUtilizing the Teledyne FLIR ADAS Thermal Dataset\nv2 [15], we curated a multimodal dataset comprising both\nRGB and thermal imagery. The capability to perceive thermal\ninfrared radiation offers both complementary and distinct\nadvantages compared to conventional sensor technologies like\nvisible cameras, Lidar, and radar systems. Teledyne FLIR ther-\nmal sensors excel in detecting and classifying objects under\nchallenging conditions, including total darkness, fog, smoke,\nadverse weather, and glare. The primary dataset comprises\n26,442 annotated images spanning 15 categories. From the\nThis paper has been accepted for publication at the IEEE International Conference on Image Processing (ICIP) 2024\n4\n\nFig. 8. Various Occluded and Low-Light Detection Scenario Comparison with Smaller YOLO Models (left to right) Using NoIR or RGB Cameras\ntraining and validation sets, we selected 10,478 thermal and\n10,053 RGB images for the training set, and a combination\nof 1,128 thermal and 1,069 RGB images for validation to\nconstruct the multimodal dataset for training. Focusing on the\nfour most represented classes (person, car, traffic light, street\nsign), we conducted experiments. Additionally, a separate\nthermal test dataset of 3,749 images and an RGB test dataset\nof 3,493 images were constructed from the main FLIR V2\ndataset’s test section for benchmarking purposes. Notably, in\nthe original FLIR V2 dataset, the RGB and thermal images\nare not directly correspondent; there is no thermal image for\nthe exact scene depicted in an RGB image or vice-versa.\nConsequently, our curated multimodal dataset mixes various\nthermal and RGB images captured in different scenes. This\nhelps the model generalize better and avoids misalignment\nbetween RGB and thermal images of the same scene.\nYOLO Phantom was trained using our multimodal dataset\non a CentOS cluster with two “NVIDIA RTX A6000\" GPUs\nand 64 CPUs for 100 epochs. Images were resized to 640x640\npixels. The “yolov8n.pt\" pre-trained weights served as the\nstarting point for the training process.\nV. R ESULTS AND ANALYSIS\nA. Out of sample testing\nThis section presents a performance comparison of YOLO\nPhantom against other state-of-the-art (SOTA) smaller YOLO\nmodels. The evaluation employed a diverse image set encom-\npassing low-light and occlusion scenarios (thermal imaging,\ntunnel interiors, cloudy evenings, foggy mornings, nighttime).\nBoth RGB and NoIR cameras captured the images. All thedetection has been performed with a non-max-suppression\n(NMS) 0.5 threshold. Figure 8 showcases YOLO Phan-\ntom’s competitive performance, matching or exceeding smaller\nmodels like YOLOv4-tiny, YOLOv7-tiny, YOLOv8n, and\nYOLOv5nu, despite having 4-2x fewer parameters.\nB. Performance on different modality data\nIn this section, we conducted a comprehensive benchmark\nanalysis of YOLO Phantom against recent small YOLO\nmodels, including YOLOv5nu, Ultralytics YOLOv8n, and\nYOLOv8n trained on multimodal data without any architecture\nmodifications. The comparison was performed on two separate\nmodalities: RGB and thermal data. We considered two key\nperformance metrics for benchmarking: accuracy (mAP50-\n95(B)) and frames per second (FPS). The mean Average\nPrecision (mAP) values were calculated using the COCO\nevaluation metrics [27], which measure the area under the\nprecision-recall curve for each class, averaging these values,\nand then averaging across all classes. The mAP scores reflect\nthe model’s accuracy in detecting objects at various Intersec-\ntions over Union (IoU) thresholds. We used the “benchmark\"\nfunction from the Ultralytics YOLO repository to calculate the\nbenchmarking scores across five different platforms: PyTorch,\nTorchScript, ONNX, OpenVINO, and ncnn, and the bench-\nmarking process was conducted on an M1 MAC OS CPU.\nAs depicted in Fig. 9, YOLO Phantom demonstrates a sig-\nnificantly superior performance compared to pre-trained mod-\nels such as YOLOv5nu and YOLOv8n. It exhibits a slightly\nhigher accuracy than the YOLOv8n fusion model trained on\nour multimodal data. Moreover, YOLO Phantom consistently\nThis paper has been accepted for publication at the IEEE International Conference on Image Processing (ICIP) 2024\n5\n\nachieves higher FPS across various model variants, surpassing\nboth pre-trained smaller models and the YOLOv8n fusion\nmodel. Similar trends were observed in Fig. 10 during the\nthermal data benchmarking. YOLO Phantom outperforms its\ncompetitors in mAP and achieves superior speed due to its\nsmaller size.\nFig. 9. mAP and FPS performance comparison on RGB data with pretrained\nYOLOV5nu, pretrained YOLOv8n, YOLOv8n transfer learned on multimodal\ndata, and YOLO Phantom\nFig. 10. mAP and FPS performance comparison on thermal data with\npretrained YOLOV5nu, pretrained YOLOv8n, YOLOv8n transfer learned on\nmultimodal data, and YOLO Phantom\nC. Cross-modality performance\nThis section evaluates the performance of multimodal mod-\nels against unimodal models using reverse modality data. This\napproach demonstrably highlights the added value of multi-\nmodal learning. Table I showcases performance (mAP) on\nRGB data comparing YOLOv8n trained on thermal, YOLOv8n\ntrained on RGB, and YOLO Phantom trained multimodally.\nWhile the thermal-trained YOLOv8n struggled, the RGB-\ntrained one excelled. Notably, YOLO Phantom achieved sim-\nilar or better performance in most cases, demonstrating its\nmultimodal advantage.\nTABLE I\nCROSS -MODALITY M AP PERFORMANCE ON RGB DATA\nFormat Thermal Trained RGB Trained YOLO Phantom\nPyTorch 9.29 19.65 19.72\nTorchScript 9.19 19.57 19.51\nONNX 9.19 19.57 19.51\nOpenVINO 8.43 10.24 16.65\nncnn 5.22 11.99 11.95\nIn the next case, we performed a similar comparison, but\ninstead of accuracy (mAP) we measured speed (FPS). As\nobserved in Table II, in this case, YOLO Phantom performed\nsimilar or better than thermal-trained or RGB-trained models.\nNext, we conducted a cross-modality mAP performance\ncomparison on the thermal dataset. As seen in Table III,TABLE II\nCROSS -MODALITY FPS PERFORMANCE ON RGB DATA\nFormat Thermal Trained RGB Trained YOLO Phantom\nPyTorch 17.50 17.14 17.53\nTorchScript 16.35 15.77 15.50\nONNX 20.12 20.45 21.92\nOpenVINO 22.86 23.28 25.86\nncnn 23.77 23.85 27.15\nin this case also the reverse modality model (RGB-trained)\nperformed poorly compared to the same modality (thermal-\ntrained) YOLOv8n model. However, the thermal-trained model\nperformed marginally better compared to YOLO Phantom on\nthis occasion. Finally, we performed a cross-modality FPS\nTABLE III\nCROSS -MODALITY M AP PERFORMANCE ON THERMAL DATA\nFormat RGB Trained Thermal Trained YOLO Phantom\nPyTorch 7.21 25.21 24.82\nTorchScript 6.81 24.9 24.46\nONNX 6.81 24.9 24.46\nOpenVINO 2.81 24.58 23.79\nncnn 3.87 15.26 14.94\ncomparison on the thermal dataset. As observed in Table\nIV , RGB, and thermal modality trained models performed\nnearly similarly, but YOLO Phantom shows much better FPS\nperformance compared to both.\nTABLE IV\nCROSS -MODALITY FPS PERFORMANCE ON THERMAL DATA\nFormat RGB Trained Thermal Trained YOLO Phantom\nPyTorch 17.76 16.54 16.65\nTorchScript 15.54 15.01 15.09\nONNX 19.16 18.48 19.32\nOpenVINO 22.92 22.38 24.22\nncnn 23.98 24.03 26.96\nVI. C ONCLUSION AND FUTURE WORK\nYOLO Phantom opens the door to many new real-world\nobject detection tasks, particularly in resource-constrained\nenvironments. This lightweight model exhibits a remarkable\nequilibrium between rapid execution, elevated accuracy, and\nresource efficiency – the three most critical factors for IoT\ndevices. The innovative Phantom Convolution block seam-\nlessly integrates into any framework, unlocking superior fea-\nture extraction with reduced computational burden. Its novel\narchitecture transcends specific tasks, demonstrating excep-\ntional performance across diverse, resource-scarce detection\nscenarios. To further strengthen an implementation’s practi-\ncal viability for resource-constrained IoT deployments, user-\ndefined temperature monitoring was added to safeguard against\nthermal excursions, while optional audio alerts offer additional\nredundancy for enhanced situational awareness and threat mit-\nigation. This advancement holds promise for broader perfor-\nmance gains across computer vision, encompassing tasks like\nclassification, segmentation, and even generative modeling.\nThe impact of YOLO Phantom extends beyond just enhanced\nroad safety and refined surveillance applications, paving the\nway for pioneering applications in environmental monitoring,\nwildlife tracking, and beyond.\nThis paper has been accepted for publication at the IEEE International Conference on Image Processing (ICIP) 2024\n6\n\nREFERENCES\n[1] A. Press. (2023, January) Ap: Tesla driver killed after plowing into\nfiretruck on freeway. [Online]. Available: http://tinyurl.com/3bejc9cy\n[2] Reuters. (2023, November) Gm’s cruise to recall 950 driverless\ncars after accident involving pedestrian. [Online]. Available: http:\n//tinyurl.com/5fj6mre5\n[3] R. Al Sobbahi and J. Tekli, “Comparing deep learning models for low-\nlight natural scene image enhancement and their impact on object detec-\ntion and classification: Overview, empirical evaluation, and challenges,”\nSignal Processing: Image Communication , p. 116848, 2022.\n[4] M. Ahmed, K. A. Hashmi, A. Pagani, M. Liwicki, D. Stricker, and\nM. Z. Afzal, “Survey and performance analysis of deep learning based\nobject detection in challenging environments,” Sensors , vol. 21, no. 15,\np. 5116, 2021.\n[5] A. Josi, M. Alehdaghi, R. M. Cruz, and E. Granger, “Multimodal data\naugmentation for visual-infrared person reid with corrupted data,” in\nProceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision , 2023, pp. 32–41.\n[6] W. Chen and T. Shah, “Exploring low-light object detection techniques,”\narXiv preprint arXiv:2107.14382 , 2021.\n[7] R. Kvyetnyy, R. Maslii, V . Harmash, I. Bogach, A. Kotyra, ˙Z. Gr ˛ adz,\nA. Zhanpeisova, and N. Askarova, “Object detection in images with low\nlight condition,” in Photonics Applications in Astronomy, Communica-\ntions, Industry, and High Energy Physics Experiments 2017 , vol. 10445.\nSPIE, 2017, pp. 250–259.\n[8] L. Ye and Z. Ma, “Llod: a object detection method under low-light\ncondition by feature enhancement and fusion,” in 2023 4th international\nseminar on artificial intelligence, networking and information technol-\nogy (AINIT) . IEEE, 2023, pp. 659–662.\n[9] X. Wang, D. Wang, S. Li, S. Li, P. Zeng, and X. Liang, “Low-light traffic\nobjects detection for automated vehicles,” in 2022 6th CAA International\nConference on Vehicular Control and Intelligence (CVCI) . IEEE, 2022,\npp. 1–5.\n[10] W. Liu, G. Ren, R. Yu, S. Guo, J. Zhu, and L. Zhang, “Image-adaptive\nyolo for object detection in adverse weather conditions,” in Proceedings\nof the AAAI Conference on Artificial Intelligence , vol. 36, no. 2, 2022,\npp. 1792–1800.\n[11] X. Yin, Z. Yu, Z. Fei, W. Lv, and X. Gao, “Pe-yolo: Pyramid enhance-\nment network for dark object detection,” in International Conference on\nArtificial Neural Networks . Springer, 2023, pp. 163–174.\n[12] Z. Wang, Z. Cai, and Y . Wu, “An improved yolox approach for low-light\nand small object detection: Ppe on tunnel construction sites,” Journal of\nComputational Design and Engineering , vol. 10, no. 3, pp. 1158–1175,\n2023.\n[13] S. Y . Nikouei, Y . Chen, S. Song, R. Xu, B.-Y . Choi, and T. R. Faughnan,\n“Real-time human detection as an edge service enabled by a lightweight\nCNN,” in 2018 IEEE International Conference on Edge Computing\n(EDGE) . IEEE, 2018, pp. 125–129.\n[14] G. Jocher, A. Chaurasia, and J. Qiu, “Ultralytics yolo,” Jan. 2023.\n[Online]. Available: https://github.com/ultralytics/ultralytics\n[15] T. FLIR, “Free teledyne flir thermal dataset for algorithm training,”\n2023, accessed: 2024-01-30. [Online]. Available: https://www.flir.com/\noem/adas/adas-dataset-form/\n[16] Raspberry Pi Foundation. (2024) Raspberry pi camera module\n2 noir. [Online]. Available: https://www.raspberrypi.com/products/\npi-noir-camera-v2/\n[17] F. Chollet, “Xception: Deep learning with depthwise separable convolu-\ntions,” in Proceedings of the IEEE conference on computer vision and\npattern recognition , 2017, pp. 1251–1258.\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\nwith deep convolutional neural networks,” Advances in neural informa-\ntion processing systems , vol. 25, 2012.\n[19] K. Han, Y . Wang, Q. Tian, J. Guo, C. Xu, and C. Xu, “Ghostnet:\nMore features from cheap operations,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , 2020, pp. 1580–\n1589.\n[20] S. Mukherjee, O. Coudert, and C. Beard, “Unimodal: Uav-aided infrared\nimaging based object detection and localization for search and disaster\nrecovery,” in 2022 IEEE International Symposium on Technologies for\nHomeland Security (HST) , 2022, pp. 1–6.\n[21] M. Zhang, S. Xu, W. Song, Q. He, and Q. Wei, “Lightweight underwater\nobject detection based on yolo v4 and multi-scale attentional feature\nfusion,” Remote Sensing , vol. 13, no. 22, p. 4706, 2021.\n[22] X. Tang, Z. Zhang, and Y . Qin, “On-road object detection and tracking\nbased on radar and vision fusion: A review,” IEEE Intelligent Trans-\nportation Systems Magazine , vol. 14, no. 5, pp. 103–128, 2022.[23] RangeKing. (2024) Github homepage. [Online]. Available: https:\n//github.com/RangeKing\n[24] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays,\nP. Perona, D. Ramanan, P. Doll’a r, and C. L. Zitnick, “Microsoft\nCOCO: common objects in context,” CoRR , vol. abs/1405.0312, 2014.\n[Online]. Available: http://arxiv.org/abs/1405.0312\n[25] Z. Su, L. Fang, W. Kang, D. Hu, M. Pietikäinen, and L. Liu, “Dynamic\ngroup convolution for accelerating convolutional neural networks,” in\nComputer Vision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part VI 16 . Springer, 2020,\npp. 138–155.\n[26] Raspberry Pi Foundation. (2024) Raspberry pi. [Online]. Available:\nhttps://www.raspberrypi.org/\n[27] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in European conference on computer vision . Springer, 2014,\npp. 740–755.\nThis paper has been accepted for publication at the IEEE International Conference on Image Processing (ICIP) 2024\n7\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nBright Channel Prior Attention for Multispectral Pedestrian Detection\nChenhang Cui1\u0003, Jinyu Xie1\u0003, Yechenhao Yang1\u0003\n1Yingcai Honors College, University of Electronic Science and Technology of China, Chengdu, China\nAbstract\nMultispectral methods have gained considerable at-\ntention due to their promising performance across\nvarious ﬁelds. However, most existing methods\ncannot e \u000bectively utilize information from two\nmodalities while optimizing time e \u000eciency. These\nmethods often prioritize accuracy or time e \u000e-\nciency, leaving room for improvement in their per-\nformance. To this end, we propose a new method\nbright channel prior attention for enhancing pedes-\ntrian detection in low-light conditions by integrat-\ning image enhancement and detection within a uni-\nﬁed framework. The method uses the V-channel of\nthe HSV image of the thermal image as an attention\nmap to trigger the unsupervised auto-encoder for\nvisible light images, which gradually emphasizes\npedestrian features across layers. Moreover, we uti-\nlize unsupervised bright channel prior algorithms\nto address light compensation in low light images.\nThe proposed method includes a self-attention en-\nhancement module and a detection module, which\nwork together to improve object detection. An ini-\ntial illumination map is estimated using the BCP,\nguiding the learning of the self-attention map from\nthe enhancement network to obtain more informa-\ntive representation focused on pedestrians. The ex-\ntensive experiments show e \u000bectiveness of the pro-\nposed method is demonstrated through.\n1 Introduction\nPedestrian detection [4; 5]is a vital aspect of autonomous\ndriving technology. Its basic task is to detect and classify\npedestrians on the roadway. As autonomous driving technol-\nogy evolves, unmanned vehicles must precisely sense their\nsurrounding, particularly in pedestrian detection to guarantee\nsafety and e \u000eciency. As a result, the accurate and low latency\ndetection of pedestrians has become an essential issue in au-\ntonomous driving. Nevertheless, the traditional pedestrian\ndetection algorithm has limited e \u000bectiveness in low-light con-\nditions. Researchers have made signiﬁcant e \u000borts to enhance\n\u0003Three authors contributed equally to this work.the performance of pedestrian detection algorithms in low-\nlight conditions [14; 22 ].\nMultispectral methods have gained considerable attention\ndue to their promising performance across various ﬁelds, such\nas image super resolution [20], remote sensing [2]and de-\ntection [16]. The multispectral method is a frequently em-\nployed enhancement traditional pedestrian detection algo-\nrithms. Traditional pedestrian detection algorithms often uti-\nlize grayscale or RGB images for detection [17]. Neverthe-\nless, these images provide inadequate color and texture in-\nformation, resulting in inaccurate detection of pedestrians.\nThe multispectral method uses data from various wavelength\nbands to provide more color and texture information, enhanc-\ning pedestrian detection algorithms’ accuracy and robustness.\nIn addition, scholars extensively use infrared thermal images\nin other modes since they provide valuable contour informa-\ntion. Multispectral pedestrian detection algorithms utilizing\nvisible and infrared light images have garnered increasing re-\nsearch attention [11; 3 ].\nCurrently, many multimodal algorithms [1; 24 ]employ the\nparallel bi-stream mode of visible light and infrared thermal\nimaging. [21]utilize the complementarity of visible light\nand infrared thermal imaging information, fusing them at a\nstage in the model, and then sending them to the pedestrian\ndetection module for processing. [18]propose a multispec-\ntral pedestrian detector by combining visual-optical (VIS)\nand IR images, utilizing a deep convolutional neural net-\nwork (DCNN) with the sum fusion strategy. [25]introduce\nan appoach designed with inter- and intra-modality attention\nmodules, which enables deep learning models to dynamically\nweigh and merge multispectral features. Although the pro-\nposed structures can potentially improve detection accuracy,\nthe challenge of e \u000bectively utilizing information from two\nmodalities while optimizing time e \u000eciency persists. To ad-\ndress above issues, we propose an attention-based feature fu-\nsion algorithm to direct the integration of multimodal infor-\nmation and pedestrian detection algorithm based on Yolov4\nmodel to advance performance.\nThe main contributions of this paper are as follows:\nFirst, we use the V-channel (brightness) of the HSV im-\nage of the thermal image as an attention map to trigger\nthe unsupervised auto-encoder for visible light images.\nThis attention map gradually emphasizes pedestrian fea-\ntures across layers.arXiv:2305.12845v1  [cs.CV]  22 May 2023\n\nSecond, our approach utilizes advanced networks and\nunsupervised bright channel prior algorithms to address\nlight compensation in low light images.\nThirdly, to improve object detection, we integrate image\nenhancement and detection within a uniﬁed framework,\nallowing the enhancement beneﬁts to extend to detection\ntasks.\n2 Related Works\n2.1 Multispectral Pedestrian Detection\nPedestrian detection has signiﬁcant practical signiﬁcance in\nvarious ﬁelds, particularly in developing advanced driver as-\nsistance systems (ADAS) [8]and autonomous vehicles. The\ncapability to detect pedestrians is critical for ensuring safety\nin these applications. In recent years, multispectral pedes-\ntrian detection due to its more robust performance in cer-\ntain special environments. Traditional methods including\nSVM [7]and AdaBoost [6], achieved the good performance\nin multispectral pedestrian detection. However, recently\ndeep learning based methods further improved detection per-\nformance compared to previous methods. [27]introduces\na novel multispectral pedestrian detection algorithm called\nGFD-SSD, which incorporates gated fusion technology with\nthe bidirectional feature map transformation SSD network.\n[26]proposes a novel multispectral pedestrian detection al-\ngorithm called Cross-Modality Interactive Attention Network\n(CIAN), designed to exploit the correlations between RGB\nand thermal data for improved detection accuracy.\n2.2 Low-Light Image Enhancement\nLow-light image enhancement is a technique that enhances\nthe quality of images taken in low-quality environments. This\ntechnique improves image clarity and brightness, making the\nimages more visually appealing and informative. [10]pro-\nposes a new algorithm for single image haze removal based\non the dark channel prior principle. [9]uses illumination\nmap estimation to correct images captured in low-light condi-\ntions. [19]proposes a novel low-light image enhancement al-\ngorithm called LR3M, which utilizes a low-rank regularized\nretinex model to e \u000bectively enhance low-light images with\nsuperior robustness and high image quality.\n3 Proposed Method\nThe proposed method’s overall architecture is shown in Fig. .\nThe model extends the established framework of YOLO and\nconsists of two main parts: the self-attention enhancement\nmodule, and the detection module. An initial illumination\nmap ˜tis estimated by using the BCP [23]. Then the encoder-\ndecoder network is used to obtain a ﬁnal illumination map t\nan with the unsupervised loss function LBCP. Moreover, the\nself-attention map Tattsourced from the infrared modal is ap-\nplied in the enhancement network to obtain more informative\nrepresentation focused on pedestrian.\n3.1 Motivation\nExisting Multispectral Pedestrian Detection methods aim at\ne\u000bectively extracting information from thermal images andvisible light images to improve detection quality ,but still\nmeet some challenges:\n(1) From existing Multispectral Pedestrian Detection meth-\nods, we can observe that many of them attempt to fuse visible\nlight images and thermal images [15; 28 ]. However, such fu-\nsion might reduce the quality of the results for the imbalanced\ninformation between thermal images and visible light images.\n(2) There are many methods [10; 9 ]of nighttime enhance-\nment that can be useful for multispectral pedestrian detec-\ntion. However, better obtaining task-oriented enhancement\nof RGB images, remains an interesting challenge.\nTo address the issues mentioned above, we propose frame-\nwork as shown in Fig. . Firstly, as pedestrians have tempera-\nture, their silhouettes are visible in thermal images. Using the\nV-channel (brightness) of the HSV image of the thermal im-\nage as the attention map, the unsupervised auto-encoder for\nvisible light images can be activated layer by layer to focus\nmore on pedestrian features. Secondly, we use enhanced net-\nworks and unsupervised bright channel prior algorithms for\nlight compensation in low light images. In addition, we com-\nbine image enhancement and object detection through a uni-\nﬁed framework to obtain enhancement that beneﬁts detection\ntasks.\n3.2 Bright Channel Prior Algorithm\nThe Brightness Correction Processing (BCP) algorithm [23]\nwas a means of adjusting local exposure within an image.\nThis approach is derived from the Dark Channel Prior (DCP)\n[33], which was initially created to forecast the transmission\nmap of a given hazy input image. We employ the BCP al-\ngorithm to predict an initial illumination map by introducing\nthe unsupervised loss in our proposed enhancement network.\nFor the BCP low-light enhancement module, we deﬁne it as\nfollows:\nIp=tpJp+(1\u0000tp)A; (1)\nwhere Iprepresents an observed low-light RGB image , Jp2\nR3is an enhanced image, tp2R is an illumination map and\nA2R3indicates an environment light. Assuming that Ais\nis known and the illumination map ˜t, , remains constant\nwithin \n(p) which is the patch centered at the pixel p,we can\nderive the following equation by applying the max operator\nto both sides of Equation (1) :\n˜tp=1\u0000max\nc;q(1\u0000Ic\nq\n1\u0000Ac) (2)\nThe darkest pixel in an image can be used to represent the\nenvironment light. Nonetheless, in a real-world setting, the\ndarkest pixel might correspond to a shadow or a dark object.\nTherefore, following the approach in [13], a set of the darkest\n0.1% pixels (referred to as K) is used to compute A in the\nbrightest channel of I (represented as max c2r;g;bIc):\nA=1\njKjX\np2KIp (3)\n3.3 Unsupervised Cross-Modal Illumination Map\nEstimation\nUnsupervised BCP Loss Given the tpobtained through the\nencoder-decoder network under the supervision of ˜tp, we\n\nFigure 1: The framework of the proposed method.\nhave the enhanced image Jpaccording to the Eq. 1:\nJp=Ip\u0000A\ntp+A (4)\nInspired by [23], considering As Eq. 1 is conceptually sim-\nilar to image matting, where an output image is expressed as\na linear combination of foreground and background, we de-\ncide to adopt the regularizer used in soft matting . Overall,\nthe unsupervised BCP loss is formulated as:\nLBCP=1\nNX\nPf(tp\u0000˜tp)2+\u0015X\ni;j2\b(P)wi j(ti\u0000tj)2g;(5)\nwhere N is the total number of pixels, \u0015is a weight parameter,\nwi jis value of the Laplacian matrix that calculates the a \u000enity\nbetween IiandIj,\b(P) represents 3 Ö3 patch around center\npixel p.\nCross-Modal Spatial Attention Module The single unsu-\npervised light enhancement could not guarantee the output\nimage beneﬁcial to pedestrian detection. From Fig. 1 we can\nobserve that pedestrians have temperature, contour of target\nto be detected is visible in thermal images. Therefore, we use\nthe brightness channel (V-channel) from the HSV image of\nthe thermal image as a cross-modal attention map:\nTatt=(TV)\r; (6)\nwhere\ris a parameter that controls the curvature of the at-\ntention map.\nTattis multiplied to convolutional activations of all layers\nin the enhancement network E, as shown in Fig. 2. This en-\nables for adapting the proposed network accordingly depend-\ning on the presence of pedestrians to capture more needed\ndetails and properly ignore the background of RGB images.\n3.4 YOLO-based Detection Module\nYOLO-v4, also known as You Only Look Once version 4,\nis a state-of-the-art real-time object detection system widelyused in computer vision and machine learning research. The\nYOLO-v4 object detection system builds on the success of\nits predecessors YOLO and YOLOv3, and is known for its\nimproved detection accuracy and speed. We combine our en-\nhancement network with YOLO-v4 and use the concat of en-\nchanced image and infrared image as the input of YOLO de-\ntection module. The total loss function Lis as follows :\nL=LBCP+\fLYOLO (7)\nCombining detection and augmentation loss for training is\nbeneﬁcial for obtaining images for the detection task. More-\nover, we establish a uniﬁed framework consisting of enhance-\nment network and detection network for multispectral pedes-\ntrian detection\n4 Experiments\nCompared Methods. We compare our Method with Yolo-v4.\nMoreover, we explored the four-channel version of Yolo-v4,\nwhere the thermal image and visible light image are concate-\nnated as the input of YOLO. We report the accuracy, recall\nand mAP of the detection results. The results are shown in\nTable 1.\nTable 1: The results of all methods on LLVIP.\nMethod Yolo-v4-3c Yolo-v4-4c ours\naccuracy 0.924 0.598 0.947\nrecall 0.601 0.571 0.647\nmAP 0.671 0.509 0.740\nDataset. We include use RGB and Thermal dataset LLVIP\n[12]to test the performance of our methods. LLVIP (low\nlight visible image person) dataset is a benchmark dataset de-\nsigned for low-light visible image person detection tasks. The\ndataset consists of over 10,000 person instances captured in\nlow-light scenarios, with large variations in poses, scales, oc-\nclusions, and illuminations. Moreover, the dataset includes\n\nFigure 2: Display of detection results on validation set.\nannotations of bounding boxes and class labels for each per-\nson instance. LLVIP dataset covers a range of applications\nsuch as video surveillance, nighttime photography, and au-\ntonomous driving. The dataset is divided into two subsets,\nwith 60% of the data in the training set and the remaining\n40% in the test set. The LLVIP dataset contributes to the com-\nputer vision community by presenting a new and challenging\ndataset for low-light visible image person detection.\nDisplay of dection results. To further show the e \u000bectiveness\nof the proposed method, we display the detection results on\nvalidation set of LLVIP as shown in Fig. 2.\nImplementation. The experiments were conducted on a\nWindows PC equipped with an Intel (R) Core (TM) i5-\n12600K CPU @ 3.69 GHz, 32 GB RAM, and a GeForce RTX\n3070ti GPU with 8 GB of cache memory.\n5 Conclusion\nIn this paper, we proposed an attention-based feature fusion\nalgorithm for multimodal pedestrian detection in low light\nconditions. The proposed algorithm utilizes the V-channel of\nthe HSV image and unsupervised auto-encoder for attention\nand feature extraction, respectively. Our approach addresses\nthe challenge of light compensation and improves the object\ndetection accuracy in low light conditions. Additionally, inte-\ngrating image enhancement and detection in a uniﬁed frame-\nwork provides enhancement beneﬁts to detection tasks. Our\ncontributions o \u000ber promising results for autonomous driving\nand di \u000berent surveillance scenarios. The proposed approach\npresents a novel and robust solution for pedestrian detection\nin low light conditions, which can be extensible to other mul-\ntimodal pedestrian detection applications. We will further\nsupplement the experiments in future work.\nReferences\n[1]Kleanthis Avramidis, Christos Garouﬁs, Athanasia Zlat-\nintsi, and Petros Maragos. Enhancing a \u000bective rep-\nresentations of music-induced eeg through multimodalsupervision and latent domain adaptation. In ICASSP\n2022-2022 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n4588–4592. IEEE, 2022.\n[2]Jose AJ Berni, Pablo J Zarco-Tejada, Lola Su ´arez, and\nElias Fereres. Thermal and narrowband multispectral\nremote sensing for vegetation monitoring from an un-\nmanned aerial vehicle. IEEE Transactions on geo-\nscience and Remote Sensing , 47(3):722–738, 2009.\n[3]Jiale Cao, Yanwei Pang, Jin Xie, Fahad Shahbaz Khan,\nand Ling Shao. From handcrafted to deep features for\npedestrian detection: A survey. IEEE transactions on\npattern analysis and machine intelligence , 44(9):4913–\n4934, 2021.\n[4]Piotr Doll ´ar, Christian Wojek, Bernt Schiele, and Pietro\nPerona. Pedestrian detection: A benchmark. In 2009\nIEEE conference on computer vision and pattern recog-\nnition , pages 304–311. IEEE, 2009.\n[5]Piotr Dollar, Christian Wojek, Bernt Schiele, and Pietro\nPerona. Pedestrian detection: An evaluation of the state\nof the art. IEEE transactions on pattern analysis and\nmachine intelligence , 34(4):743–761, 2011.\n[6]Yoav Freund and Robert E Schapire. A decision-\ntheoretic generalization of on-line learning and an ap-\nplication to boosting. Journal of computer and system\nsciences , 55(1):119–139, 1997.\n[7]Mariano Garc ´ıa, David Ria ˜no, Emilio Chuvieco, Javier\nSalas, and F Mark Danson. Multispectral and lidar data\nfusion for fuel type mapping using support vector ma-\nchine and decision rules. Remote Sensing of Environ-\nment , 115(6):1369–1379, 2011.\n[8]David Geronimo, Antonio M Lopez, Angel D Sappa,\nand Thorsten Graf. Survey of pedestrian detection\nfor advanced driver assistance systems. IEEE trans-\n\nactions on pattern analysis and machine intelligence ,\n32(7):1239–1258, 2009.\n[9]Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light\nimage enhancement via illumination map estimation.\nIEEE Transactions on image processing , 26:982–993,\n2016.\n[10]Kaiming He, Jian Sun, and Xiaoou Tang. Single im-\nage haze removal using dark channel prior. IEEE trans-\nactions on pattern analysis and machine intelligence ,\n33(12):2341–2353, 2010.\n[11]Soonmin Hwang, Jaesik Park, Namil Kim, Yukyung\nChoi, and In So Kweon. Multispectral pedestrian detec-\ntion: Benchmark dataset and baseline. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition , pages 1037–1045, 2015.\n[12]Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and\nWenli Zhou. Llvip: A visible-infrared paired dataset for\nlow-light vision. In Proceedings of the IEEE /CVF Inter-\nnational Conference on Computer Vision , pages 3496–\n3504, 2021.\n[13]Hunsang Lee, Kwanghoon Sohn, and Dongbo Min. Un-\nsupervised low-light image enhancement using bright\nchannel prior. IEEE Signal Processing Letters , 27:251–\n255, 2020.\n[14]Guofa Li, Yifan Yang, Xingda Qu, Dongpu Cao, and\nKeqiang Li. A deep learning based image enhancement\napproach for autonomous driving at night. Knowledge-\nBased Systems , 213:106617, 2021.\n[15]Jingjing Liu, Shaoting Zhang, Shu Wang, and Dim-\nitris N Metaxas. Multispectral deep neural networks for\npedestrian detection. arXiv preprint arXiv:1611.02644 ,\n2016.\n[16]Kavir Osorio, Andr ´es Puerto, Cesar Pedraza, David Ja-\nmaica, and Leonardo Rodr ´ıguez. A deep learning ap-\nproach for weed detection in lettuce crops using multi-\nspectral images. AgriEngineering , 2(3):471–488, 2020.\n[17]Wanli Ouyang and Xiaogang Wang. Joint deep learning\nfor pedestrian detection. In Proceedings of the IEEE in-\nternational conference on computer vision , pages 2056–\n2063, 2013.\n[18]Dashun Pei, Mingxuan Jing, Huaping Liu, Fuchun Sun,\nand Linhua Jiang. A fast retinanet fusion framework for\nmulti-spectral pedestrian detection. Infrared Physics &\nTechnology , 105:103178, 2020.\n[19]Xutong Ren, Wenhan Yang, Wen-Huang Cheng, and Ji-\naying Liu. Lr3m: Robust low-light enhancement via\nlow-rank regularized retinex model. IEEE Transactions\non Image Processing , 29:5862–5876, 2020.\n[20]Claire Thomas, Thierry Ranchin, Lucien Wald, and Jo-\ncelyn Chanussot. Synthesis of multispectral images to\nhigh spatial resolution: A critical review of fusion meth-\nods based on remote sensing physics. IEEE Transac-\ntions on Geoscience and Remote Sensing , 46(5):1301–\n1312, 2008.[21]J¨org Wagner, V olker Fischer, Michael Herman, Sven\nBehnke, et al. Multispectral pedestrian detection using\ndeep fusion convolutional neural networks. In ESANN ,\nvolume 587, pages 509–514, 2016.\n[22]Hai Wang, Yanyan Chen, Yingfeng Cai, Long Chen,\nYicheng Li, Miguel Angel Sotelo, and Zhixiong Li.\nSfnet-n: An improved sfnet algorithm for semantic seg-\nmentation of low-light autonomous driving road scenes.\nIEEE Transactions on Intelligent Transportation Sys-\ntems, 23(11):21405–21417, 2022.\n[23]Yinting Wang, Shaojie Zhuo, Dapeng Tao, Jiajun Bu,\nand Na Li. Automatic local exposure correction using\nbright channel prior for under-exposed images. Signal\nprocessing , 93(11):3227–3238, 2013.\n[24]Xin Xu, Wei Liu, Zheng Wang, Ruimin Hu, and\nQi Tian. Towards generalizable person re-identiﬁcation\nwith a bi-stream generative model. Pattern Recognition ,\n132:108954, 2022.\n[25]Heng Zhang, Elisa Fromont, S ´ebastien Lef `evre, and\nBruno Avignon. Guided attentive feature fusion for mul-\ntispectral pedestrian detection. In Proceedings of the\nIEEE /CVF winter conference on applications of com-\nputer vision , pages 72–80, 2021.\n[26]Lu Zhang, Zhiyong Liu, Shifeng Zhang, Xu Yang,\nHong Qiao, Kaizhu Huang, and Amir Hussain. Cross-\nmodality interactive attention network for multispectral\npedestrian detection. Information Fusion , 50:20–29,\n2019.\n[27]Yang Zheng, Izzat H Izzat, and Shahrzad Ziaee. Gfd-\nssd: gated fusion double ssd for multispectral pedestrian\ndetection. arXiv preprint arXiv:1903.06999 , 2019.\n[28]Kailai Zhou, Linsen Chen, and Xun Cao. Improving\nmultispectral pedestrian detection by addressing modal-\nity imbalance problems. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part XVIII 16 , pages\n787–803. Springer, 2020.\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nComputers and Electronics in Agriculture 216 (2024) 108541\nAvailable online 21 December 2023\n0168-1699/© 2023 Published by Elsevier B.V.An improved YOLOv7 network using RGB-D multi-modal feature fusion for \ntea shoots detection \nYanxu Wua, Jianneng Chena,b,*, Shunkai Wua, Hui Lia, Leiying Hea,b, Runmao Zhaoa,b, \nChuanyu Wua,b \naFaculty of Mechanical Engineering & Automation, Zhejiang Sci-Tech University, Hangzhou 310018, China \nbKey Laboratory of Transplanting Equipment and Technology of Zhejiang Province, Hangzhou 310018, China   \nARTICLE INFO  \nKeywords: \nMulti-modal \nDepth image \nYOLOv7 \nLightweight \nAttention mechanism \nTea detection ABSTRACT  \nDue to the increasing scarcity of tea pickers, the implementation of intelligent harvesting for premium tea is a \ncrucial prerequisite for the sustainable development of the premium tea industry. The initial step towards \nachieving intelligent and precise harvesting is the accurate detection of tender shoots, which consist of one bud \nand one leaf. However, accurately identifying tea shoots poses a challenging visual task due to their small size, \nvariable shapes, as well as similar colors and backgrounds. The existing model, based on RGB images, can only \ndetect partial targets. To address this issue and further enhance the detection of tea buds, this study proposes the \nutilization of multi-modal features encompassing red, green, blue, and depth (RGB-D) for identification. In \naddition, a unidirectional complementary multi-modal fusion method is introduced to minimize the adverse \neffects caused by low-quality depth information. Firstly, an RGB-D dataset comprising high-quality tea leaves is \nconstructed, and the samples are carefully calibrated. Subsequently, an enhanced end-to-end RGB-D multi-modal \nobject detection network, referred to as YOLO-RGBDtea, is developed based on You Only Look Once version 7 \n(YOLOv7). This model incorporates a parallel lightweight depth image feature extraction backbone network and \nincorporates a self-attention mechanism to prioritize contextual information. Lastly, a cross-modal spatial \nattention fusion module (CSFM) is devised to collaboratively integrate depth features with RGB features in a \nunidirectional manner. The experimental results reveal that YOLO-RGBDtea achieves an AP50 of 91.12% when \nconfronting complex outdoor tea shoots, exhibiting significant performance improvements compared to YOLOv7, \nespecially in scenarios involving small targets, overlapping target groups, and highly overexposed images. \nNotably, the parameter increment in YOLO-RGBDtea compared to the original YOLOv7 model is merely 17.8%, \nand the additional components can be seamlessly transferred to other models. Overall, this study introduces a \nstraightforward yet effective multi-modal fusion method that bears theoretical and practical significance in \nadvancing the detection of high-quality tea shoots in complex outdoor environments.   \n1.Introduction \nTea, an esteemed nonalcoholic beverage, holds a distinct flavor and \npossesses medicinal properties, making it one of the top three nonal -\ncoholic beverages worldwide (Liu et al., 2017 ). The global tea produc -\ntion in 2020 amounted to 6.269 million tons, with China ranking first \nand contributing 2.986 million tons of tea production, as reported by the \nTea Circulation Association. Tea is broadly categorized into bulk tea and \nfamous tea. Presently, China ’s tea plantation area has expanded to 48.96 \nmillion mu, encompassing a mere 30% dedicated to premium tea \nplantations, which, despite their limited extent, hold the highest economic value, accounting for 75% of the entire tea market. Due to its \nexceptional quality, the plucking requirements for premium tea are \nnotably stringent, with specific morphological criteria, such as plucking \nsingle leaves or one bud and one leaf (Xu et al., 2022 ). In recent years, \nthe aging population has led to a decrease in the number of individuals \nwilling to engage in tea shoots picking. Consequently, there are signif -\nicant challenges in ensuring the timely and accurate harvest of tea (Han \net al., 2014 ). The traditional tea picking robots used for bulk tea are \nunable to perform precise positioning and picking. Applying this method \nto the harvesting of high-quality tea may result in the absence or damage \nof delicate tea shoots, and even harm the tea trees themselves, thereby \n*Corresponding author at: Faculty of Mechanical Engineering & Automation, Zhejiang Sci-Tech University, Hangzhou 310018, China. \nE-mail address: jiannengchen@zstu.edu.cn (J. Chen).  \nContents lists available at ScienceDirect \nComputers and Electronics in Agriculture \nu{�~zkw! s{yo| kro>! ÐÐÐ1 ow�o�to~1m{y2 w{mk�o2m{y| kr!\nhttps://doi.org/10.1016/j.compag.2023.108541 \nReceived 28 February 2023; Received in revised form 9 November 2023; Accepted 11 December 2023   \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n2impacting the subsequent harvest and resulting in substantial economic \nlosses. The accurate detection of tender tea shoots is the primary step \ntowards achieving intelligent tea harvesting. However, tea leaves pre-\nsent several complexities, including their color similarity to the back-\nground, diverse varieties, variable postures, and small size. \nFurthermore, the chaotic and unstructured environment of the field \nmakes it easy for tea shoots to obstruct each other and be affected by \nunpredictable lighting conditions. Consequently, tender tea shoot \ndetection poses the greatest difficulty among all agricultural product \ndetections. \nIn recent years, various methods and techniques have been devel -\noped for detecting tea shoots, with a significant focus on machine \nlearning approaches that extract color and shape features. However, \ntraditional detection algorithms have substantial limitations, and \nmanually designed features are susceptible to the complex environ -\nmental changes typically found in tea plantations. Consequently, ma-\nchine learning models trained using these features often exhibit weak \ngeneralization and poor robustness. With the advancement of deep \nlearning techniques, deep convolutional neural networks (DCNNs) have \nemerged as a widely employed method for target detection in this field. \nResearchers have been progressively applying convolutional neural \nnetworks to tea detection, drawing inspiration from these developments. \nFor instance, Xu et al. (2020) used a Faster R-CNN deep network model \nbased on the VGG-16 feature extraction network and obtained an ac-\ncuracy of 85.14% for the tea shoot detection model. Tao et al. (2021) \nproposed a Mask- RCNN-based tea picking point localization method \nwith better generality and robustness in complex environments. Sun \net al. (2019) combined super green feature segmentation to remove the \ncomplex background of tea buds and then used the improved multiscale \nYOLO network model to detect tea buds to obtain an average precision \nof 84.2%. Yang et al. (2019) proposed a method for identifying shoot \npicking points using an improved YOLOv3 deep convolutional neural \nnetwork algorithm, which can achieve an accuracy of 90% on the \nverification set. Li et al. (2022) compressed the YOLOv3-SPP model \nusing the channel pruning algorithm and layer pruning algorithm, \nincreasing the detection speed by a factor of 3.18 while maintaining only \na 0.40% decrease in average accuracy. \nPrevious research suggests that studies focusing on RGB image-based \ntea target detection have been progressing gradually. However, the ac-\ncuracy of identification in these studies has consistently fallen short of \nachieving levels comparable to the detection of other crops. We contend \nthat this limitation arises from the inadequate information provided by \nRGB images to support the completion of complex detection tasks in \nchallenging environments. Recognizing the complementary and \nconsistent nature of multimodal information, we propose that \nleveraging multimodal data and expanding data types may theoretically \nenhance the recognition rate of tea shoots. RCNN-Depth (Gupta et al., \n2015 ) was the first deep learning framework for RGB-D object detection, \nand this bimodal parallel processing approach later became the basis for \nmany visual perception tasks in RGB-D images (Gupta et al., 2015; Li \net al., 2016; Li et al., 2019 ). They have been gradually applied in posture \nrecognition and salient target detection (Piao et al., 2020; Li et al., \n2021 ). With the advancement of image acquisition devices, the use of \nmultimodal data for agricultural target detection has significantly \nincreased. This includes depth images, infrared images, remote sensing \nimages, etc. The diverse types of information provide greater possibil -\nities for accomplishing visual tasks in complex outdoor agricultural \nenvironments. Depth images offer intuitive depth relationships, aiding \nin better distinction between foreground, background, and overlapping \ntargets. Near-infrared images provide stable imaging even in changing \nenvironmental lighting conditions, and they can also be used for crop \nmaturity detection (Li et al., 2021 ). Therefore, several recent works have \nbeen devoted to the use of multimodal information to improve the \nperformance of crop detection. Nguyen et al. (2016) obtained RGB and \ndepth images of apples by an RGB-D camera and developed an algorithm \nto detect and localize apples based on color and shape features, which improved the recognition rate of obscured apples but required complex \npreprocessing of color data and depth data and was less effective in \nrecognizing apples with similar color to the background. Tao and Zhou \n(2017) extracted color features and 3D geometric features from pre-\nprocessed point clouds, subdivided the targets into apples, branches, and \nleaves, and used a genetic algorithm to optimize the support vector \nmachine (SVM) model. Yu et al. (2021) and Fu et al. (2020) used depth \nimages to filter out unwanted redundant image information and \nbackground. \nAccording to the location of multimodal data fusion, the fusion \nstrategy can be divided into data-layer fusion, feature-layer fusion, and \ndecision-layer fusion (Sun et al. 2022 ). Sa et al. (2016) acquired RGB \nimages and NIR images, explored data layer fusion and decision layer \nfusion based on Faster R-CNN, and achieved some improvement in the \ndetection of various fruit datasets. Gen˘e-Mola et al. (2019) fused data \nfrom three modalities, including color maps, depth maps and range- \ncorrected IR intensity in the data layer, to detect apples based on a \nmodified Faster R-CNN network for five channel inputs. Sun et al. (2022) \nproposed an end-to-end RGB-D target detection network to extract \nunimodal features using two feature extractors with the same structure \nand fuse them at the feature layer through an attention mechanism, \nwhich in turn complements the two-modal information, and the method \nachieves an AP50 of 95.4% for citrus detection. Cao (2021) established \ntwo processing streams for color and depth images of Agaricus Bosporus, \nfused the features of the four scales in stages through an attention \nmechanism, and then dynamically adjusted the weighting to differen -\ntiate regional features, thus improving the robustness of deep neural \nnetworks. Tu et al. (2018) also used Faster R-CNN to detect two mo-\ndalities of RGB and depth images of passion fruit independently and \nfinally integrated the two types of detection results on the decision layer \nto improve the detection performance. \nConsidering the three fusion strategies mentioned above, data-level \nfusion can supplement incomplete information during the input stage, \nwhile decision-level fusion can make judgments by combining various \nmodal features. The fusion structure of the data and decision levels is \nsimple, but the fusion method is relatively rough, and the improvement \neffect is also limited. In contrast, the network structure of feature-level \nfusion is complex and flexible in design, capable of integrating modal \nfeatures of multiple scales and maximizing the correlation and \ncomplementarity between modal information. Currently, most multi -\nmodal detection methods apply the same feature extraction operation to \nboth color images and depth images, resulting in significantly higher \ncomputational requirements compared to single-modal model training. \nIn reality, color images are three-channel inputs, while depth images \nhave only one channel. The amount of information contained in the two \nmodalities is not balanced, so using conventional feature extraction \nmethods for color images to process depth images is not the best \napproach. From the analysis of the features contained, color images and \ndepth images have a significant correlation, and adopting a comple -\nmentary form to form two feature layers does not yield a cost-effective \nsolution. \nTo address the limitations of tea shoot detection using RGB images \nalone, this study introduces YOLO-RGBDtea, a tea shoot detection \nnetwork based on YOLOv7 that utilizes fused RGB-D features. Building \nupon the original RGB image processing of YOLOv7, we incorporate a \nlightweight depth image processing backbone and a self-attention \nmechanism in a parallel fashion. Moreover, we propose a cross-modal \nspatial attention fusion module to integrate the multiscale depth infor-\nmation into the RGB feature layer. This study consists of the following \nmain parts:  \n(1) Establishing a RGB-D tea shoot image dataset under a complex \noutdoor environment and optimizing the data enhancement \nstrategy for RGB-D images of tea shoots.  \n(2) Adding parallel deep image processing streams based on the \nYOLOv7 model, the number of network channels in the deep Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n3processing stream is reduced in equal proportion, and some of the \nconvolutional layers are replaced by bottleneck transformers \n(BoT) to introduce a self-attention mechanism so that the model \ncan focus on global contextual information.  \n(3) A cross-module fusion module based on the spatial attention \nmechanism is designed. The fusion module extracts the multi -\nscale depth information and global features from the depth image \nand complements the RGB feature layer of the corresponding \nscale unidirectionally, thus improving the feature extraction ef-\nfect of the backbone in complex images.  \n(4) Comparative experiments are conducted on the RGB-D tea shoot \ndataset constructed in this paper, and the results show that the \npresent model has obvious advantages and better robustness in \nthe face of some extreme environments. In addition, the related \ndesign can be widely transferred to other crop detection models. \n2.Materials and methods \n2.1. Dataset construction \n2.1.1. Image collection \nAll RGB-D tea shoot samples in this database were collected using the \nZivid Two (ZCD2) 3D vision camera from Zivid Norway. We have \ncollected Zhongcha 108 samples for three days in September 2022, at \nthe tea research base in Songyang County, Lishui City, Zhejiang Prov-\nince, China. In order to enhance the diversity of sample images captured \nunder varying lighting conditions, data collection was carried out during \nboth morning and afternoon sessions. This database is specifically \ndesigned to facilitate the advancement of our team ’s third-generation \nintelligent tea harvesting robot, as depicted in Fig. 1. Fig. 2 illustrates \nthe relative positioning of the depth camera and the tea bush surface. \nThe yellow dotted box signifies the tea bush ’s location, the blue solid \nline denotes the camera ’s field of view, and the red dashed line repre -\nsents the camera ’s relative coordinate system. \nAccording to Xu et al. (2022) , for optimal detection accuracy, \ncapturing tea buds from a sideways angle is recommended when \nselecting the shooting angle. By referring to Fig. 3, it is evident that an \nexcessively small tilt angle results in an overwhelming number of targets \nwithin the camera ’s field of view, thereby exacerbating the detection \ndifficulty. Moreover, even if distant targets are successfully recognized, \nthey may exceed the reach of the harvesting robotic arm, rendering the \ntarget recognition ineffective. Therefore, considering the practical re-\nquirements of harvesting, it was determined that positioning the camera at a distance of 250 ±30 mm from the tea bush surface, with a tilt angle \nof 35 ±2•relative to the horizontal plane, was the most suitable choice. \nThe captured RGB and depth images must exhibit distinct and non- \nblurry tea leaf shoots. Both modalities have a fixed pixel resolution of \n1944 ×1200. The collection process entails frame-by-frame shooting, \nwith each frame accompanied by its corresponding RGB and depth im-\nages. Fig. 4 illustrates an example of the final captured samples. It is \nworth noting that the depth images in the figure have been subjected to \nvisualization processing, whereas the actual input to the model is a \nsingle-channel grayscale image. \n2.1.2. Image annotation \nAfter pairing the captured RGB images with depth images, calibra -\ntion was exclusively performed on the RGB images using the Baidu BML \nplatform. To simulate the real-world complexity of field conditions, tea \nleaf bud bounding boxes that could still be identified under overlapping \nscenarios were preserved. On average, each sample included 15.8 \nbounding boxes, with 5.6% of the boxes having an overlap ratio between \n0.25 and 0.5, and 2.7% of the boxes surpassing an overlap ratio of 0.5. As \na result of the selection process, a total of 1200 pairs of RGB-D image \nsamples depicting tea leaf buds were obtained, with 1080 pairs allocated \nfor training purposes and 120 pairs for validation. \n2.1.3. Data augmentation \nFirstly, we have learned from previous experiences that small data-\nsets can lead to overfitting of training models, resulting in failure to meet \nour expectations. However, acquiring a large volume of data entails \nsubstantial investments in terms of manpower, financial resources, and \ntime. Consequently, this study implements specific data augmentation \nstrategies tailored to tea leaf samples to expand the database. These \nstrategies encompass resizing, scaling, aspect ratio distortion, and \nrandom horizontal flipping. Furthermore, to mimic the performance of \ntea leaves under diverse lighting conditions, we randomly adjust the \nthree channels within the range of the HSV color space. Given the \nintrinsic growth characteristics of tea plants, conventional enhancement \nmethods like rotating images by 90•or 180•are not employed. It is \nimportant to note that the enhancement techniques applied to RGB \nimages, apart from HSV color space adjustments, necessitate consistent \nnumerical values for application to depth images, ensuring that \naugmented samples remain aligned. Several instances of data augmen -\ntation are depicted in Fig. 5. \nFig. 1.The third generation of intelligent picking robot of premium tea.  Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n42.2. Methodologies \nThe YOLO series is a renowned one-stage object detection network. \nSince its initial proposal by Redmon et al. (2015) with YOLOv1, and the \nYOLO family has established a substantial presence in the field of object \ndetection. In comparison to the R-CNN series, YOLO offers a simpler \narchitecture and faster detection speed. In August 2022, Wang et al. (2022) introduced the latest YOLOv7 detection model, which in-\ncorporates the amalgamation of existing techniques, module repar -\nameterization, and dynamic label assignment strategies. Notably, this \nmodel surpasses all existing object detectors in terms of both speed and \naccuracy, ranging from 5 FPS to 160 FPS. \nThe YOLOv7 network comprises four modules: Input, Backbone, \nNeck, and Head. The primary objective of this paper is to augment RGB \nFig. 2.Depth camera installation location diagram.  \nFig. 3.Imaging effect of the camera at different tilt angles. (a) Tilt angle set to 25•. (b) Tilt angle set to 35•.  \nFig. 4.Example of RGB-D tea samples taken in a complex environment in the field.  Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n5information with depth information in order to improve the precision of \noutdoor tea shoots detection. A novel multi-modal tea shoots detection \nnetwork, named YOLO-RGBDtea, is proposed based on YOLOv7. The \nchanges are mainly around the feature extraction backbone network, \nand the related structure is shown in Fig. 6. To begin with, a parallel \ndepth processing backbone, named depth-backbone, was developed by \nbuilding upon the use of YOLOv7 for RGB image processing. This \nimplementation maintains the majority of YOLOv7 modules while pro-\nportionally downsizing the channels in each layer. The objective of this \napproach is to mitigate the redundancy associated with handling single- \nlayer images within the model, ultimately leading to a reduction in the \nparameter count and computational load of the entire model. In the deep \nimage processing stream, small and medium convolutional layers are \nsubstituted with bottleneck transformers (BoT) (Srinivas et al., 2021 ) to \nincorporate a self-attention mechanism. This modification, known as \nELAN-Bot, enables the deep image to focus on global information. \nSubsequently, the multimodal features of equal dimensions are directed \nto the CSFM. The CSFM complements the RGB feature layer at the cor-\nresponding scale by incorporating depth information and global features \nextracted from the depth image. The resulting fused RGB feature map is \nfurther passed onto the feature extraction layer of the next size and the \nnetwork ’s neck layer. The remaining steps of the process retain the \noriginal design of YOLOv7. \n2.2.1. Modal feature extractor backbone \nIn this paper, we propose RGB-backbone networks for processing \nRGB images and depth-backbone networks for processing depth images \nbased on YOLOv7 in order to handle multimodal data. Although the two \nbackbone networks have similar structures, they do not share parame -\nters. The RGB image processing stream retains the backbone network \nmodule and parameters of YOLOv7 itself. However, since the number of \nRGB image channels in the input layers is three times larger than that of \nthe depth image layer, there is a potential redundancy if both modalities are processed using the same parameters. To address this, we have \ndesigned the depth-backbone network by reducing the output channels \nof all modules in equal proportion. The selection of the specific reduc -\ntion ratio parameter and the corresponding experimental results are \nfurther elaborated in Section 3.3.1. BoT is a powerful backbone archi -\ntecture with simple concept, it only replaces the spatial convolution in \nthe last three bottleneck blocks of ResNet with global self-attention to \nimprove target detection recognition, reduce parameters, and further \nreduce latency. The structure of the BoT is shown on the right in Fig. 7 \nand compared to the ResNet bottleneck block, which uses multihead \nself-attention (MHSA) to replace the 3 ×3 convolution layer. \nIn this paper, we propose a module called ELAN-BoT by replacing the \nlast convolutional layer in ELAN-B of YOLOv7. The specific structure of \nELAN-BoT can be seen in Fig. 6. This module retains the original func-\ntionality of ELAN-B while introducing global attention through self- \nattention mechanisms. Our goal is to train the model to learn the \ngrowth characteristics of tea leaves, which are typically located at the \ntop of the plant. Additionally, we aim to optimize the identification of \nshoots based on their texture, color, and shape alone. Since depth images \nexhibit a clear front-back relationship and a more explicit global rela-\ntionship compared to RGB images, we apply ELAN-BoT only to the \ndepth-backbone. To address the computational cost, we replace two \nELAN-B modules with ELAN-BoT in the depth-backbone, specifically, \nthose with output sizes of 40 * 40 and 20 * 20. \n2.2.2. Cross-module spatial attention fusion module \nThe depth image exhibits more prominent edge features and shape \ncharacteristics compared to RGB images, yet it also presents depth gaps \nand blind spots. In some cases, severe gaps can directly cause the target \nin the depth image to vanish, indicating that a mere fusion of RGB and \ndepth images is not an optimal approach. This study introduces a spatial \nattention mechanism into the fusion module to accommodate multi -\nmodal information. By assigning weights to different spatial regions \nFig. 5.Example of the results of the same random data augmentation on RGB-D samples.  Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n6based on the modalities ’ specific traits, the model enhances attention \ntowards targets that possess both RGB and depth features simulta -\nneously. In contrast to conventional complementarity strategies that \ntreat both modalities equally, we acknowledge the unequal information quantity between the two modalities and select a fusion strategy that \nsolely supplements RGB information with depth information. Based on \nthe above, we designed the CSFM, the structure of which is shown in \nFig. 8. \nThe principle can be briefly summarized as follows, spatial weights \nare derived from the depth image, activated by overlaying them with \nspatial weights extracted from the RGB image, and subsequently \nmultiplied solely with the RGB image. This approach simplifies the \nmodel while effectively utilizing the valuable information in the depth \nimage, mitigating the side effect of depth holes and blind spots on the \noverall model. In cases of low-quality or missing depth input, CSFM may \ndegrade to a conventional spatial attention mechanism. Additionally, \nthe fusion of depth information aids in distinguishing between target \ngroups with significant overlap and targets that appear blurred in the \nRGB image due to overexposure. \nThe spatial weight WS is computed using Eq. (1), the procedure is \ndescribed below. First, for the depth feature map (Fdepth) of size H ×W \n×C/2, two depth feature maps of size H ×W ×1 are obtained by \naveraging pooling and maximum pooling operations. Then, the two \nfeature maps are stitched together, and the number of channels is \nreduced to 1 by a standard convolutional layer. The same average \npooling and maximum pooling operations are also performed on the \nRGB feature map of size H ×W ×C to obtain two RGB feature maps of \nsize H ×W ×1. Concatenate the processed depth feature map with two \nFig. 6.The structure of YOLO-RGBDtea.  \nFig. 7.Left: A ResNet bottleneck block. Right: A bottleneck transformer (BoT) \nblock, . \nreproduced from Srinivas et al. (2021) Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n7RGB feature maps, and the number of channels is reduced to 1 by a \nstandard convolutional layer. Then, the spatial weight WS can be ob-\ntained after activation by a sigmoid function. Finally, the spatial weights \nWS are fused with the RGB feature map of size H ×W ×C by the ele-\nmentwise multiplication method to obtain the RGB feature map FRGB-D \nof size H ×W ×C with fused depth information. \nWSσ\u0000\nf\u0000[\nAvgPFRGBCMaxPFRGBCf[\nAvgP\u0000\nFDepth)\nCMaxP\u0000\nFDepth)]]))\n(1)  \nwhere σ denotes the sigmoid function and f denotes a convolutional \nlayer with a kernel size of 5 ×5. \n3.Experimental results and analysis \nThis section presents the evaluation metrics and experimental de-\ntails, and analyzes the results of the study. Two primary sets of experi -\nments are conducted to assess the performance of the proposed \narchitecture: one involving the depth-backbone and the other focusing \non the CSFM. Specifically, experiments are performed around the depth- \nbackbone to compare the model ’s complexity and network performance, \nincorporating BoT for comparison experiments and heatmap visualiza -\ntion analysis. Additionally, different attention fusion mechanisms \naround the CSFM are compared to determine their precision and recall. \nFinally, a comprehensive analysis of our models, YOLO-RGBDtea and \nYOLOv7, is conducted using various tea datasets. \n3.1. Experimental settings and implementation details \nThe experiments in this paper were conducted under Ubuntu 18.04 \nOS, GPU NVIDIA GeForce GTX 3090, and v11.6 CUDA. All programs are \nwritten in Python. We used the official YOLOv7 codebase as the basis, \nused the official pretraining weights provided, and implemented the \nmodifications described in Section 2.2 based on the above. Training is \nperformed using the ADAM optimizer, setting the initial learning rate to \n8E\u00004. 300 batches are run, with a batch size of 4. Finally, non max \nsuppression (NMS) is used to remove detection frames with an inter-\nsection over union (IoU) greater than 0.3 and retain those with confi -\ndence greater than 0.5. 3.2. Evaluation indicators \nThere are many deep learning evaluation metrics, and in this paper, \nparameters, floating point operations per second (FLOPs), precision, \nrecall, F1, and AP50 are used as evaluation metrics mainly to measure \nthe model complexity and model performance. \nThe parameters primarily impact the memory or video memory \nneeded for model operations, while FLOPs represent the computational \nworkload. These metrics serve to gauge the complexity of the model. In \nthis paper, the key parameters and computational effort of the model are \nderived from the convolutional layer, which can be computed using the \nfollowing equations. \nParameterCin×Cout×K×K (2)  \nFLOPs2CinK2\u00001H outWoutCout (3) \nIn the above equation, Cin and Cout respectively denote the number of \nchannels in the input convolutional layer and the number of channels in \nthe output convolutional layer, K denotes the convolutional kernel size. \nHout and Wout denote the width and height of the feature map in the \noutput convolutional layer, respectively. \nPrecision represents the proportion of correct test results to total test \nresults, and recall represents the proportion of correct test results to all \nground truth. They can be calculated by the following equations: \nPrecisionTP\nTPFP(4)  \nRecallTP\nTPFN(5)  \nwhere TP, FP, and FN denote the number of correctly detected objects, \nincorrectly detected objects, and undetected objects, respectively. To \nbetter represent the balance between accuracy and completeness, we use \nthe common F1-score, which is the summed average of precision and \nrecall. Additionally, considering that different confidence thresholds \ncorrespond to different precision, recall and F1-score values, we intro-\nduce average precision (AP) to establish an evaluation metric indepen -\ndent of the confidence threshold. The general definition of AP is the area \nunder the calculation accuracy-recall curve. Both of them can be \nFig. 8.The structure of the cross-module spatial attention fusion module.  Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n8calculated by Eqs. (6) and (7), respectively. \nF12×Precision×Recall\nPrecisionRecall(6)  \nAPα̃1\n0prdr (7)  \nwhere p(r) represents the corresponding precision when the recall is r, α \ndenoted the IoU threshold. To better balance the checking accuracy and \ncompleteness, the confidence threshold is set to 0.2, and the IoU \nthreshold is set to 0.5 in this paper, which is denoted as AP50 in sub-\nsequent experiments. \n3.3. Results and analysis \n3.3.1. Multimodal model lightweighting experiments and training process \nTo illustrate the superiority of our multimodal data processing \napproach and investigate the optimal parameters for both light-\nweighting and performance of the depth-backbone, we conducted \ncomparative experiments as presented in Table 1. As the baseline \ndetection model, we refer to YOLOv7, and the depth-backbone model \nutilizing the same module as the RGB-backbone is denoted as YOLOv7- \nRGBD. In the experiment, we introduced a channel scaling factor for the \ndepth-backbone, selecting coefficients of 1, 0.5, 0.25, and 0.125. These \ncoefficients were chosen to ensure that the number of channels remains \nan integer after multiplication. \nThe results in Table 1 demonstrate the significant improvement in \ndetection performance achieved by incorporating multimodal data, \nguided by prior knowledge. Model (b) represents the prevailing \napproach in current multimodal models, adopting the same depth- \nbackbone settings and parameters as the RGB-backbone to achieve \nnearly optimal detection performance. However, it exhibits 1.7 times \nhigher FLOPs compared to YOLOv7-Base, making training and deploy -\nment more challenging. Model (c), in which all output channels of the \ndepth-backbone are halved, maintains comparable detection perfor -\nmance to (b) with a slight improvement in F1-score. Moreover, it re-\nduces FLOPs by 30.6% compared to (b). These findings indicate that \ncompressing the number of channels within a certain range does not \nadversely affect model performance. To further explore the compress -\nibility of the depth-backbone, we introduce Models (d) and (e). Conse -\nquently, a decrease of 0.78% and 0.71%, respectively, is observed in \nAP50 compared to (c), suggesting that the application of these scaling \nfactors begins to impact the extraction of depth image features. \nConsidering the balance between complexity and performance, we \nselect the parameter settings of Model (c) for subsequent experiments. \nThe detailed structure and relevant parameters are presented in \nTable 2, which includes information about the output size and output \nchannels of each module. ELAN modules are labeled with different \nsuffixes based on their composition. For specific details regarding the \ncomposition of these modules, please refer to the notes located at the bottom right of Fig. 6. \nThe loss value and AP50 curve for YOLOv7 and YOLOv7-RGBD are \npresented in Figs. 9 and 10, respectively. Upon examining the initial \ncurves in both figures, it is evident that YOLOv7-RGBD experiences a \nquicker drop in loss and a more rapid improvement in AP50. These \nfindings demonstrate the valuable contribution of the depth image in \nfacilitating feature extraction during the early stages of training. \nMoreover, upon observing the subsequent course of the curve, it can be \nobserved that YOLOv7-RGBD maintains the stability achieved by \nYOLOv7 training and converges earlier, ultimately resulting in lower \nloss and higher AP50 values at the conclusion of the training process. \n3.3.2. Self-attention mechanism added and its visualization \nIn the depth-backbone, we have innovatively employed BoT as a \nreplacement for a portion of the convolution operation. To demonstrate \nthe effectiveness of this design, a comprehensive comparison experi -\nment is conducted in this section. While keeping all other data constant, \nwe specifically focus on replacing the last convolutional layer in the last \ntwo ELAN-B modules within the depth backbone with BoT. The per-\nformance metrics of the models are provided in Table 4. \nTo accurately gauge the model ’s global attentional capacity resulting \nfrom the inclusion of a self-attention module like BoT, we employ \ngradient-weighted class activation mapping (Grad-CAM) (Selvaraju \net al., 2016 ) for visualization. Grad-CAM provides a visual representa -\ntion of the model ’s perception regarding the location of the target based \non tea shoots. By leveraging the gradient of any target concept, Grad- \nCAM maps it onto the final convolution layer, generating a coarse \nlocalization map displayed through weighted visualizations. These \nweight values are depicted with decreasing colors ranging from red to \nyellow, green, and blue. In this scheme, regions with a redder color \nindicate the greater significance of their contribution in the detection of \ntea shoots. \nIn Fig. 11, we present heatmaps depicting three different view ranges \nof premium tea for comparative analysis. Observing the first row, it is \nevident that all three models accurately detect larger targets. However, \nwhen faced with smaller and overlapping target groups, the disparity \nbetween the models becomes substantial. The YOLOv7 model exhibits \nlimited information gathering for identifying small targets, and struggles \nwith accurately identifying targets within overlapping groups. On the \nother hand, the multimodal models showcased in columns (c) and (d) \nbenefit from the input of multi-class information, enabling them to \nidentify more targets compared to YOLOv7. These multimodal models \nalso demonstrate superior segmentation ability when it comes to over-\nlapping target groups. Further examining columns (c) and (d), the \nincorporation of the self-attention mechanism noticeably broadens the \nmodel ’s understanding of the target, extending beyond merely the shoot \ntip. These models also recognize the connected stems, leaves, and stalks \nrelated to the target. The visualization results obtained from Grad-CAM \nprovide empirical evidence of the intuitive enhancement imparted by \nmultimodal input and the inclusion of BoT. Table 1 \nComparison results of models with different numbers of depth-backbone \nchannels.  \n# Model Depth-backbone \nchannel scale \nfactor Param \n(M) FLOPs \n(G) F1 AP50 \n(a) YOLOv7 / 36.49 103.50 0.868 87.75% \n(b) YOLOv7- \nRGBD 1 49.19 170.82 0.900 90.42% \n(c) YOLOv7- \nRGBD 0.5 39.67 120.33 0.905 90.31% \n(d) YOLOv7- \nRGBD 0.25 37.28 107.71 0.891 89.53% \n(e) YOLOv7- \nRGBD 0.125 36.69 104.55 0.894 89.60%  Table 2 \nFeature extractor architecture and parameters of the backbone network.  \nModule Output \nSize Output channel (RGB- \nbackbone) Output channel (depth- \nbackbone) \nCBS1 640 ×640 32 16 \nCBS2 320 ×320 64 32 \nCBS3 320 ×320 64 32 \nCBS4 160 ×160 128 64 \nELAN-1 160 ×160 256 128 \nMPConv1 80 ×80 256 128 \nELAN-2 80 ×80 512 256 \nMPConv2 40 ×40 1024 512 \nELAN-3 40 ×40 1024 512 \nMPConv3 20 ×20 1024 512 \nELAN-4 20 ×20 1024 512  Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n93.3.3. Attentional fusion mechanism comparison experiment \nTo evaluate the performance of the designed Cross-module Spatial \nattention Fusion Module, this section conducts comparative experiments on attention mechanisms. YOLOv7 serves as the base model, and four \nattention mechanisms are considered: namely, SE (Hu et al., 2020 ), \nCBAM (Woo et al., 2018 ), ECA (Wang et al., 2020 ), and our proposed \nCSFM. YOLOv7 represents a unimodal model that solely processes RGB \nimages. To ensure fair comparisons with multimodal data, we employ \nthe Gated Information Fusion (GIF) network (Kim et al., 2018 ) as a \ncommon multimodal fusion network in constructing the multimodal \nmodel. The two types of feature maps undergo the attention mechanism \nprior to entering the GIF network. The deployment location of the GIF \nnetwork is depicted as the placement of the three CSFM networks in \nFig. 6. The experimental configuration for each group is detailed in \nTable 3, with separate evaluations of precision, recall, F1 score, and \nAP50 for these models. \nThe experimental findings are presented in Table 3. Notably, the \nFig. 9.Training and validation loss curves for two models.  \nFig. 10.AP50 curves for two models.  \nTable 3 \nResults of each training with different attention mechanisms.  \n# Model Precision Recall F1 AP50 \n(a) YOLOv7 86.11% 84.38% 0.852 87.02% \n(b) YOLOv7-GIF 87.30% 86.44% 0.868 87.91% \n(c) YOLOv7-GIF-SE 87.23% 85.66% 0.864 87.75% \n(d) YOLOv7-GIF-CBAM 90.46% 86.62% 0.885 89.16% \n(e) YOLOv7-GIF-ECA 89.55% 88.17% 0.889 89.24% \n(f) YOLOv7-CSFM (ours) 92.20% 88.74% 0.904 90.31%  Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n10results demonstrate that even the standard GIF fusion mode outperforms \nYOLOv7, highlighting the positive influence of multimodal information \ninput on the model. However, integrating SE did not yield a substantial \nenhancement in the model ’s performance. In contrast, the ECA, a type of \nchannel attention mechanism, exhibited significant improvements in the \nmodel. Specifically, in comparison to model (b), its precision improved \nby 2.25% and recall by 1.73%. CBAM combines both the original \nchannel attention mechanism and spatial attention mechanism. \nCompared to SE, we observe that the spatial attention mechanism \ndirectly enhances the tea shoot detection model, surpassing even the \nelaborately designed ECA in terms of precision. This observation jus-\ntifies our focus on the spatial attention mechanism in the design of our \nattention mechanism. Model (f) demonstrates the significant improve -\nment of our proposed CSFM in metrics and presents a well-balanced \ntradeoff between precision and recall. Additionally, we note that \nimproving recall is more challenging than improving precision, owing to \nthe small and heavily overlapping target groups of tea shoot targets. This \nfactor necessitates careful consideration for effective recall improve -\nment in subsequent stages. \n3.3.4. Ablation study of detection module \nIn order to facilitate comparison of all our improvements based on \nYOLOv7, we conducted an ablation study on the detection model, and \nthe relevant experimental settings and data are shown in Table 4. It is \nclear from the table that the multimodal model has an inherent advantage over the unimodal one. Compared to the common GIF fusion \nmethod, the model using CSFM which we designed has a more obvious \nimprovement. This proves that the design of the fusion method between \nmodes has a crucial role in the multimodal domain. The model YOLO- \nRGBDtea with the self-attention mechanism added to YOLOv7-RGBD \nhas some improvement in overall performance, the most obvious of \nwhich is the recall of the model. \n3.3.5. Visualization results on the tea dataset \nTo facilitate a more intuitive comparison between YOLOv7 and \nYOLO-RGBDtea, this subsection presents a visual analysis of the iden-\ntification results for different types of samples. This approach allows us \nto gain a clearer understanding of the models ’ strengths and weaknesses. \nThe input data for both modes is displayed, with calibration only applied \nto the RGB image. The colored bounding boxes indicate the recognition \nresults of the respective models, while a yellow bounding box denotes a \nsignificant discrepancy from the results of another model at the same \nposition. It is important to note that the depth image is visualized for \nillustrative purposes, and the actual input for the model is a single- \nchannel grayscale map. \nIn Fig. 12, we conducted visualizations and analyses on samples with \nvarying target amounts. From top to bottom, the target amount pro-\ngressively increased in the samples. Both models demonstrate accurate \ntarget identification when faced with larger targets. Upon comparing the \nnumber and positions of the yellow boxes from the two models, it be-\ncomes evident that YOLO-RGBDtea identifies a higher overall number of \ntargets compared to YOLOv7. Additionally, a majority of these targets \nare distant, small targets located at the edges or groups of targets with \nhigh overlap. Based on these observations, we can conclude that YOLO- \nRGBDtea outperforms YOLOv7 in samples with a higher quantity of \ntargets and exhibits significant improvements in recognizing small tar-\ngets and groups of targets with high overlap. \nGiven the presence of diverse and unstable environmental variations \nin real tea plantations, we carefully selected specific samples for \nexperimentation to assess the robustness of our model. Our sample se-\nlection included images affected by blur caused by wind blowing or \ncamera shaking, overexposed samples under intense natural lighting \nFig. 11.Illustration of the self-attention weight map generated based on the global context modeling module. Column (a) is the input image, Column (b) is the \nheatmap generated by YOLOv7, Column (c) is the heatmap generated by the multimodal model YOLOv7-RGBD, and Column (d) is the heatmap generated by YOLO- \nRGBDtea with the addition of the BoT. \nTable 4 \nAblation study of detection module.  \nModel YOLOv7 YOLOv7-GIF YOLOv7-RGBD YOLO-RGBDtea \nRGB √ √ √ √ \nDepth  √ √ √ \nBoT    √ \nCSFM   √ √ \nPrecision 86.11% 87.30% 92.20% 92.40% \nRecall 84.38% 86.44% 88.74% 90.06% \nF1 0.852 0.868 0.905 0.912 \nAP50 87.02% 87.91% 90.31% 91.12%  Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n11conditions, and tea samples from the same species captured during \ndifferent seasons. \nComparing and analyzing the results presented in Fig. 13, several \nobservations can be made. In sample (a), the presence of shaking in \neither the tea plants or the equipment resulted in defects in the collected \ndata. This is evident in the RGB image, where the targets appear blurred \nand the depth image shows serious fragmentation. Notably, even with \npartial missing depth image information, YOLO-RGBDtea outperforms YOLOv7 in terms of target identification, indicating that our model is \nrobust to low-quality depth images. In sample (b), overexposure causes \ncertain tea color features in the RGB image to be inconspicuous and the \ntexture to almost disappear. In contrast, the depth image provides more \ninformative details in the overexposed area. The recognition results of \nsample (b) demonstrate a significant improvement in identifying over-\nexposed targets by YOLO-RGBDtea. Lastly, sample (c) consists of tea \nsamples from different seasons, and neither model performs well in this \nFig. 12.Visualization results on the partial tea dataset.  \nFig. 13.Visualization results on special tea samples. From top to bottom are (a) fuzzy samples, (b) overexposed samples, and (c) samples of the same tea species in \nspring (the samples of dataset captured in autumn). Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n12scenario. The variation in posture between spring and autumn is highly \nsignificant, even for the same tea. Spring tea shoots are more tender, \nsmaller, and densely distributed, with a shorter distance between buds \nand leaves. In contrast, autumn tea shoots are larger, with a greater \ndistance between buds and leaves. Nevertheless, YOLO-RGBDtea \nconsistently identifies targets more accurately than YOLOv7. In \nconclusion, YOLO-RGBDtea exhibits improved robustness when con-\nfronted with unstable environmental changes in real-world conditions. \n3.3.6. Discussion \nIn current research studies, there is often a need to employ multiple \nmodules and techniques in order to enhance model performance. \nHowever, this approach requires extensive experimentation and heavily \nrelies on experiential design processes. Consequently, it results in \ncomplex models that are challenging to interpret. In this paper, we draw \ninspiration from the emerging field of multimodal research to address \nthe demanding task of tea shoots recognition by incorporating supple -\nmentary input information. We introduce YOLO-RGBDtea, a multimodal \nmodel that builds upon improvements made to YOLOv7. Our proposed \nmodel preserves all the modules and configurations of YOLOv7 while \nincorporating parallel processing of depth image streams and cross- \nmodal fusion modules. Importantly, this design can be directly inte-\ngrated into the feature extraction network of similar models like Faster \nR-CNN or custom-designed ones. Our approach proves to be efficient \nand straightforward for tackling complex detection tasks. \nMeanwhile, our improvement methods demonstrate a high level of \ninterpretability. Firstly, we incorporate prior knowledge to increase the \namount of input information, enabling the deep learning network to \nacquire more features and enhance target detection. When dealing with \ntargets such as tea shoots that bear a striking resemblance to the back-\nground color, extracting the edge features from RGB images becomes \nchallenging. However, the model experiences a substantial improve -\nment in convergence speed and training outcomes as it incorporates the \nevident edge features from the depth image. Secondly, instead of \nemploying the conventional complementary strategy, we employ a \nunidirectional supplementation strategy that possesses a straightfor -\nward structure and a certain tolerance for additional depth information. \nThe underlying essence of this strategy lies in enhancement. Faced with \ninformation loss caused by depth holes or edge blanks in the depth \nimage, the strategy guarantees that the model does not encounter \nnegative gains. Moreover, this approach reduces the quality demands for \nthe depth image, enhancing its suitability for outdoor depth image \nacquisition. Lastly, we introduce the BoT net as a replacement for the \nconvolution layer in the ELANB module, which allows for the incorpo -\nration of a self-attention mechanism. By establishing a dependency \nrelationship across the entire image, we expand the image’s receptive \nfield and obtain more contextual information. Within the context of this \npaper, the contextual information is representative of the natural growth \npatterns of crops. We aim to enable the model not only to recognize tea \nshoots based on texture and color but also to consider their growth \ncharacteristics, predominantly their location at the top of plants. \nFurthermore, by further determining the target position through the \ngrowth relationship between tea stems, leaves, and tender shoots, the \nmodel’s understanding of this correlation aids in furthering our subse -\nquent generalization experiments. \nAlthough the proposed multimodal YOLOv7 model for tea shoots \ndetection with improvements demonstrates notable advantages, we \nhave encountered limitations in accurately detecting and addressing the \nissue of overlapping shoots. There are two primary reasons contributing \nto this challenge. Firstly, our fusion strategy primarily focuses on spatial \nattention fusion and still employs a similar approach to processing depth \nimages as color images. As a consequence, the extracted features mainly \nconsist of enhanced edge and shape characteristics. However, the most \ndistinctive attribute in depth images lies in the representation of depth \nvalues using pixel values and their relationship to the foreground- \nbackground depth. Future research can explore this unique attribute of depth images to devise more effective fusion strategies. Another factor \nis the significant variation in the size and shape of tea leaves. Tea shoots \npredominantly exhibit a Y-shaped form in images, and the utilization \nrate of quadrilateral bounding boxes for shoot detection is not optimal in \ncovering the targets comprehensively. Consequently, some of the targets \nmight be partially occluded, leading to substantial overlap of bounding \nboxes after calibration. In turn, these occluded boxes are eliminated by \nthe Non-Maximum Suppression (NMS) strategy. This is one of the pri-\nmary factors contributing to the consistently low recall rate. \nIn our study, we conducted experiments to identify the tender shoots \nof tea leaves throughout different seasons. While the outcomes did not \nmeet our ideal expectations, they offered valuable insights for our future \nresearch. Given the wide variety of renowned tea types and the notable \nvariations in size, color, and shape among tea shoots from different \nvarieties, it is imperative to regularly gather and process data to enhance \nthe accuracy of identification. However, this process inevitably demands \na substantial investment of time and financial resources. Therefore, an \nimportant objective of our future research is to design a model that \nexcels at identifying the majority of tea shoots, with a key emphasis on \nimproving the model’s ability to generalize. Initially, we aim for the \nmodel to effectively detect tender shoots in complex outdoor environ -\nments. Additionally, we aspire for the model to simultaneously identify \ntender shoots suitable for harvesting in both spring and autumn, thereby \nmitigating the challenges caused by seasonal differences in tea shoot \ngrowth. Ultimately, our goal is to develop a model capable of detecting \ntender shoots from various tea varieties, including those that are entirely \nunknown to the existing database. \n4.Conclusion \nTo tackle the challenge of a low detection rate of tea leaf tender \nshoots in complex outdoor conditions using RGB images, this study \npresents YOLO-RGBDtea, a multimodal detection model that integrates \nRGB and depth features through YOLOv7. The enhancements made to \nthis model consist of: (1) leveraging multimodal data to overcome the \nlimitations of single-modal data detection; (2) optimizing network pa-\nrameters for depth image feature extraction and employing lightweight \nprocessing techniques; (3) introducing a self-attention mechanism to \nenhance the model’s perception of global dependencies; and (4) pro-\nposing a unidirectional complementary cross-modal fusion module. \nOur proposed multimodal detection model, YOLO-RGBDtea, ach-\nieves an AP50 of 91.12%, an accuracy rate of 92.40%, a recall rate of \n90.06%, and an F1 score of 0.912 when confronted with complex out-\ndoor tea shoot samples. Compared to YOLOv7, it enhances AP50 by \n4.10%, thus improving the balance between detection precision and \nrecall. Furthermore, in contrast to conventional RGB-D modal process -\ning approaches, we manage to reduce FLOPs by 30.6% while only \nexperiencing a minor fluctuation of 0.1% in AP50. This indicates that by \nincreasing the parameter size of YOLOv7 by a mere 17.8%, we can \nincorporate the entire depth modal processing flow, effectively main -\ntaining a balance between model complexity and performance. Addi-\ntionally, we implement a self-attention mechanism by replacing specific \nconvolutional layers in the ELANB module with BoT net. This leads to \nenhancements in both the convergence speed and final convergence \nvalues compared to the model without self-attention. Moreover, the \nheatmaps demonstrate that the model learns to prioritize tea shoots and \ntheir connected stem and leaf parts, aligning with the human practice of \nidentifying tea leaf tender shoots. \nThis study presents a multimodal fusion module CSFM based on a \nspatial attention mechanism that leverages multi-scale depth image \nfeatures to provide supplementary guidance for RGB image learning in \ndepth networks. The proposed fusion module effectively addresses the \nissue of information loss in depth images by maximizing the utilization \nof valuable information from the depth map to enhance the features of \nRGB images, thereby significantly improving the effectiveness of feature \nextraction in complex images. Comparative analysis shows that our Y. Wu et al.                                                                                                                                                                                                                                      \n\nComputers and Electronics in Agriculture 216 (2024) 108541\n13attention mechanism outperforms the multi-modal feature fusion \nmethod without an attention mechanism, resulting in a 3.21% increase \nin AP50 for object detection. Additionally, when compared to the pop-\nular attention mechanism modules SE, CBAM, and ECA, our proposed \nattention mechanism achieved respective improvements in AP50 by \n2.56%, 1.15%, and 1.07%. \nYOLO-RGBDtea has also achieved good detection results in the face \nof tea shoot data in a complex outdoor environment. This includes a \nsignificant improvement over YOLOv7 in the face of small targets, high \ntarget overlaps, and high environmental exposure, verifying the \nrobustness of the model. In addition to optimizing performance, we have \nnot disrupted or replaced any module of the original YOLOv7 during the \nmodel improvement. This allows for the easy incorporation of related \nenhancements into the feature extraction step of other models while \nensuring no adverse effects on the original model. \nThis study presents an improved approach to integrate depth feature \nlayers and RGB feature layers, and performs experiments on the detec -\ntion of tender tea shoots in complex outdoor environments. The research \nholds both theoretical and practical implications for the intelligent and \nprecise harvesting of crops, specifically including premium tea leaves. In \nfuture investigations, the goal is to develop more refined processing \nmethods for depth images to devise fusion strategies, thereby enhancing \ndifferent performance aspects of the model. Additionally, the study aims \nto explore multi-modal models with superior generalization capabilities, \nenabling the identification of a wider range of premium tea varieties \nusing limited samples. \nCRediT authorship contribution statement \nYanxu Wu: Conceptualization, Formal analysis, Investigation, \nMethodology, Resources, Software, Validation, Visualization, Writing – \noriginal draft, Writing – review & editing. Jianneng Chen: Conceptu -\nalization, Funding acquisition, Investigation, Project administration, \nSupervision, Writing – review & editing. Shunkai Wu: Data curation, \nFormal analysis, Resources, Software, Validation. Hui Li: Data curation, \nFormal analysis, Methodology, Resources, Visualization. Leiying He: \nConceptualization, Data curation, Supervision. Runmao Zhao: Investi -\ngation, Investigation, Supervision. Chuanyu Wu: Conceptualization, \nProject administration, Supervision. \nDeclaration of Competing Interest \nThe authors declare that they have no known competing financial \ninterests or personal relationships that could have appeared to influence \nthe work reported in this paper. \nData availability \nData will be made available on request. \nAcknowledgements \nThis study was financially supported by the National Natural Science \nFoundation of China (Grant No. U23A20175, 51975537, 52105284) and \nthe “Leading Goose ” R&D Program of Zhejiang (2022C02052). The \nauthors also acknowledge the Tea Research Institute, Chinese Academy \nof Agricultural Sciences for their assistance in conducting field \nexperiments. References \nCao Jingjun, 2021. Deep Learning-Based Vision System for Agaricus Bisporus Picking \nRobot. Chinese Academy of Agricultural Sciences, PhD dissertation. doi:10.27630/d. \ncnki.gznky.2021.000062. \nFu, L., Majeed, Y., Zhang, X., Karkee, M., Zhang, Q., 2020. Faster R-CNN –based apple \ndetection in dense-foliage fruiting-wall trees using RGB and depth features for \nrobotic harvesting. Biosyst. Eng. 197, 245–256. \nGen˘e-Mola, J., Vilaplana, V., Rosell-Polo, J.R., Morros, J.-R., Ruiz-Hidalgo, J., \nGregorio, E., 2019. KFuji RGB-DS database: Fuji apple multi-modal images for fruit \ndetection with color, depth and range-corrected IR data. Data Brief 25, 104289 . \nGupta, S., Hoffman, J., Malik, J., 2015. Cross modal distillation for supervision transfer. \nCoRR . \nHan, Y.u., Xiao, H., Qin, G., Song, Z., Ding, W., Mei, S., 2014. Developing situations of tea \nplucking machine. ENG 06 (06), 268–273. \nHu, J., Shen, L.i., Albanie, S., Sun, G., Wu, E., 2020. Squeeze-and-excitation networks. \nIEEE Trans. Pattern Anal. Mach. Intell. 42 (8), 2011 –2023 . \nKim, J., Koh, J., Kim, Y., Choi, J., Hwang, Y., Choi, J.W., 2018. Robust deep multi-modal \nlearning based on gated information fusion network. In: Asian Conference on \nComputer Vision, pp. 90–106. \nLi, Z., et al., 2016. RGB-D scene labeling with long short-term memorized fusion model. \nCoRR . \nLi, Y., et al., 2022. High-efficiency tea shoot detection method via a compressed deep \nlearning model. Int. J. Agricult. Biol. Eng. 003, 015. \nLi, G., Gan, Y., Wu, H., Xiao, N., Lin, L., 2019. Cross-modal attentional context learning \nfor RGB-D object detection. IEEE Trans. Image Process. 28 (4), 1591 –1601 . \nLi, L., Zhao, S., Sun, R., Chai, X., Zheng, S., Chen, X., Lv, Z., Doulamis, A.D., 2021. AFI- \nNet: attention-guided feature integration network for RGBD saliency detection. \nComput. Intell. Neurosci. 2021, 1–10. \nLiu, L.i., Fan, Y., Fu, H., Chen, F., Ni, C., Wang, J., Yin, Q., Mu, Q., Yang, T., She, Y., \n2017. “Turn-off ” fluorescent sensor for highly sensitive and specific simultaneous \nrecognition of 29 famous green teas based on quantum dots combined with \nchemometrics. Anal. Chim. Acta 963, 119–128. \nNguyen, T.T., Vandevoorde, K., Wouters, N., Kayacan, E., De Baerdemaeker, J.G., \nSaeys, W., 2016. Detection of red and bicoloured apples on tree with an RGB-D \ncamera. Biosyst. Eng. 146, 33–44. \nPiao, Y., et al., 2020. Depth-induced multi-scale recurrent attention network for saliency \ndetection. 2019 IEEE/CVF International Conference on Computer Vision (ICCV) . \nRedmon Joseph, et al., 2015. You only look once: unified, real-time object detection. \nCoRR abs/1506.02640. \nInkyu Sa, et al., 2016. DeepFruits: a fruit detection system using deep neural networks. \nSensors 16.8, doi:10.3390/s16081222. \nSelvaraju, R.R., et al., 2016. Grad-CAM: Why did you say that? Visual explanations from \ndeep networks via gradient-based localization. CoRR . \nSrinivas, A., et al., 2021. Bottleneck Transformers for Visual Recognition. \nSun, X., et al., 2019. Detection algorithm of tea tender buds under complex background \nbased on deep learning. J. Hebei Univ. (Nat. Sci. Ed.) 39 (02), 211–216. \nSun, Q., Chai, X., Zeng, Z., Zhou, G., Sun, T., 2022. Noise-tolerant RGB-D feature fusion \nnetwork for outdoor fruit detection. Comput. Electron. Agric. 198, 107034 . \nTao, W.A., et al., 2021. Tea picking point detection and location based on Mask-RCNN. \nInf. Process. Agricult. (2021). \nTao, Y., Zhou, J., 2017. Automatic apple recognition based on the fusion of color and 3D \nfeature for robotic fruit picking. Comput. Electron. Agric. 142, 388–396. \nTu, S., Xue, Y., Zheng, C., Qi, Y.u., Wan, H., Mao, L., 2018. Detection of passion fruits and \nmaturity classification using red-green-blue depth images. Biosyst. Eng. 175, \n156–167. \nWang, Q., et al., 2020. ECA-Net: Efficient channel attention for deep convolutional \nneural networks. In: Proceedings of the IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition. https://doi.org/10.1109/ \nCVPR42600.2020.01155 . \nWang, C.Y., Bochkovskiy, A., Liao, H., 2022. YOLOv7: Trainable bag-of-freebies sets new \nstate-of-the-art for real-time object detectors. arXiv e-prints. \nWoo, S., Park, J., Lee, J.Y., Kweon, I.S., 2018. Cbam: Convolutional block attention \nmodule. In: Proceedings of the European Conference on Computer Vision, pp. 3–19. \nXu Gaojian, Zhang Yun, Lai Xiaoyi, 2020. Faster R-CNN deep network-based image \nrecognition of tea leaves shoots. J. Optoelectron. ⋅Laser 31(11) (2020) 1131 –1139, \ndoi:10.16136/j.joel.2020.11.0164. \nXu, W., Zhao, L., Li, J., Shang, S., Ding, X., Wang, T., 2022. Detection and classification of \ntea buds based on deep learning. Comput. Electron. Agric. 192, 106547 . \nYang, H., Chen, L., Chen, M., Ma, Z., Deng, F., Li, M., Li, X., 2019. Tender tea shoots \nrecognition and positioning for picking robot using improved YOLO-V3 model. IEEE \nAccess 7, 180998 –181011 . \nYu, L., Xiong, J., Fang, X., Yang, Z., Chen, Y., Lin, X., Chen, S., 2021. A litchi fruit \nrecognition method in a natural environment using RGB-D images. Biosyst. Eng. 204, \n50–63. Y. Wu et al.                                                                                                                                                                                                                                      \n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nJANUS® G3 Blood iQ™ Workstation\nAPPLIED GENOMICS\nFor research use only. Not for use in diagnostic procedures.\nTRACEABLE, EFFICIENT, & REPRODUCIBLE  \nBLOOD FRACTIONATION\n\nPerkinElmer understands centralized biobanking facilities face demanding challenges to setup an efficient \nand scalable workflow for storage and processing of biological specimens. These challenges include: sample \ntraceability, blood fractionation, quality control, and sample preparation for genetic analysis. The new JANUS® \nG3 Blood iQ™ workstation is the first step in a seamless, efficient, and traceable workflow for the genetic analysis \nof cfDNA, cfRNA, and genomic DNA from fractionated blood. \nThe JANUS® G3 Blood iQ™ workstation is the premium biobanking solution for unattended automation of any \nblood fractionation workflow. The JANUS® G3 Blood iQ™ workstation combines:\n• Deep-learning based image analysis\n• Highly reproducible liquid transfer technology\n• Downstream gripper\n• Deck capacity\n• Out-of-the-box protocols\nThe JANUS® G3 Blood iQ™ workstation guarantees traceable, efficient, and reproducible blood fractionation \nnecessary for the isolation of gDNA, cfDNA, and cfRNA from buffy coat and plasma layers of fractionated blood.\nVolume Verification or Volume-driven Pipetting: Two Ways to Automate \ncfDNA and cfRNA Workflows\nVolume verification allows the user to set a desired level of plasma \nrequired to provide an adequate yield for downstream analysis. \nThe JANUS® G3 Blood iQ™ workstation verifies the volume in each \ntube and will only process those with adequate available plasma.\nIn volume-driven pipetting the JANUS® G3 Blood iQ™ workstation \ntransfers all available plasma volume based on the calculated \nplasma height. This is particularly useful in some cfDNA workflows \nwhere obtaining the highest volume of plasma is necessary to \nincrease the likelihood of uncovering a rare variant or mutation.\nPRECISE REFORMATTING \nOF PLASMA LAYER \nFROM CENTRIFUGED \nWHOLE BLOODHIGH YIELD & PURITY OF \nEXTRACTED DNA OR RNA \nFROM PLASMAPRECISE FRAGMENT \nSIZE DISTRIBUTION \n& QUANTITATION OF \nPURIFIED DNA OR RNAAUTOMATION FOR \nREPRODUCIBLE, LIBRARY \nCONSTRUCTION FROM \ncfDNA OR cfRNAPRECISE FRAGMENT \nSIZE DISTRIBUTION \n& QUANTITATION OF \nLIBRARIES \nBlood Fractionation:\nJANUS® G3 Blood iQ™ WorkstationNGS Library Preparation: \nSciclone® G3 NGSx iQ™ Workstation\ncfDNA Library Preparation:\nNEXTFLEX® Rapid DNA-Seq Kit 2.0\nSmall RNA Library Preparation:\nNEXTFLEX® Small RNA-seq Kit v3Cell-Free Nucleic Acid Extraction: \nchemagic™ 360 Nucleic Acid Extractor\nCell-Free  DNA Extraction: \nNextPrep-Mag™ Automated cfDNA Isolation Kit\nCell-Free  RNA Extraction: \nNextPrep™ Magnazol™ cfRNA Isolation KitCell-free DNA & RNA Quality Control: \nLabChip® GX-Touch™ Microfluidic TechnologyLibrary Quality Control: \nLabChip® GX-Touch™ Microfluidic Technology\nEither method creates an efficient and reliable first step in \nthe genetic analysis of fractionated blood. This powerful tool \ncombined with other PerkinElmer products, such as the chemagic™ \n360 nucleic acid extractor, LabChip® GX-Touch™ nucleic acid \nanalyzer, and the Sciclone® G3 NGSx iQ™ workstation, enables a \nfully automated genetic analysis workflow from blood, a common \nsample matrix for complex workflows.\nAUTOMATED cfDNA & cfRNA WORKFLOW\n\nBlood Layer Detection\nProprietary imaging technology allows the JANUS® G3 Blood iQ™ workstation to \nprecisely identify plasma and buffy coat layers in centrifuged blood tubes. A blue \nimage is taken and identifies the buffy layer height based on a bounded square \naround the pixels identified to contain white blood cells. A white image is taken \nand identifies the plasma layer height through points on the plasma layer meniscus \nand their relative position to the buffy layer. This imaging technology is driven by \na deep-learning based image analysis model which precisely identifies the distinct \nblood layers. Subsequently, the JANUS® G3 WinPrep™ software calculates volumes \nbased on the layer height and tube geometries. Height calculations are unique to \neach tube. This important technology outputs the following parameters:\nThis vital information is stored in a CSV file which can be seamlessly integrated with laboratory information management systems. \nThe JANUS® G3 Blood iQ™ workstation provides users with a notification to recover their samples if these variables do not meet the \nuser-assigned target volume.\nThe above graph shows the difference between manually measured plasma heights and measurements from the JANUS® G3 Blood iQ™ \nimaging system. The overall percent accuracy shows the JANUS® G3 Blood iQ™ workstation accurately measures the plasma height of \nfractionated blood.INPUT TUBE \nBARCODES1\nPLASMA \nVOLUME2\nBUFFY COAT \nVOLUME3\nPlasma Height\n\nThe JANUS® G3 Blood iQ™ workstation comes with an 8-channel VariSpan™ pipetting module which enables \nindustry leading accuracy and precision variable pipetting. The four main pipetting tips used in this workstation, \nshown below, allow for complex plasma and buffy coat extraction in fractionated blood.\nJANUS® G3 VARISPAN™ TECHNOLOGY:  \nPRECISION PIPETTING FOR cfDNA, cfRNA & gDNA ANALYSIS\nMatrix Volume Accuracy (Standard Error) Precision (Random Error)\nPlasma 5.0 mL +/-5% 5.0% CV\nPlasma 500 µL +/-10% 5.0% CV\nBuffy Coat 900 µL +/-5% 5.0% CV\nBuffy Coat 100 µL +/-10% 5.0% CV\nDownstream Extraction: Integrated Gripper and Deck Capacity\nThe JANUS® G3 Blood iQ™ workstation is equipped with generous deck capacity and an integrated gripper to fully automate \ndownstream workflows such as magnetic-bead based nucleic acid extraction. This instrument can further prepare wash and elution \nplates for nucleic acid extraction platforms such as the chemagic™ 360 instrument. \nPLASMA TIP  \n5 mL Conductive  \nFilter TipBUFFY COAT TIP  \n900 µL Wide Bore  \nConductive Filter Tip\n\nBuffy Coat DNA Yield\nSamples were processed in an automated workflow using the JANUS® G3 Blood iQ™ workstation. On average, the JANUS® G3 Blood \niQ™ workstation extracted 58.7 µg of total genomic DNA from the available buffy coat layer.\nOut of the Box Protocols\nPerkinElmer specializes in push-button protocols for \nautomation of blood fractionation. At launch, the JANUS® \nG3 Blood iQ™ workstation will be equipped with:\nAll you need to do is input your desired transfer volumes \n(or automated image-determined volumes) and click Go!\nPLASMA LAYER \nREFORMATTING\nBUFFY COAT LAYER \nREFORMATTING\nPLASMA & BUFFY \nCOAT LAYER \nREFORMATTING\n\nFor more information, visit www.applied-genomics.com\nCopyright ©2020 PerkinElmer, Inc. All rights reserved. PerkinElmer® is a registered trademark of PerkinElmer, Inc. All other trademarks are the property of their respective owners.\n \nAG012001_14_BR PKIPerkinElmer, Inc.  \n940 Winter Street  \nWaltham, MA 02451 USA \nP: (800) 762-4000 or \n(+1) 203-925-4602\nwww.perkinelmer.com\n\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nClinica Chimica Acta 531 (2022) 254–260\nAvailable online 12 April 2022\n0009-8981/© 2022 Elsevier B.V. All rights reserved.A deep learning-based system for assessment of serum quality using \nsample images \nChao Yang1, Dongling Li1, Dehua Sun1, Shaofen Zhang, Peng Zhang, Yufeng Xiong, \nMinghai Zhao, Tao Qi, Bo Situ*, Lei Zheng* \nDepartment of Laboratory Medicine, Nanfang Hospital, Southern Medical University, Guangzhou 510515, PR China   \nARTICLE INFO  \nKeywords: \nDeep learning \nSerum quality \nHemolysis \nIcterus \nLipemia ABSTRACT  \nBackground: Serum quality is an important factor in the pre-analytical phase of laboratory analysis. Visual in-\nspection of serum quality (including recognition of hemolysis, icterus, and lipemia) is widely used in clinical \nlaboratories but is time-consuming, subjective, and prone to errors. \nMethods: Deep learning models were trained using a dataset of 16,427 centrifuged blood images with known \nserum indices values (including hemolytic index, icteric index, and lipemic index) and their performance was \nevaluated by five-fold cross-validation. Models were developed for recognizing qualified, unqualified and image- \ninterfered samples, predicting serum indices values, and finally composed into a deep learning-based system for \nthe automatic assessment of serum quality. \nResults: The area under the receiver operating characteristic curve (AUC) of the developed model for recognizing \nqualified, unqualified and image-interfered samples was 0.987, 0.983, and 0.999 respectively. As for subclas -\nsification of hemolysis, icterus, and lipemia, the AUCs were 0.989, 0.996, and 0.993. For serum indices and total \nbilirubin predictions, the Pearson ’s correlation coefficients (PCCs) of the developed model were 0.840, 0.963, \n0.854, and 0.953 respectively. Moreover, 30.8% of serum indices tests were deemed unnecessary due to the \npreliminary application of the deep learning-based system. \nConclusions: The deep learning-based system is suitable for the assessment of serum quality and holds the po-\ntential to be used as an accurate, efficient, and rarely interfered solution in clinical laboratories.   \n1.Introduction \nLaboratory activities are virtually partitioned into three phases: pre- \nanalytical phase, analytical phase, and post-analytical phase. The pre- \nanalytical phase encompasses all the procedures before the laboratory \nanalysis, and is responsible for the majority of the laboratory errors [1]. \nPre-analytical errors can occur in vivo or in vitro. Major in vivo factors \nincluding hemolysis (also occur in vitro), icterus, and lipemia present in \ndiagnostic samples may be regarded as an important pre-analytical \nfactor, which may generate biological, analytical, and physical inter-\nference on diagnostic testing [2]. \nVisual inspection of serum quality is widely used in clinical labora -\ntories, and its accuracy varies among different individuals because of \nenvironmental and physiological factors [3]. To reduce subjectivity, \ncolored charts with corresponding degrees of hemolysis are adopted for \nstandardized handling of hemolyzed samples. However a study reported that it still led to an unacceptably high rate (31%) of incorrectly pro-\ncessed hemolyzed samples in the emergency clinical chemistry labora -\ntory [4]. Thus, visual estimation with standardization or not is still \nsubjective and time-consuming. \nSome sample pretreatment equipment takes pictures to identify un-\nqualified samples based on the traditional image segmentation algo-\nrithm. A sample image is split into two parts and the serum part is used \nto identify hemolysis, icterus, and lipemia by comparison of the RGB \ncolor space. However, this method is susceptible to interference, and \nstudies on the accuracy of this method have not been reported. \nAutomated detection of hemolysis, icterus, and lipemia using H- \nindex, I-index, and L-index on clinical chemistry analyzers is a more \nreliable, accurate, and standardized way for assessment of serum quality \nand is highly recommended [5–7]. But the routine assessment of serum \nindices (or HIL-indices) will take about 10 min for every sample and may \ngenerate suppression of test results which may cause potential harm to \n*Corresponding authors. \nE-mail addresses: bositu@smu.edu.cn (B. Situ), nfyyzhenglei@smu.edu.cn (L. Zheng).   \n1 Chao Yang, Dongling Li, and Dehua Sun contributed equally to this work. \nContents lists available at ScienceDirect \nClinica Chimica Acta \nu{�~zkw! s{yo|kro>! ÐÐÐ1ow�o �to~1m{y2w{m k�o2mmk!\nhttps://doi.org/10.1016/j.cca.2022.04.010 \nReceived 31 December 2021; Received in revised form 9 April 2022; Accepted 9 April 2022   \n\n\nClinica Chimica Acta 531 (2022) 254–260\n255emergency patients. Therefore, the application of fast visual recognition \nfrom images may reduce the unnecessary serum indices measurement \nand be a benefit to patients. \nDeep learning models especially convolutional neural networks \n(CNNs) have achieved human-level performance in object-classification \ntasks and have been widely applied to various clinic medical tasks [8], \nincluding recognition of blood cells [9], TBS classification [10], and \ninterpretation of serum protein electrophoresis [11]. But the deep \nlearning-based method for serum quality assessment is still very rare. \nInspired by the above observations, we proposed a new deep \nlearning-based system for automated assessing serum quality to improve \nthe accuracy of visual recognition and reduce the unnecessary serum \nindices measurement. \n2.Method and materials \n2.1. Dataset selection \nThis study took place at the Department of Laboratory Medicine, \nNanfang Hospital, Southern Medical University, Guangzhou, China. The \nstudy included samples received in the emergency biochemistry labo-\nratory from March 2021 to July 2021. The procedure includes the \nfollowing steps (Fig. 1). Samples were collected with vacuum blood \ncollection tubes and centrifuged at 1917 g for 10 min to separate serum \nfrom blood. Then centrifuged samples were delivered to the Roche \npretreatment 612 (P612) and sample images were photographed by the \ninside camera with a white background panel to eliminate exposure \neffects. HIL-indices were measured by Roche Cobas 702 (C702) which \nwas connected to P612 by automatic transmission track. The cutoff \nvalues of HIL-indices to classify hemolysis, icterus and lipemia were 18 \n(H-index, SI unit, or 29 for conventional unit), 86 (I-index, SI unit, or 5 \nfor conventional unit), and 40 (L-index, without unit), following the \ncriteria previously developed in our laboratory. The laboratory techni -\ncian visually assessed collected images and identified the image- \ninterfered samples at last. The image-interfered class was defined as \nthe serum part totally covered by bad rotation, labels, or handwriting. \nFinally, deep learning models for classification and regression tasks were \ndeveloped and evaluated. \nThis study was reviewed by the local ethics committee, and consent was obtained under reference number NFEC-2021-206. \n2.2. Network architecture and training \nThe Inception-Resnet-V2 (Keras 2.2 with TensorFlow 2.4 as the \nbackend.) network was chosen for our tasks [12]. The resolution of \nsample images was 120x500x3. Training and validation sets were \nrandomly divided at the ratio of 8:2. The models were trained on a \nworkstation with an Intel 9900 k CPU and an NVIDIA GeForce RTX3090 \ngraphics card. The initial learning rate was set as 0.0001 and changed to \n1/2 every 10 epochs. The model was trained up to 120 epochs and the \nweight was saved for the best performance according to the validation \nloss. Five-fold crosschecks for each deep learning model were imple -\nmented and similar performance was achieved (Supplementary \nTables 1–3). \n2.3. Classification tasks \nTo evaluate the classification ability of the deep learning model, a \nclassification model to identify qualified, unqualified and image- \ninterfered samples was developed. The unqualified class was defined \nas HIL-index exceeding the threshold of 18(H-index), 86(I-index), and \n40(L-index). As unqualified samples were in the complex situation of \ncombinations of hemolysis, icterus, and lipemia, a model that can output \ncombined results would be more helpful. Therefore, we further imple -\nmented a binary classification network with a sigmoid activation func-\ntion to output the probability of hemolysis, icterus, lipemia, and image- \ninterfered. \nThe P612 can identify samples into six categories based on the \ntraditional image segmentation algorithm. To evaluate the specific \nperformance of the P612, we planned to train a six-categories deep \nlearning model to compare with it. As a preliminary, 139 hemolytic \nsample images (H-index F19), 51 seriously hemolytic sample images \n(H-index F44), 137 icteric sample images (I-index F86), and 117 \nlipemic sample images (L-index F40) were collected for the P612 image \nclassification calibration before the dataset collecting. Then, a dataset of \n4633 consecutive images was collected as the test dataset to evaluate the \nP612 and the trained six-categories deep learning model. \nFig. 1.Data handling workflow. Centrifuged samples were photographed by the camera, HIL-indices results were used to classify hemolysis, icterus, and lipemia. \nImage-interfered samples were identified by the laboratory technician. Sample images were exported from the LIS and the sample images above exhibited the \ndifferent status of samples. C. Yang et al.                                                                                                                                                                                                                                    \n\n\nClinica Chimica Acta 531 (2022) 254–260\n2562.4. Regression tasks \nTo evaluate the quantitative prediction ability of the deep learning \nmodel, we trained our model to accomplish the regression task (pre-\ndicting H-index, I-index, and L-index) and chose to use mean squared \nerror (MAE) as a loss function. After achieving the good performance of \nprediction of HIL-indices, we implemented a further study of prediction \nof TBIL and TG. First, Samples that tested TBIL and TG by the C702 were \nobtained from the total dataset. Then study participant metadata \nincluding age, sex, TBIL, and TG were collected and analyzed. Finally, a \ndeep learning model for the prediction of TBIL was developed and \nevaluated. \n2.5. Deep learning-based system \nWe aimed to develop a deep learning-based system that combined \nimage identification with chemical assays for reducing unnecessary HIL- \nindices tests. The system was developed based on the Roche platform. \nThe deep learning model was developed as a multi-output network. The \nclassification model was used to identify the image-interfered samples. \nThe regression model interacted with LIS was used to determine whether \nthe sample should implement HIL-indices measurement or cancel tests. \nThen, the test dataset of 4633 consecutive images was used to evaluate the system and another extra dataset was collected after the preliminary \napplication of this system. \n3.Results \n3.1. Samples \nA dataset of 16,427 sample images was collected. Sample images of \nqualified (68.11%, n 11190), unqualified (30.98%, n 5090) and \nimage-interfered (0.89%, n 147) were shown in Fig. 1. The mean age \nof the patients was 45.3 ±18.0, including 9215 men and 7212 women. \nStatistics analysis from a continuous dataset (n 10667, selected \nfrom the total dataset.) showed that unqualified samples reached \n10.38% of all the biochemical emergency samples. Hemolysis accounted \nfor 61.82% was the main cause of the unqualified samples. Samples with \nmultiple combinations of hemolysis, icterus, and lipemia were shown in \nFig. 2a. Hemolytic samples accompanied with lipemia reached 21.88% \nof the unqualified samples, accounted for 35.39% of hemolytic samples \nand 66.58% of lipemic samples. \nParts of sample images interfered with an emergency label, hand -\nwriting, and patient label (Supplementary Fig. 1). We counted the \nsamples where the serum was partly or totally covered with the in-\nterferences in the total dataset. Nearly half (44.63%, n 7332) of the \nFig. 2.Performances of deep learning models for classification tasks. (a) Venn chart of component ratio for hemolysis, icterus, and lipemia. (b) Sensitivities and \nspecificities of the three classes were determined by the maximum of output probabilities. (c-e) ROC curves showing performance of classification model in qualified \nsamples, unqualified samples, and image-interfered samples. (f-i) ROC curves showing performance of binary classification models for hemolysis, icterus, lipemia, \nand PSUM. C. Yang et al.                                                                                                                                                                                                                                    \n\n\nClinica Chimica Acta 531 (2022) 254–260\n257sample images were covered with objects in our laboratory (Only images \nwith totally covered serum parts were counted as the image-interfered \nclass). Among them, 1.64% (n 269) of sample images interfered \nwith an emergency label, 28.03% (n 4604) interfered with hand -\nwriting and 23.31% (n 3829) interfered with a patient label. \n3.2. Classification of unqualified and image-interfered samples \nThe sensitivity and specificity for recognizing unqualified samples \nwere 97.2% and 89.4% (Fig. 2b). The AUC for qualified class was 0.981 \n(95% CI: 0.978 –0.985), unqualified class was 0.983 (95% CI: \n0.976 –0.986) and image-interfered class was 0.999 (95% CI: \n0.998 –1.000). To achieve a better sensitivity of the model, we recalcu -\nlated the specificity of this model. When sensitivity F95%, sensitivity F\n99% and sensitivity 100%, 6.26%, 18.47% and 48.14% qualified \nsamples will be misclassified to unqualified samples. \n3.3. Binary classification of hemolysis, icterus, and lipemia \nThe model achieved good performance for classes of hemolysis, \nicterus and lipemia with AUCs F98% (Fig. 2f-h). Also to distinguish \nunqualified samples with only one network, we determined the sum of \neach class probability: PSUM Phemolysis Piterus Plipemia . The AUC of \nthe PSUM was 0.986 (Fig. 2i), as well as the aforesaid model in identifying \nunqualified samples (AUC 0.983). When class thresholds were set to \nreceive the max AUCs with t hemolysis F0.016, titerus F0.001and tlipemia F\n0.004, the sensitivity and specificity of identifying unqualified samples \nwas 0.982 and 0.865 respectively, achieving a comparable performance \nof PSUM (sensitivity 0.979, specificity 0.931). To know which part of \nthe sample image determined the classification results. Saliency maps \nwere generated and shown in Supplementary Fig. 2. \n3.4. Comparison of the deep learning model and the traditional image \nsegmentation algorithm \nFor the P612 classification results in the test dataset, 75.61% (n \n3024) of qualified samples were misclassified to other classes, which \naccounted for 65% of the test dataset. Among them, 40.46% (n 1640) \nwere misclassified to lipemic samples. 27.73% (n 1124) were mis-\nclassified to icteric samples. 5.57% (n 226) were misclassified to un-\nknown samples. While for the deep learning model, the overall accuracy \nwas greatly improved from 30.19% to 95.85%. Comparing the confusion \nmatrix plot of the P612 (Fig. 3a), very good performance of the deep learning model (Fig. 3b) is observed for all classes. The full statistics of \nprediction quality are provided in Table 1. \n3.5. Prediction of HIL-indices \nA linear correlation between the predicted HIL-indices and measured \nHIL-indices showed strong associations with the coefficient of determi -\nnation (R2) of 0.689, 0.928, 0.720, Pearson ’s correlation coefficient \n(PCC) of 0.840, 0.963, 0.854 and mean Square Error (MAE) of 4.25, \n8.44, 7.63 (Fig. 4a-c). The level of agreement between the predicted HIL- \nindices and measured HIL-indices was assessed using a Bland –Altman \nplot. The deep learning model achieved an intraclass correlation coef-\nficient (ICC) of 0.83 for H-index, 0.96 for I-index, and 0.82 for L-index \n(Fig. 4d-f). The deep learning model estimated the HIL-indices level to a \ngreater extent at high levels than at low levels. \n3.6. Prediction of TBIL \n10,484 samples that tested TBIL by the C702 were obtained from the \ntotal dataset and 1433 samples that tested TG were obtained. The mean \nage of the patients was 48.1 ±20.0, including 6349 men and 4135 \nwomen. The I-index value showed a high correlation with TBIL (PCC \n0.998, P D0.01, Fig. 4g), while the PCC of L-index versus TG was 0.764 \n(P D0.01, Supplementary Fig. 3c). Other correlation analyses for DBIL, \nIBIL, CHOL, HDL, and LDL were exhibited in Supplementary Fig. 3a-f. \nThe PCC of predicted TBIL versus measured TBIL was 0.953, the R2 was \n0.907, and the MAE was 8.808 (Fig. 4h) on the validation dataset (n \n2096). As shown in the Bland –Altman plot (Fig. 4i), the deep learning \nmodel estimated the TBIL level to a greater extent at high levels than at \nFig. 3.Comparison of the P612 and the deep learning model. (a) Confusion matrix plot for the P612. The darkness presents the relative frequency of each class. (b) \nSame as (a), but for the deep learning model. Table 1 \nClass-wise sensitivity and specificity of the Roche P612 and the deep learning \nmodel.  \nClass Roche P612 Deep learning model No. of \nimages Sensitivity Specificity Sensitivity Specificity \nQualified  0.253  0.993  0.974  0.882 4053 \nHemolysis  0.312  0.993  0.618  0.983 262 \nLipids  0.666  0.622  0.755  0.994 180 \nBilirubin  0.709  0.735  0.777  0.995 148 \nUnknown  0.892  0.939  0.678  1.0 56 \nSevere \nhemolysis  0.303  0.991  0.449  0.998 89  C. Yang et al.                                                                                                                                                                                                                                    \n\n\nClinica Chimica Acta 531 (2022) 254–260\n258low levels. What ’s more, the deep learning model showed comparable \nexcellent performance in predicting I-index and TBIL. \n3.7. Deep learning-based system for automated assessing serum quality \nThe improved workflow of automated assessing serum quality in our \nlaboratory was shown in Fig. 5. After the centrifugal sample was pho-\ntographed by the camera, the deep learning model received the sample \nimage for identifying and sent the results to LIS. Once the LIS received \nthe results, it would send messages to the instrument to append serum \nindices or cancel tests. Determined by the classification results, the \nimage-interfered samples were sent to the chemical analyzer and per-\nformed HIL-indices tests additionally, while the other samples need to be \nfurther processed with the quantitative predicted HIL-indices values. \nThe lower and upper limit for each analyte was set as C-2.58SD and C \n2.58SD respectively (C is the cutoff values for different analytes adopted \nfrom the manufacturer ’s declarations. SD is the standard deviation of the \ndifference between predicted and true values in the test set.). When the \npredicted HIL-indices values were greater than the upper limit, the \ninterfered test will be canceled. When the predicted HIL-indices values \nwere lower than the lower limit, the sample can be reported normally. \nWhen the predicted HIL-indices values were between the lower limit and \nthe upper limit, The HIL-indices tests were appended. \nSerum indices tests were reduced for 26.76% (n 1225) of samples on the test dataset (n 4577, excluded 56 image-interfered samples). \nAmong the 26.76% of samples, 1 sample was misclassified for predicted \nserum indices lower than the lower limit but measured serum indices \ngreater than the cutoff values, 3 samples were misclassified for predicted \nserum indices greater than the upper limit but measured serum indices \nlower than the cutoff values. As shown in Supplementary Tabel 4, the \nlower rate of reducing HIL-indices assays was mainly due to the low \ncutoff value of the H-index for HBDH, LDH, and AST. Further steps to \ndefining cutoffs for flagging, alarming, or suppressing test results with \npredicted HIL-indices values in our laboratory should be taken and more \nsamples may avoid HIL-indices tests expectantly. \n3807 sample images were collected from September 2nd to \nSeptember 8th after the primary application of the system. 30.8% of \nsamples avoid HIL-indices assays. \n4.Discussion \nAs shown in Fig. 2a, up to 66.58% of lipemic samples, meanwhile, \naccompanied with hemolysis. Consulting to the reagent instructions, L- \nindex was determinate by a simple measure of two wavelengths of 660 \nnm and 700 nm. However, the lipemia has a broad-spectrum absorption \npeak which covered that of hemolysis (at 570 nm and 600 nm) and \nicterus (at 480 nm and 505 nm). Thus, further studies aimed at dis-\nsecting overlapping interference for serum indices are still needed. \nFig. 4.Performance of deep learning models for HIL-indices and TBIL prediction. (a-c) Scatter plots of HIL-indices prediction. (d-f) Bland –Altman plots of HIL-indices \nprediction. (g) Correlation analysis of I-index versus TBIL. (h) Scatter plot of TBIL prediction. (i) Bland –Altman plot of TBIL prediction. C. Yang et al.                                                                                                                                                                                                                                    \n\n\nClinica Chimica Acta 531 (2022) 254–260\n259The deep learning models presented in this study show outstanding \nperformance on assessing serum quality with sample images under the \nsituation that nearly half of the images interfered with objects. The \nmodels attain the AUCs above 98% in the classification of hemolytic, \nicteric, lipemic, and image-interfered samples. It offers the promise that \nmore accurate models can be achieved once the image interferences are \navoided and sharper cameras are being equipped. Considering the \ncomplex situation in different laboratories, it is recommended that every \nlaboratory should collect their own sample images and train deep \nlearning models to improve the accuracy of image recognition and it is \nthe right time to discard the unstable and ineffective human visual \ncheck. \nHolding unqualified samples for analysis can quickly delay a result to \nunacceptable lengths of time that can potentially cause more harm to the \npatient. With the HIL-indices prediction using sample images, we can \ndiscover the problematic sample and take steps instantly, especially for \nemergency patients. Our model shows over 0.84 PCCs of predict HIL- \nindices versus measured HIL-indices and the PCC of I-index reached \nup to 0.963. It provides the basis for further research in optimizing HIL- \nindices tests workflow and a new method for HIL-indices quality control \n[2]. \nHyperbilirubinemia is a manifestation of underlying liver and biliary \ntract disease. An acute jaundice onset is extremely important and raises \nthe possibility of an acute process such as hepatitis, or complete biliary \ntract obstruction [13]. Delayed diagnosis of biliary atresia is an impor -\ntant cause of pediatric end-stage liver failure and liver transplantation \n[14]. Our deep learning model shows a high PCC of 0.95 in the pre-\ndiction of TBIL using sample images. It can be expected to be developed \nas a new fast and easy method for TBIL measurement and can be applied \nin clinical practice. \nCompare with Roche P612, our model shows a greater anti- \ninterference capability and can output not only classification results \nbut also regression results. With the rapid development of artificial in-\ntelligence, deep learning or other new algorithms will finally replace the \ntraditional image segmentation algorithm and provide more potential \napplications. Further research in transferring the ability of deep learning \nmodels to detect sample clock and serum volume is achievable. \nIn conclusion, our method holds the potential to act as a rapid and \nefficient solution for automated assessment of serum quality and might \nfurther be applied in more laboratories and achieve better performance. Author contributions \nChao Yang: designed research, performed research, designed algo-\nrithm, and wrote the paper. \nDongling Li: performed research and wrote the paper. \nDehua Sun: performed research and wrote the paper. \nShaofen Zhang: established dataset and analyzed data. \nPeng Zhang: established dataset. \nYufeng Xiong: analyzed data and wrote the paper. \nMinghai Zhao: established dataset. \nTao Qi: established dataset. \nBo Situ: designed research, performed research, and wrote the paper. \nLei Zheng: designed research, performed research, and wrote the \npaper. \nDeclaration of Competing Interest \nThe authors declare that they have no known competing financial \ninterests or personal relationships that could have appeared to influence \nthe work reported in this paper. \nAcknowledgment \nThe authors acknowledge the support received from medical workers \nin Department of Laboratory Medicine, Nanfang Hospital, Southern \nMedical University. \nAppendix A.Supplementary material \nSupplementary data to this article can be found online at https://doi. \norg/10.1016/j.cca.2022.04.010 . \nReferences \n[1]G. Lima-Oliveira, W. Volanski, G. Lippi, G. Picheth, G.C. Guidi, Pre-analytical \nphase management: a review of the procedures from patient preparation to \nlaboratory analysis, Scand. J. Clin. Lab. Invest. 77 (3) (2017) 153–163. \n[2]G. Lippi, J. Cadamuro, A. von Meyer, A.-M. Simundic, Local quality assurance of \nserum or plasma (HIL) indices, Clin. Biochem. 54 (2018) 112–118. \n[3]G. Lippi, J. Cadamuro, Visual assessment of sample quality:quo usque tandem ? \nClin. Chem. Lab. Med. (CCLM) 56 (2018) 513–515. \n[4]A.H. Luksic, N. Nikolac Gabaj, M. Miler, L. Dukic, A. Bakliza, A. Simundic, Visual \nassessment of hemolysis affects patient safety, Clin. Chem. Lab. Med. (CCLM) 56 \n(2018) 574–581. \nFig. 5.The improved workflow of the deep learning-based system for automated assessing serum quality. The system was developed as a multi-output network. The \nclassification results were used to identify the image-interfered samples. The regression results interacted with LIS were used to determine whether the sample should \nimplement HIL-indices measurement. C. Yang et al.                                                                                                                                                                                                                                    \n\n\nClinica Chimica Acta 531 (2022) 254–260\n260[5]G. Lippi, J. Cadamuro, A. von Meyer, A. Simundic, F.O.C.C. European, Practical \nrecommendations for managing hemolyzed samples in clinical chemistry testing, \nClin. Chem. Lab. Med. 56 (2018) 718. \n[6]A.-M. Simundic, G. Baird, J. Cadamuro, S.J. Costelloe, G. Lippi, Managing \nhemolyzed samples in clinical laboratories, Crit. Rev. CL Lab. Sci. 57 (1) (2020) \n1–21. \n[7]G. Lippi, P. Avanzini, D. Campioli, G. Da Rin, M. Dipalo, R. Aloe, D. Giavarina, G. \nL. Salvagno, Systematical assessment of serum indices does not impair efficiency of \nclinical chemistry testing: A multicenter study, Clin. Biochem. 46 (13-14) (2013) \n1281–1284. \n[8]A. Esteva, A. Robicquet, B. Ramsundar, V. Kuleshov, M. DePristo, K. Chou, C. Cui, \nG. Corrado, S. Thrun, J. Dean, A guide to deep learning in healthcare, Nat. Med. 25 \n(1) (2019) 24–29. \n[9]C. Matek, S. Schwarz, K. Spiekermann, C. Marr, Human-level recognition of blast \ncells in acute myeloid leukaemia with convolutional neural networks, Nat. Mach. \nIntell. 1 (11) (2019) 538–544. [10] X. Zhu, X. Li, K. Ong, W. Zhang, W. Li, L. Li, D. Young, Y. Su, B. Shang, L. Peng, \nW. Xiong, Y. Liu, W. Liao, J. Xu, F. Wang, Q. Liao, S. Li, M. Liao, Y.u. Li, L. Rao, \nJ. Lin, J. Shi, Z. You, W. Zhong, X. Liang, H. Han, Y. Zhang, N.a. Tang, A. Hu, \nH. Gao, Z. Cheng, L.i. Liang, W. Yu, Y. Ding, Hybrid AI-assistive diagnostic model \npermits rapid TBS classification of cervical liquid-based thin-layer cell smears, Nat. \nCommun. 12 (1) (2021), https://doi.org/10.1038/s41467-021-23913-3. \n[11] F. Chabrun, X. Dieu, M. Ferre, et al., Achieving Expert-Level Interpretation of \nSerum Protein Electrophoresis through Deep Learning Driven by Human \nReasoning, Clin. Chem. (2021). \n[12] C. Szegedy, S. Ioffe, V. Vanhoucke, A. Alemi, Inception-v4, Inception-ResNet and \nthe Impact of Residual Connections on Learning. arXiv e-prints, 2016, pp. \n1602–7261. \n[13] J.I. Sullivan, D.C. Rockey, Diagnosis and evaluation of hyperbilirubinemia, Curr. \nOpin. Gastroen. 33 (2017) 164–170. \n[14] L. Lam, S. Musaad, C. Kyle, S. Mouat, Utilization of Reflex Testing for Direct \nBilirubin in the Early Recognition of Biliary Atresia, Clin. Chem. 63 (2017) \n973–979. C. Yang et al.                                                                                                                                                                                                                                    \n\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nReceived: 28 November 2022 Revised: 7 February 2023 Accepted: 22 February 2023\nDOI: 10.1002/mp.16338\nRESEARCH ARTICLE\nA modality-collaborative convolution and transformer\nhybrid network for unpaired multi-modal medical image\nsegmentation with limited annotations\nHong Liu1Yuzhou Zhuang1Enmin Song1Xiangyang Xu1Guangzhi Ma1\nCoskun Cetinkaya2Chih-Cheng Hung2\n1School of Computer Science and\nTechnology, Huazhong University of Science\nand Technology, Wuhan, China\n2Center for Machine Vision and Security\nResearch, Kennesaw State University,\nKennesaw, Georgia, USA\nCorrespondence\nEnmin Song, School of Computer Science\nand Technology, Huazhong University of\nScience and Technology, Wuhan, 430074,\nChina.\nEmail: enmin_song@126.com\nFunding information\nWuhan Major Science and Technology Project\nfor Breaking Through the Key Bottleneck\nTechnologies, Grant/Award Number:\n2021022002023426; HUST-UIH-IRI Joint\nLaboratory Project for Medical Imaging\nTechnology Research and Medical Product\nDevelopment, Grant/Award Number:\n2021420118000418Abstract\nBackground: Multi-modal learning is widely adopted to learn the latent com-\nplementary information between different modalities in multi-modal medical\nimage segmentation tasks. Nevertheless, the traditional multi-modal learn-\ning methods require spatially well-aligned and paired multi-modal images for\nsupervised training, which cannot leverage unpaired multi-modal images with\nspatial misalignment and modality discrepancy . For training accurate multi-\nmodal segmentation networks using easily accessible and low-cost unpaired\nmulti-modal images in clinical practice, unpaired multi-modal learning has\nreceived comprehensive attention recently .\nPurpose: Existing unpaired multi-modal learning methods usually focus on the\nintensity distribution gap but ignore the scale variation problem between differ-\nent modalities. Besides, within existing methods, shared convolutional kernels\nare frequently employed to capture common patterns in all modalities, but they\nare typically inefﬁcient at learning global contextual information. On the other\nhand, existing methods highly rely on a large number of labeled unpaired multi-\nmodal scans for training,which ignores the practical scenario when labeled data\nis limited. To solve the above problems, we propose a modality-collaborative\nconvolution and transformer hybrid network (MCTHNet) using semi-supervised\nlearning for unpaired multi-modal segmentation with limited annotations, which\nnot only collaboratively learns modality-speciﬁc and modality-invariant repre-\nsentations, but also could automatically leverage extensive unlabeled scans for\nimproving performance.\nMethods: We make three main contributions to the proposed method. First,\nto alleviate the intensity distribution gap and scale variation problems across\nmodalities, we develop a modality-speciﬁc scale-aware convolution (MSSC)\nmodule that can adaptively adjust the receptive ﬁeld sizes and feature\nnormalization parameters according to the input. Secondly, we propose a\nmodality-invariant vision transformer (MIViT) module as the shared bottleneck\nlayer for all modalities, which implicitly incorporates convolution-like local oper-\nations with the global processing of transformers for learning generalizable\nmodality-invariant representations.Third,we design a multi-modal cross pseudo\nsupervision (MCPS) method for semi-supervised learning, which enforces the\nconsistency between the pseudo segmentation maps generated by two per-\nturbed networks to acquire abundant annotation information from unlabeled\nunpaired multi-modal scans.\nResults: Extensive experiments are performed on two unpaired CT and MR\nsegmentation datasets, including a cardiac substructure dataset derived from\n5460 © 2023 American Association of Physicists in Medicine. wileyonlinelibrary .com/journal/mp Med Phys. 2023;50:5460–5478.\n\n\nMCTHNet 5461\nthe MMWHS-2017 dataset and an abdominal multi-organ dataset consisting of\nthe BTCV and CHAOS datasets. Experiment results show that our proposed\nmethod signiﬁcantly outperforms other existing state-of-the-art methods under\nvarious labeling ratios, and achieves a comparable segmentation performance\nclose to single-modal methods with fully labeled data by only leveraging a small\nportion of labeled data. Speciﬁcally, when the labeling ratio is 25%, our pro-\nposed method achieves overall mean DSC values of 78.56% and 76.18% in\ncardiac and abdominal segmentation, respectively, which signiﬁcantly improves\nthe average DSC value of two tasks by 12.84% compared to single-modal U-Net\nmodels.\nConclusions: Our proposed method is beneﬁcial for reducing the annotation\nburden of unpaired multi-modal medical images in clinical applications.\nKEYWORDS\nabdominal multi-organs, attention mechanism, cardiac substructures, semi-supervised learning,\nunpaired multi-modal learning\n1 INTRODUCTION\nMulti-modal imaging technologies provide different\nbut complementary image information of anatomical\nstructures and lesion areas for quantitative analy-\nsis, disease diagnosis, and surgical planning in the\nclinic.1,2Two types of multi-modal images are fre-\nquently used for medical image segmentation tasks.\nOne is multi-modal images obtained by different imag-\ning techniques,3such as computed tomography (CT)\nand magnetic resonance imaging (MRI). The other is\nmulti-sequence images generated by the same imag-\ning technique but different parameters,4such as T1,\nT2, FLAIR, and ADC sequences in MRI. In multi-modal\nmedical image segmentation, multi-modal learning5\ncould guide deep learning-based segmentation net-\nworks (e.g., U-Net6) to learn the latent complementary\ninformation from different modalities, which achieves\nbetter segmentation performance than single-modal\nsegmentation methods. Multi-modal learning methods\nusually employ multi-modal feature fusion strategies to\nlearn multi-modal deep representations in segmenta-\ntion networks,7which mainly include: early fusion,8late\nfusion,9modality-speciﬁc representation,10and hyper-\ndense connection.11However,the traditional multi-modal\nlearning methods using feature fusion strategies require\npaired multi-modal images (i.e., multi-modal scans are\nacquired from the same patient and co-registered\nacross different modalities) for supervised training. In\nunpaired multi-modal images with spatial misalignment\nand different statistical distributions,12these traditional\nmulti-modal learning methods cannot obtain aligned\nmulti-modal representations and usually produce signif-\nicant performance degradation. Furthermore, unpaired\nmulti-modal images are more accessible and less costly\nthan paired multi-modal images in clinical practice.\nHence, unpaired multi-modal learning13–16has recently\nobtained comprehensive attention, which is used toconstruct robust and accurate segmentation methods\nfor jointly segmenting unpaired images from different\nmodalities, especially for non-registered CT and MR\nimages.\nDue to the modality discrepancy between differ-\nent modalities,17unpaired multi-modal segmentation is\nmore difﬁcult and challenging than traditional paired\nmulti-modal segmentation tasks. As shown in Figure 1,\nfor unpaired multi-modal abdominal multi-organ18and\ncardiac substructure19segmentation, the segmenta-\ntion regions and anatomical structures of CT and MR\nimages are consistent. However, the modality discrep-\nancy problem, including spatial misalignment, intensity\ndistribution gap, and scale variation, signiﬁcantly exists\nin the unpaired multi-modal images of Figure 1,resulting\nin the difﬁculty of unpaired multi-modal represen-\ntation learning. Current unpaired multi-modal learn-\ning methods mainly study how to effectively learn\nmodality-speciﬁc representations within each modality\nand modality-invariant representations between differ-\nent modalities.20By using a shared bottleneck layer\nto learn modality-invariant features and multiple inde-\npendent encoders and decoders to learn modality-\nspeciﬁc representations, Valindria et al.13proposed a\ndual-stream encoder-decoder network for accurately\nsegmenting abdominal multi-organs from unpaired CT\nand MR images, called the “X”-shape model. Although\nthe “X”-shape model ﬁrst investigated how to learn\nmodality-speciﬁc and modality-invariant representations\ncollaboratively, this approach suffers from high model\ncomplexity and low representation learning efﬁciency\ndue to too many redundant network layers. To over-\ncome the drawbacks of the “X”-shape model,Dou et al.14\nproposed a compact and lightweight “Chilopod”-shape\nmodel for unpaired multi-modal segmentation, which\nshared all convolutional kernels for capturing common\npatterns (e.g., geometry shapes or anatomical struc-\ntures) in all modalities and utilized modality-speciﬁc\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n5462 MCTHNet\nFIGURE 1 Visual comparison and statistical analysis of unpaired multi-modal abdominal multi-organ and cardiac substructure scans. (a)\nand (d) are visual segmentation results of axial images, which show the spatial misalignment and scale variation problems between different\nmodalities. (b) and (e) are intensity distributions of segmentation regions in different modalities. (c) and (f) are the voxel percentages of differ ent\nregions in the segmentation results, which also demonstrate the scale variation problem between different modalities and patients.\nfeature normalization layers21to compute speciﬁc\nintensity distribution of each modality . Moreover, some\nrecent studies22,23also demonstrated that the modality-\nspeciﬁc feature normalization layer could tackle the\nintensity distribution gap between different modalities by\nusing multiple sets of individual normalization param-\neters, which is effective for representation learning,\ndomain adaptation, or domain generalization tasks in\nunpaired multi-modal images.\nAlthough the above methods have achieved promis-\ning results in unpaired multi-modal segmentation, there\nare still several issues that limit their segmentation per-\nformances and generalization abilities. First, the existing\nmethods mainly focus on the intensity distribution gap24\nbut ignore the scale variation problem of segmentation\nregions in unpaired multi-modal images. As shown in\nFigure 1, due to different imaging parameters and indi-\nvidual differences among patients, the same anatomical\nstructures have different scale sizes between differ-\nent modalities, resulting in the scale variation problem.\nSince current unpaired multi-modal segmentation net-\nworks using ﬁxed convolutional kernel size cannot\nlearn the multi-scale information of segmented regions,\nthe scale variation problem possibly reduces the seg-\nmentation accuracy and generalization performance\nacross modalities. Second, shared convolutional layers\nare widely used for learning modality-invariant repre-\nsentations in unpaired multi-modal learning. However,\nconvolution operations are good at modeling local rela-\ntions but inefﬁcient at capturing global relations between\ndistant regions, resulting in the lack of global contextual\ninformation in unpaired multi-modal segmentation net-\nworks. Vision Transformer (ViT)25is currently applied\nto obtain rich global contextual information in medi-cal image segmentation.26However, due to the lack\nof spatial inductive biases,27,28the original ViT mod-\nule requires a large number of parameters to learn\nglobal representations and exhibits sub-standard opti-\nmization ability, which has a risk of overﬁtting for\nunpaired multi-modal segmentation tasks with small-\nscale datasets.Third,most of the current methods highly\nrely on a considerable amount of labeled unpaired\nmulti-modal scans for supervised training. However,\nfor the scenario when labeled data is scarce and\nunlabeled data is abundant in clinical practice, these\nmethods using supervised learning show signiﬁcant\nperformance degradations due to the lack of sufﬁ-\ncient supervision signals.29,30By automatically learning\nabundant annotation information from extensive unla-\nbeled scans, semi-supervised learning methods based\non pseudo-label learning or consistency-based regu-\nlarization are applied in unpaired multi-modal segmen-\ntation tasks with limited annotations.31Nevertheless,\nexisting methods32,33still require additional genera-\ntive adversarial networks and deformable registration\nnetworks to overcome the modality discrepancy prob-\nlem and generate the cross-modality training samples,\ncausing high model complexity and training instability .\nMeanwhile, most of the existing methods only learn\nannotation information of unlabeled images from a\nsingle modality by one of pseudo-label learning or\nconsistency-based regularization methods, which lacks\nthe capabilities to utilize the complementary information\nof different modalities and cannot leverage the strengths\nof both pseudo-label learning and consistency-based\nregularization for better performance improvement.34,35\nTo solve the above problems, we propose a Modality-\ncollaborative Convolution and Transformer Hybrid\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nMCTHNet 5463\nNetwork (MCTHNet) using semi-supervised learning\nfor unpaired multi-modal segmentation with limited\nlabels. In our proposed method, we ﬁrst construct a\nModality- Speciﬁc Scale-aware Convolution (MSSC)\nmodule to efﬁciently extract modality-speciﬁc represen-\ntations and adaptively adjust the receptive ﬁeld size\nof segmentation networks, which can overcome the\nintensity distribution gap and scale variations between\ndifferent modalities. Then, we design a lightweight and\nmemory-efﬁcient Modality- Invariant Vision Transformer\n(MIViT) module as the bottleneck layer shared for all\nmodalities. Compared with other heavy-weight trans-\nformer modules that lack spatial inductive biases, MIViT\nimplicitly incorporates convolution-like local operations\nwith global processing of transformers for learning\ngeneralizable modality-invariant representations by\nfewer learnable parameters. Finally, by enforcing the\nconsistency between the pseudo segmentation maps\ngenerated by two unpaired multi-modal perturbed\nnetworks during training, we develop a Multi-modal\nCross Pseudo Supervision (MCPS) method for semi-\nsupervised learning. To evaluate the effectiveness of\nour proposed method, we tested our proposed method\nin unpaired CT and MR abdominal multi-organ and\ncardiac substructure segmentation tasks with limited\nannotations. In our experiments, by varying the labeling\nratio of the training data, we not only compare the\nsegmentation performance of our proposed method\nwith other state-of-the-art methods under fully labeled\ndata, but also particularly focus on the effectiveness of\nour proposed method under the scenario when labeled\ndata is scarce.\nIn summary, the main contributions of this paper are\nas follows:\n1. We propose a MSSC module in the encoding stage,\nwhich can alleviate both the intensity distribution\ngap and scale variation problems across different\nmodalities and scans.\n2. We present a MIViT module with spatial inductive\nbiases as the bottleneck layer, which combines local\nspatial information and global contextual information\nfor better representation learning.\n3. We design an effective MCPS method for semi-\nsupervised learning, which signiﬁcantly improves\nthe segmentation performances of all modalities\nby learning abundant annotation information from\nextensive unlabeled unpaired multi-modal scans.\n4. By combing all proposed components and methods,\nwe construct a robust and uniﬁed semi-supervised\nsegmentation framework with the merits of both con-\nvolutions and transformers for accurately segmenting\nunpaired multi-modal images with limited labels.\nExtensive experiments on two unpaired multi-modal\nsegmentation tasks are performed to demonstrate\nthe effectiveness and advantages of our proposed\nframework.2 RELATED WORKS\nIn this section, we review the existing works\nrelated to our proposed method, including multi-\nmodal representation learning, ViTs for multi-modal\nlearning, and semi-supervised multi-modal image\nsegmentation.\n2.1 Modality-speciﬁc and\nmodality-invariant representation learning\nTo extract modality-invariant representations from dif-\nferent modalities, shared-weight networks or modules\nare employed for learning a universal feature rep-\nresentation space between different modalities. Chen\net al.36employed a shared encoder to map differ-\nent features extracted from unpaired CT and MR\nimages to a common latent feature space for over-\ncoming the domain shift problem of cross-modality\nsegmentation. Wu et al.37proposed a metric based on\nthe distance of characteristic functions to reduce the\nfeature distribution discrepancy of different modalities\nlearned by the shared segmentation network. To learn\nmodality-speciﬁc representations within each modal-\nity, multiple independent modules are used separately\nfor feature extraction of each modality .38Chen et al.33\nused two modality-speciﬁc encoders to extract modality-\nindependent knowledge for cross-modal registration\nin unpaired multi-modal segmentation. Moreover, Pei\net al.39utilized the disentangled representation learning\nto embed unpaired multi-modal images into two different\nspaces, including a shared modality-invariant content\nspace and a modality-speciﬁc style space. Unlike the\nabove heavy-weight approaches, Chang et al.21pro-\nposed a lightweight and effective domain-speciﬁc fea-\nture normalization layer with shared convolutional ﬁlters\nbut different batch normalization parameters for learn-\ning the intensity distributions of different domains in\ncolorful images. Related works14,15,22,23further applied\nthe domain-speciﬁc feature normalization layer in rep-\nresentation learning of different imaging modalities\nfor unpaired multi-modal medical image segmenta-\ntion. Unlike the above methods, our proposed method\nnot only collaboratively learns modality-invariant and\nmodality-speciﬁc representations but also further alle-\nviates the lack of global contexts and scale variation\nproblems in unpaired multi-modal learning.\n2.2 ViTs for multi-modal learning\nDue to the capability of global context modeling and rep-\nresentation learning, ViT25based on the self-attention\nmechanism has been successfully applied to multi-\nmodal medical image synthesis, segmentation, and\nregistration tasks. Feng et al.40proposed a multi-modal\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n5464 MCTHNet\ntransformer network with multiscale feature transfer for\nmulti-modal MR imaging acceleration. Zhang et al.41\npresented a multi-modal medical transformer for brain\ntumor segmentation, which bridged Transformer and\nCNN to build the long-range dependencies both within\nand across different modalities for learning modality-\ninvariant representation. Yang et al.17developed a\ntransformer-based framework using modality-speciﬁc\nclass embeddings and the structured semantic consis-\ntency for unpaired multi-modal segmentation. However,\ndue to the lack of spatial inductive biases existing in\nconvolutional operations, ViT modules used in current\nunpaired multi-modal segmentation methods require\na large number of parameters to learn multi-modal\nrepresentations. To solve these problems, inspired by\nrelated works,28we designed a lightweight transformer\nmodule with the strengths of convolution operations\nand self-attention for learning better modality-invariant\nrepresentations.\n2.3 Semi-supervised multi-modal\nimage segmentation\nSemi-supervised learning could effectively leverage\nlarge amounts of unlabeled scans to improve the perfor-\nmance of multi-modal segmentation networks,42mainly\ncontaining pseudo-label learning and consistency-\nbased regularization methods. Pseudo-label learning43\nleverages reﬁned pseudo labels of unlabeled images\npredicted by the segmentation model trained on the\nlabeled images to expand training data for iterative\ntraining, which is also referred to as self-training.\nConsistency-based regularization44enforces the con-\nsistency of the predictions with various perturbations\nfor learning from labeled and unlabeled data. Cui\net al.45proposed a semi-supervised mean-teacher brain\nlesion segmentation framework by different noise per-\nturbations. Yu et al.46introduced an uncertainty-aware\nmean teacher (UA-MT) framework for left atrium seg-\nmentation from 3D MR images, which exploits the\nuncertainty information of the teacher model by Monte\nCarlo sampling. Luo et al.47designed a cross teach-\ning method between CNN and Transformer to utilize\nthe unlabeled data by combining input and network\nperturbations. In unpaired multi-modal segmentation,\nself-training methods48were adopted to expand the\nvaluable annotations of unpaired T1 and T2 images\nin vestibular schwannoma and cochlea segmenta-\ntion. Based on the previous works35,49that impose\nthe consistency between two perturbated segmen-\ntation networks by using the pseudo segmentation\nmaps, we further incorporate cross-model pseudo-\nsupervision learning with unpaired multi-modal repre-\nsentation learning for constructing an effective semi-\nsupervised learning method in unpaired multi-modal\nsegmentation.TABLE 1 Summary of notations.\nSymbol Interpretation\nMC Modality category (CT or MR modality)\nDct\nL,Dmr\nLUnpaired labeled CT and MR datasets\nDct\nU,Dmr\nUUnpaired unlabeled CT and MR datasets\nxct\nL,yct\nLImages and annotations of labeled CT datasets\nxmr\nL,ymr\nLImages and annotations of labeled MR datasets\nxct\nU,xmr\nUImages of unlabeled CT and MR datasets\nf𝜃1,f𝜃2Two perturbed networks with the same structure but\ndifferent initialization for semi-supervised learning\np1,p2 Predictions of two perturbed networks\nL Loss Functions\n3 PROPOSED METHODS\nIn this section, we ﬁrst give an overall overview of\nour proposed method. Then, we describe our proposed\nmodality-speciﬁc scale-aware convolution (MSSC) and\nMIViT module in detail. Finally, we introduce the MCPS\napproach and the associated loss functions. The nota-\ntions used in this paper are interpreted in Table 1.\n3.1 Overall overview of our proposed\nmethod\nFor unpaired multi-modal image segmentation with\nlimited annotations, we deﬁne Dct\nL={(xct\nL,yct\nL)i}Nct\ni=1and\nDmr\nL={(xmr\nL,ymr\nL)i}Nmr\ni=1as two unpaired labeled CT and\nMR datasets, where yLis the segmentation mask cor-\nresponding to the labeled image xL. Meanwhile, we\ndeﬁne Dct\nU={(xct\nU)i}Mct\ni=1and Dmr\nU={(xmr\nU)i}Mmr\ni=1as two\nunpaired and unlabeled CT and MR datasets, where\nxUis the unlabeled image. Since segmentation tasks\nwith limited annotations focus on the scenario in which\nlabeled scans are scarce and unlabeled scans are abun-\ndant, we assume Nct≪Mctand Nmr≪Mmr. As shown\nin Figure 2, we propose a MCTHNet for effectively\nsegmenting unpaired abdominal and cardiac images\nfrom CT and MR modalities. Moreover, we incorporate\nMCTHNet with our proposed semi-supervised learn-\ning method for learning annotation information from\nabundant unlabeled multi-modal scans.\nFigure 2ashows the network structure of MCTH-\nNet, which is a robust and uniﬁed unpaired multi-modal\nsegmentation network for CT and MR modalities,includ-\ning modality-speciﬁc encoder using MSSC modules,\nmodality-invariant bottleneck layer with the MIViT mod-\nule, and modality-speciﬁc decoder . In the input stage,\nfor a single input sample xMCfrom CT or MR modal-\nity, MCTHNet could adaptively select the corresponding\nfeature normalization layers to learn the intensity dis-\ntribution of the input modality according to the current\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nMCTHNet 5465\nFIGURE 2 The overall overview of our proposed method. (a) shows the architecture details of the proposed unpaired multi-modal\nsegmentation network. (b) is the training process with semi-supervision learning. (c) is the inference process in testing data.\nmodality category MC∈{ct, mr }o fxMC.Thus,we deﬁne\nMCTHNet as f𝜃, and the output pMCis computed as:\npMC=f𝜃(xMC,M C ). As shown in Figure 2b, during the\ntraining process,the proposed MCPS method is adopted\nfor semi-supervised learning, which exploits two per-\nturbed networks f𝜃1and f𝜃2with different initializations\nfor obtaining sufﬁcient pseudo supervision signals from\nunlabeled multi-modal data. As shown in Figure 2c,i n\nthe inference process,since the ﬁnal prediction probabil-\nity maps of two perturbed networks tend to be consistent\nafter training, we choose the ﬁrst of two perturbed net-\nworks f𝜃1as the inference model and predict the ﬁnal\nsegmentation results of testing CT or MR images.\n3.2 MSSC module\nTo alleviate the performance degradation caused by\nscale variation and intensity distribution gap between\ndifferent modalities, as shown in Figure 3, we propose\na MSSC module, which is embedded in the encoding\nstage of our proposed network. In unpaired multi-modal segmentation, shared convolution kernels could\neffectively capture common patterns (e.g., anatomi-\ncal structures) between different modalities, but shared\nfeature normalization parameters easily force the fea-\nture distributions learned from different modalities to\nbe consistent,14,23ignoring the intensity distribution\ngap between modalities. Hence, we adopted modality-\nspeciﬁc batch normalization (MSBN) layers to learn the\nfeature distributions of different modalities. Speciﬁcally,\nthe MSBN layer uses two groups of individual normal-\nization parameters to normalize the features of CT and\nMR images, respectively, which can be formulated as:\n⎧\n⎪\n⎨\n⎪⎩y=𝛾 ct(x−𝜇\n𝜎)\n+𝛽 ct,ifMC is CT,\ny=𝛾 mr(x−𝜇\n𝜎)\n+𝛽 mr,ifMC is MR,(1)\nwhere x, y∈ℝB×C×H×Ware the original input tensor and\nthe normalized output tensor,respectively,where B,C,H,\nand Wdenote the batch size, number of channels, ten-\nsor height, and tensor width, respectively . Meanwhile, 𝛾\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n5466 MCTHNet\nFIGURE 3 Architecture details of the modality-speciﬁc scale-aware convolution module. In this module, modality-speciﬁc normalization\nlayers are adopted to learn feature distributions of different modalities, and the scale-aware module can adaptively gain the most efﬁcient scale\ninformation of unpaired multi-modal images.\nand𝛽are the trainable normalization parameters used\nto adjust the feature distributions, and 𝜇and𝜎are the\nmean and standard deviation of the sample set in the\ninput tensor . There are different deﬁnitions50of sample\nsets for computing 𝜇and𝜎, such as Batch Normaliza-\ntion, Instance Normalization, Group normalization. By\nexperimental comparison, we choose the sample set\ndeﬁned in Batch Normalization to compute 𝜇and𝜎,\nwhich is expressed as\n⎧\n⎪\n⎨\n⎪⎩𝜇=1\nm∑\nk∈Sxk,\n𝜎=√\n1\nm∑\nk∈S(xk−𝜇)2+𝜀,(2)\nwhere𝜀=1e−5 is a constant to avoid dividing by zero,\nSis the set of samples along the ( B, H, W ) axis, and\nmis the number of samples. Then, we integrate the\nMSBN layer with the scale-aware module based on the\nmulti-scale branch attention mechanism51to construct\nthe MSSC module.\nIn the MSSC module, for the input tensor xi∈\nℝB×Ci×H×Wand the output tensor xo∈ℝB×Co×H×W,\nwhere Ci,Coare the number of input and output chan-\nnels, we ﬁrst use two individual convolutional branches\nwith the kernel size of 3 ×3 and 5×5 to obtain\ntwo output tensors xa,xb∈ℝB×∕2×H×Wwith different\nscale information,respectively .Then,we use global aver-\nage pooling (GAP) and fully connected layers to obtain\ntwo low-rank scale attention weights 𝛼,𝛽∈ℝB×Co∕2×1×1\nfrom the summation of xaand xb. By the tensor mul-\ntiplication between the scale attention weights and\nthe branch output tensors, the scale-aware module of\nthe MSSC module could adaptively select the mosteffective scale information and adjust the receptive\nﬁeld sizes to overcome the scale variation problem of\nunpaired multi-modal images. Additionally, we concate-\nnate two multi-scale recalibrated tensors and exploit the\nchannel shufﬂe operation52to achieve the multi-scale\ninformation interaction, which is formulated as:\nxfusion=fshufﬂe ([𝛼⊙ xa,𝛽⊙ xb]) (3)\nwhere xfusion∈ℝB×Co×H×Wis the output tensor with\ncomprehensive multi-scale information, ⊙denotes the\nelement-wise multiplication, and fshufﬂe denotes the\nchannel shufﬂe operation. In the channel shufﬂe oper-\nation denoted, we ﬁrst divide the concatenated tensor\ninto two groups of tensors in the channel dimension,and\nthen we stack these two groups of tensors alternately\nfor interaction. Finally, the output tensor xoof the MSSC\nmodule is obtained by the convolutional layer with a 3 ×3\nconvolution,a MSBN layer,and ReLU activation function.\n3.3 MIViT module\nIn current unpaired multi-modal segmentation meth-\nods, shared convolution kernels for all modalities are\nfrequently adopted to learn modally invariant represen-\ntations. However, these methods ignore the limitations\nof convolution operations in global context model-\ning. As shown in Figure 4, to effectively incorporate\nlocal spatial information with global contextual informa-\ntion for learning more generalizable modality-invariant\nrepresentations,we propose a MIViT module as the bot-\ntleneck layer, where all parameters are shared for CT\nand MR modalities. Unlike the original heavy-weight ViT\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nMCTHNet 5467\nFIGURE 4 Architecture details of the modality-invariant vision transformer module, which integrates the local representations with\nneighboring relations and global representations with spatial induction biases for learning better modality-invariant representations.\nmodule that lacks spatial inductive biases,by introducing\nconvolution-like operations into transformers,28includ-\ning unfolding,local processing,and folding,our proposed\nMIViT module efﬁciently learns global representations\nwith spatial induction biases.\nIn the MIViT module, for the input tensor xi∈\nℝB×Ci×H×Wgenerated by the modality-speciﬁc encoder\nfrom CT or MR images, we ﬁrst employ a 3 ×3 convo-\nlutional layer and a 1 ×1 convolutional layer to obtain\nthe tensor xL∈ℝB×d×H×Wwith compact local relations\nas the input of the transformer module, where d=Ci∕2\nis the embedding dimension of the transformer module.\nThen, we exploit the unfolding operation of convolution\noperation to replace the linear projection of ﬂattened\npatches used in the original ViT module to learn the\nspatial induction biases. Thus, we unfold xLinto xU∈\nℝB×P×N×dby the patch splitting and tensor reshape\noperations,where P=pw×phis the patch size and N=\nH×W\nPis the number of patches. To reduce memory con-\nsumption, we set pw=ph=2. To learn global contexts,\nwe employ the lightweight transformer module without\npositional embedding to extract features from xU, and\nthe tensor xG∈ℝB×d×H×Wwith global representations\ncould be gained and formulated as:\nxG=fFold(fTrans (xU)) (4)\nwhere fFold denotes the folding operation using the\ntensor reshape operation, and fTrans denotes the trans-\nformer module consisting of L=4 transformer layers.\nFor the l-th transformer layer, where 0 <l≤L, the out-\nput tensor zl∈ℝB×P×N×dencoded by transformer layers\nis computed by the multi-head attention (MHA) mod-\nule and the multi-layer perceptron (MLP) network from\nthe input tensor zl−1∈ℝB×P×N×d, which is expressed inEquations ( 5) and ( 6):\nz′l=MHA (LN(zl−1))+zl−1, (5)\nzl=MLP(\nLN(\nz′l))\n+z′l, (6)\nwhere LN(·) denotes the layer normalization layer .\nIn the MHA module, nhead=4 parallel self-attention\n(SA) heads are used to construct the multi-head self-\nattentive mechanism.The output tensor SAi(z)o ft h e i-th\nself-attention head in the MHA module is deﬁned as:\nSAi(z)=softmax(\nQi⋅(Ki)⊤\n√\ndhead)\n⋅Vi, (7)\nwhere z∈ℝB×P×N×dis the input tensor, and dhead=8\ndenotes the dimension of each self-attention head. The\nquery tensor Ql, key tensor Kl, and value tensor Vlare\ncomputed from the input tensor zby linear transfor-\nmation, and Ql,Kl,Vl∈ℝB×P×N×dhead. Hence, the MHA\nmodule could be formulated as:\nMHA (z)=[\nSA1(z);⋯;SAnhead(z)]\nWlinear ,(8)\nwhere [⋯] denotes the concatenation operation, and\nWlinear∈ℝB×(nhead⋅dhead)×ddenotes the trainable weights\nof the linear transformation layer used to compute the\noutput tensor MHA (z)∈ℝB×P×N×dof the MHA mod-\nule. Finally, we exploit the shared convolutional layers\nand skip-connections to map the tensor xG∈ℝB×d×H×W\nto the output tensor xo∈ℝB×Co×H×W, and xohave\nmore expressive modality-invariant representations with\nglobal contexts than the output tensors generated by the\nshared convolutional bottleneck layers of other methods.\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n5468 MCTHNet\nALGORITHM 1 MCPS\nInput: Labeled CT and MR datasets: Dct\nL,Dmr\nL, unlabeled CT and MR\ndatasets: Dct\nU,Dmr\nU, and max number of iterations Tmax\nOutput: Trained segmentation models f𝜃1,f𝜃2, where f𝜃1is used for\nthe inference procedure.\n1: while iterations <do\n2: for MCin {ct, mr }d o\n3: ( xmc\nL,ymc\nL),xmc\nU←Sample mini-batches from DMC\nLand DMC\nU;\n4: p1MC\nL=f𝜃1(xM\nL,M C );\n5: p2MC\nL=f𝜃2(xM\nL,M C );\n6: Lsup=(Lseg(p1MC\nL,yMC\nL)+Lseg(p2MC\nL,yMC\nL))×0.5;\n7: p1MC\nU=f𝜃1(xMC\nU,M C );\n8: p2MC\nU=f𝜃2(xMC\nU,M C );\n9: y1MC\nU=Argmax (p1MC\nU); // stop-gradient\n10: y2MC\nU=Argmax (p2MC\nU); // stop-gradient\n11: Lmcps=Lce(p1MC\nU,y2MC\nU)+Lce(p2MC\nU,y1MC\nU);\n12: Ltotal=Lsup+𝜆 mcps Lmcps;\n13: Compute gradients of the total loss function Ltotaland\nupdate network parameters 𝜃1,𝜃2by back propagation;\n14: return 𝜃1,𝜃2\n3.4 MCPS method\nBecause of the spatial misalignment and domain\nshift problems between different modalities, annotating\nunpaired multi-modal images is laborious and expen-\nsive. To efﬁciently leverage a large amount of unlabeled\nunpaired multi-modal scans to improve the segmen-\ntation performance, we design a simple but effective\nMCPS method for semi-supervised learning during\ntraining, which is explained in Algorithm 1.I nt h eM C P S\nmethod, we utilize two segmentation networks with the\nsame MCTHNet architecture but different normaliza-\ntions as network perturbations, which are deﬁned as\nf𝜃1and f𝜃2, respectively . In each training iteration, to\nreduce the computational cost caused by two perturbed\nnetworks,we compute the gradients and update the net-\nwork parameters by sampling the mini-batch data that\ncontain labeled and unlabeled images from CT or MR\nmodalities, where the batch size of each modality is 4.\nFor labeled samples ( xMC\nL,yMC\nL), the prediction maps of\nf𝜃1and f𝜃2are computed as:\n{p1MC\nL=f𝜃1(\nxMC\nL,M C)\n,\np2MC\nL=f𝜃2(\nxMC\nL,M C)\n,(9)\nwhere both p1MC\nLand p2MC\nLcorrespond to the ground\ntruth yMC\nL. Then, we could compute the supervised\nsegmentation loss Lsupbetween prediction maps and\nground truths for training f𝜃1and f𝜃2, which is formulatedas follows:\nLsup=0.5⋅(\nLseg(\np1MC\nL,yMC\nL)\n+Lseg(\np2MC\nL,yMC\nL))\n,\n(10)\nwhere Lsegis described as follows:\nLseg=Ldice+Lce, (11)\nLdice=1\nCC∑\nc=1⎛\n⎜\n⎜⎝1−2∑N\ni=1gc\nipc\ni+𝜀\n∑N\ni=1(\ngc\ni)2+∑N\ni=1(\npc\ni)2+𝜀⎞\n⎟\n⎟⎠,(12)\nLce=−1\nC⋅1\nN⋅C∑\nc=1N∑\ni=1gc\nilogpc\ni, (13)\nwhere giand pidenote the ground truths and prediction\nresults of the i-th pixel point,respectively,and Ndenotes\nthe number of pixels, and Cdenotes the category num-\nber of segmentation labels, and 𝜀=10−6denotes the\nsmooth factor .\nFor unlabeled samples xMC\nU,the prediction maps p1MC\nU\nand p2MC\nUoff𝜃1and f𝜃2are computed as:\n⎧\n⎪\n⎨\n⎪⎩p1MC\nU=f𝜃1(\nxMC\nU,M C)\n,\np2MC\nU=f𝜃2(\nxMC\nU,M C)\n,(14)\nwhere p1MC\nUand p2MC\nUdo not have the corresponding\nsegmentation annotations in training data. Hence, in our\nproposed MCPS method, we exploit pseudo segmenta-\ntion labels generated from p1MC\nUand p2MC\nUto cross-train\ntwo perturbed networks using unpaired multi-modal\nlearning, thereby improving the consistency between\nnetwork perturbations for learning the abundant anno-\ntation information and cross-modality representations\nfrom unlabeled unpaired multi-modal scans.Speciﬁcally,\nwe ﬁrst stop the gradient ﬂow of p1MC\nUand p2MC\nUand\ngenerate pseudo labels y1MC\nUand y2MC\nUby the argmax\nfunction during training, which is expressed as:\ny1MC\nU=Argmax (p1MC\nU),y2MC\nU=Argmax (p2MC\nU).(15)\nThen, we use y1MC\nUand y2MC\nUas the ground truths of\npredictions p2MC\nUand p1MC\nU, respectively, which imple-\nments pseudo-label supervision across two perturbed\nnetworks in unlabeled data and can be formulated as:\nLmcps=Lce(\np1MC\nU,y2MC\nU)\n+Lce(\np2MC\nU,y1MC\nU)\n.(16)\nwhere Lmcps is the MCPS loss. By combining the super-\nvised loss Lsupand the semi-supervised loss Lmcps,\nwe could construct the total loss Ltotal for unpaired\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nMCTHNet 5469\nmulti-modal segmentation with limited annotations. The\nLtotalis computed as:\nLtotal=Lsup+𝜆 mcps Lmcps; (17)\nwhere𝜆mcps is a trade-off weight for balancing super-\nvised and unsupervised consistency losses, which is a\ntime-dependent Gaussian warming up function46,47and\ndeﬁned as: 𝜆mcps(t)=0.1⋅e(−5(1−t∕tmax)2), where tis the\ncurrent number of training iterations and tmaxis the total\nnumber of iterations.\n4 EXPERIMENTAL RESULTS\nIn this section, we evaluate the segmentation perfor-\nmance of the proposed method in two unpaired CT\nand MR image segmentation tasks with limited anno-\ntations: (1) cardiac substructure segmentation; and (2)\nabdominal multi-organ segmentation. To validate the\neffectiveness of our proposed method, we describe the\nexperimental details and perform extensive comparison\nexperiments and ablation experiments. In addition, we\ndiscuss the differences between the proposed method\nand other state-of-the-art methods.\n4.1 Datasets\n4.1.1 Unpaired multi-modal cardiac\nsubstructure dataset\nThe Multi-Modality Whole Heart Segmentation Chal-\nlenge 2017 (MMWHS-2017) dataset19is employed for\ncardiac substructure segmentation, and the MMWHS-\n2017 dataset consists of unpaired 20 CT and 20 MRI\nscans collected from different patients and institutions.\nIn our experiments, four cardiac substructure regions\nare used for quantitative evaluation,including Ascending\nAorta (AA),Left Atrium Blood Cavity (LAC),Left Ventricle\nBlood Cavity (LVC), and Myocardium of the Left Ventri-\ncle (MYO) regions. For the data preprocessing, we ﬁrst\nresampled the CT and MR scans to the isotropic res-\nolution of 1 mm3; then, we obtained sub-volumes with\nthe axial size of 192 ×192 from the original volumetric\nimages by center cropping, and we removed the top 2%\nof their intensity histograms to reduce artifacts; ﬁnally,\nwe scaled the intensities of cropped sub-volumes to [0,\n1] by min-max normalization. For each modality, we ran-\ndomly split all patients into 80% for training and 20% for\ntesting. Additionally, by reducing the labeling ratio of the\ntraining data,we study the segmentation performance of\ndifferent unpaired multi-modal segmentation methods\nunder the scenario when labeled data is scarce. For the\nscenario with the minimum labeling ratio, we randomly\nselect 25% of training data as labeled data and 75% of\ntraining data as unlabeled data in the training process.4.1.2 Unpaired multi-modal abdominal\nmulti-organ dataset\nFor a fair comparison with other methods, we used\nCT scans with 30 scans from the BTCV dataset53and\n20 MR scans from the CHAOS dataset18to build the\nunpaired multi-modal dataset for abdominal multi-organ\nsegmentation. In this dataset, we mainly focus on four\nabdominal organs: Liver, Left Kidney (L-kdy), Right Kid-\nney (R-kdy), and Spleen. Based on related works,54\nwe adopted different methods to preprocess unpaired\nabdominal CT and MR scans separately to alleviate the\nmodality discrepancy problem. For the abdominal CT\nscans, we scaled the axial plane size of all scans to\n256×256 by trilinear interpolation, and used the inten-\nsity window of [ −275,125] for normalizing the intensities\nof all scans into [0, 1]. For the abdominal MR scans,\nwe ﬁrst resampled all scans into 1 .25mm×1.25mm×\n7.7mmand obtained the scans with an axial plane size\nof 256×256 by center cropping; then, we clipped the\ntop 0.5% of the intensity histograms of all scans and\nscaled the intensities of all scans to [0, 1] by the min-\nmax normalization. Similar to the data splitting strategy\nof the cardiac substructure dataset, each modality is\nrandomly divided into 80% for training and 20% for test-\ning. For the scenario with the minimum labeling ratio,\n25% training data is used as labeled data in the training\nprocess.\n4.2 Evaluation metrics\nIn our experiments, the Dice Similarity Coefﬁcient\n(DSC) and Average Surface Distance (ASD) metrics\nare employed for the quantitative evaluation of seg-\nmentation results. The DSC metric measures the region\noverlap between segmentation results and the ground\ntruths, which is formulated as:\nDSC( P, G)=2|P⋂G|\n|P|+|G|×100% (18)\nwhere Pand Gdenote the segmentation results and\nground truths, respectively . The higher DSC values\nmean better segmentation performance. The ASD met-\nric is utilized to measure the segmentation boundary\ndistance between segmentation results and the ground\ntruths, which is described as:\nASD( P, G)=1\n2(∑\nVi∈SPmin Vj∈SGd(Vi,Vj)\n∑\nVi∈SP1\n+∑\nVj∈SGmin Vi∈SPd(Vi,Vj)\n∑\nVj∈SG1)\n,(19)\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n5470 MCTHNet\nwhere SPand SGis the surfaces of segmentation\nresults and ground truths, respectively, and d(Vi,Vj)\ndenotes the Euclidean distance from a vertex Vi\nt oav e r t e x Vi. The smaller ASD values mean that\nthe segmentation results are closer to the ground\ntruths.\n4.3 Implementation details\nOur methods are implemented by the Pytorch frame-\nwork, and our codes will be available at: https://github.\ncom/Healingl/MCTHNet . We performed all experiments\non an Ubuntu 18.04 workstation with two 24G NVIDIA\nGeForce RTX 3090 GPUs and an Intel Xeon Gold 5117\n2.00 GHz CPU. In the training process, we set the batch\nsize of each modality as 4, and initialize the parameters\nof two perturbation networks used in semi-supervised\nlearning by Kaiming and Xavier methods,55respectively .\nBesides, we use the AdamW optimizers with an ini-\ntial learning rate of 0.0002 and a weight decay rate of\n0.0005 to iteratively train segmentation networks for 300\nepochs, and the learning rate is periodically reduced by\nmultiplying 0.5 after every 50 epochs. To improve data\ndiversity and alleviate the overﬁtting problem, we exploit\nonline data augmentation techniques to expand the\ntraining data,including random ﬂipping,random rotation,\nand random elastic deformation.\n4.4 Comparison results with other\nstate-of-the-art methods\nTo verify the effectiveness of the proposed method\nin unpaired multi-modal segmentation tasks with lim-\nited annotations, under the scenarios when the labeling\nratios of training data are 100% and 25%, we com-\npare the proposed method with seven state-of-the-art\nunpaired multi-modal segmentation methods,including:\n∙U-Net (Single)6: A multi-model segmentation frame-\nwork containing two individual U-Net models, where\neach U-Net model corresponds to one individual\nmodality .\n∙U-Net (Joint)6: A single U-Net model with all network\nparameters shared for CT and MR images.\n∙TransUNet (Single)26: A multi-model segmentation\nframework containing two individual TransUNet mod-\nels, where each TransUNet model uses the original\nViT module25as the encoder and corresponds to one\nmodality .\n∙X-shape13: A two-stream segmentation architecture\nusing the modality-speciﬁc encoder, shared bottle-\nneck layers, and modality-speciﬁc decoder .\n∙UMMKD14: A unpaired multi-modal segmentation\narchitecture constructed by the “Chilopod”-shaped\nmodel and knowledge distillation loss.∙UNetDSBN23: A modiﬁed and lightweight U-Net\nmodel with the dual-normalization module for gen-\neralizable cross-modality segmentation between\nunpaired CT and MR images.\n∙UNetDSBN +MT23,44: We constructed an unpaired\nmulti-modal segmentation framework composed by\nUNetDSBN and the Mean-Teacher approach, which\ncould learn annotation information from unlabeled\nimages.\nBesides, the \"Ours\" method is our proposed complete\nsegmentation framework using MCTHNet architecture\nand the MCPS method together, which removes semi-\nsupervised learning losses when the annotation rate is\n100%.\n4.4.1 Unpaired multi-modal cardiac\nsubstructure segmentation\nFor unpaired multi-modal cardiac substructure segmen-\ntation, Table 2shows the quantitative evaluation results\nof all methods in testing CT and MR scans under\ndifferent labeling ratios. For the scenario when the\nlabeling ratio of training data is 100%, U-Net (Single)\nwithout unpaired multi-modal learning obtains the over-\nall mean DSC value of 86.31%, which achieves the\nbaseline segmentation performance in cardiac segmen-\ntation. TransUNet (Single) employs ViTs to obtain rich\nglobal contextual information.However,due to the signif-\nicant overﬁtting problem caused by the large number of\nparameters of 134.1 M, the segmentation performance\nof TransUNet (Single) is lower than U-Net (Single). On\nthe other hand, UMMKD exploits unpaired multi-modal\nlearning and knowledge distillation loss to raise the aver-\nage DSC value to 91.9% for CT and 86.8% for MR,which\nis better than single-modal methods, that is, U-Net (Sin-\ngle) and TransUNet (Single). By collaboratively learning\nmodality-speciﬁc and modality-invariant representations\nof unpaired multi-modal images, our proposed method\nachieves the best overall mean DSC value of 90.0%\n(speciﬁcally,92.12% on CT and 87.88% on MRI),outper-\nforming other single-modal and unpaired multi-modal\nmethods. Further, the number of parameters of our pro-\nposed method is only 23.59 M, which is much less\nthan the current state-of-the-art UMMKD with 39.34 M\nparameters.\nFor the scenario when the labeling ratio of training\ndata is 25%, we study the segmentation performance\nof different methods by using limited annotations and\nextensive unlabeled images for training. From Table 2,\ndue to scarce labeled data, we observe that U-Net (Sin-\ngle) produces a 17.32% decrease in the overall mean\nDSC value compared to segmentation models trained\non fully labeled data. The U-Net (Joint) with all parame-\nters shared for all modalities only obtained an average\nDSC value of 62.46%, which is lower than U-Net\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nMCTHNet 5471\nTABLE 2 Performance comparison of different methods for unpaired multi-modal cardiac substructure segmentation.\nDSC (%)↑\nCardiac CT Cardiac MR\nLabeling ratio Methods Param MYO LAC LVC AA Avg. MYO LAC LVC AA Avg.Overall\nmean\n100% U-Net (Single)614.48 M 86.09 91.39 90.08 73.91 85.37 82.48 88.95 93.40 84.20 87.26 86.31\nTransUNet (Single)26134.1 M 82.24 88.05 88.22 76.70 83.80 81.69 87.71 92.68 83.93 86.50 85.15\nUMMKD1439.34 M 87.61 91.54 91.20 94.09 91.11 82.16 88.04 93.14 87.55 87.72 89.42\nOurs 23.59 M 88.14 90.16 92.40 97.66 92.12 82.26 89.20 93.36 86.69 87.88 90.00\n25% U-Net (Single)614.48 M 73.42 75.05 81.04 71.74 75.31 58.73 56.43 65.89 69.69 62.68 68.99\nU-Net (Joint)67.24 M 58.14 71.35 76.37 62.34 67.05 50.44 50.02 63.88 67.15 57.87 62.46\nX-shape1365.96 M 75.51 61.88 76.92 42.13 64.11 52.55 49.40 58.48 64.99 56.36 60.23\nUMMKD1439.34 M 79.66 81.84 88.35 84.84 83.67 58.88 51.82 66.56 75.18 63.11 73.39\nUNetDSBN2312.74 M 79.21 84.70 85.55 76.99 81.61 54.00 54.49 62.17 73.93 61.15 71.38\nUNetDSBN +MT23,4412.74 M 76.32 85.16 85.46 92.08 84.76 61.59 58.28 66.53 75.27 65.42 75.09\nOurs 23.59 M 81.45 88.37 85.69 80.69 84.05 59.38 77.56 75.69 79.71 73.08 78.56\nNote : The best results are indicated by bold fonts. “Labeling Ratio” is the labeling ratio of the training data, “Avg.” denotes the average DSC value of all r egions in a\nsingle modality, and “Overall Mean” indicates the overall average DSC value of testing CT and MR scans.\n(Single).Meanwhile,insufﬁcient supervision signals also\ncause the signiﬁcant performance degradation of cur-\nrent state-of-the-art unpaired multi-modal segmentation\nmethods, including X-shape, UMMKD, and UNetDSBN.\nBy using the Mean-Teacher approach based on con-\nsistency constraints for semi-supervised learning, the\nUNetDSBN +MT method improves the average DSC val-\nues in all cardiac regions by about 4% compared to\nthe original UNetDSBN, which demonstrates the impor-\ntant potential value of a large amount of unlabeled data\nfor unpaired multi-modal learning with limited annota-\ntions. Compared with the UNetDSBN +MT method, by\nleveraging the strengths of both pseudo-label learning\nand consistency-based regularization in unpaired multi-\nmodal representation learning, our segmentation result\nhas a 3.47% promotion of overall mean DSC value.\nSpeciﬁcally,under the scenario when the labeling ratio is\n25%, our proposed achieves the best overall mean DSC\nvalue of 78.56% in cardiac segmentation, which signif-\nicantly outperforms other methods and demonstrates\nthat our proposed method could learn abundant anno-\ntation information from unlabeled multi-modal cardiac\nscans. In addition, as shown in Figure 5, we visual-\nize the segmentation results of different methods in\nunpaired cardiac CT and MR images for qualitative\nanalysis. Figure 5shows that our proposed method\ncan accurately and automatically segment four cardiac\nsubstructure regions using scarce labeled data, and\nits segmentation results have less noise and greater\nprecision than other methods.\n4.4.2 Unpaired multi-modal abdominal\nmulti-organ segmentation\nTable 3reports the quantiﬁcation results of different\nmethods in unpaired multi-modal abdominal multi-organsegmentation. In fully labeled (100%) training data,\nour proposed method achieves the best overall mean\nDSC value of 85.46% (speciﬁcally, 92.27% on CT and\n78.64% on MRI). Compared with single-modal fully-\nsupervised methods without multi-modal learning, such\nas U-Net (Single) and TransUNet (Single), our proposed\nmethod obtains higher DSC values of four abdominal\norgan regions in testing CT and MR scans. Mean-\nwhile, although the DSC value of the Liver region\nof UMMKD is higher than our proposed method, the\nDSC values of the R-kdy, L-kdy, and Spleen regions of\nour proposed method are signiﬁcantly improved from\nUMMKD.\nFor the scenario when the labeling ratio is 25%,\nbecause of the lack of labeled scans and modality dis-\ncrepancy, U-Net (Single) only achieves the overall mean\nDSC value of 60.07%,which is a signiﬁcant decrease of\n20.23% compared to 80.30% obtained by the fully super-\nvised U-Net (Single).Although UMMKD and UNetDBSN\nutilize modality-speciﬁc feature normalization layers to\nalleviate the performance degradation caused by the\nmodality discrepancy, due to the inability to learn from\nunlabeled data, the performance improvement of these\nmethods over U-Net (Single) segmentation is subtle,\nand the increase of overall mean DSC value is less than\n5%. By effectively alleviating the modality discrepancy\nproblem and learning unlabeled scans, our proposed\nmethod achieves the best overall mean DSC value of\n76.18% (speciﬁcally,81.85% on CT and 70.52% on MRI)\nin testing CT and MR scans, and signiﬁcantly improves\nthe overall segmentation performance over U-Net (Sin-\ngle) by 16.11% (from 60.07% to 76.18%). Compared\nwith the semi-supervised learning methods, the overall\nmean DSC values of our scheme also outperform\nthe UNetDSBN +MT method by a large margin, that is,\n19.67% on CT and 7.39% on MRI.Besides,the segmen-\ntation performance (76.18%) of our proposed method\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n5472 MCTHNet\nFIGURE 5 Qualitative comparison of different segmentation methods in unpaired multi-modal cardiac substructure segmentation, where\nthe labeling ratio of training data is 25%. The MYO, LAC, LVC, and AA regions of cardiac substructures correspond to red, green, blue, and\nyellow colors, respectively .\nTABLE 3 Performance comparison of different methods for unpaired multi-modal abdominal multi-organ segmentation.\nDSC (%)↑\nAbdominal CT Abdominal MR\nLabeling ratio Methods Param Liver R-kdy L-kdy Spleen Avg. Liver R-kdy L-kdy Spleen Avg.Overall\nmean\n100% U-Net (Single)614.48 M 87.52 88.27 86.34 89.24 87.84 81.96 66.61 57.45 85.10 72.76 80.30\nTransUNet (Single)26134.1 M 88.32 80.40 79.92 85.01 83.41 81.80 59.61 52.82 70.93 66.29 74.85\nUMMKD1439.34 M 94.43 88.13 86.93 93.64 90.78 85.42 64.30 73.30 87.23 77.56 84.17\nOurs 23.59 M 93.13 89.72 91.09 95.15 92.27 78.52 70.40 74.97 90.66 78.64 85.46\n25% U-Net (Single)614.48 M 84.64 51.55 43.88 67.64 61.93 69.14 56.96 50.25 56.54 58.22 60.07\nU-Net (Joint)67.24 M 75.69 41.86 36.64 77.51 57.99 70.69 43.47 46.50 56.39 54.26 56.12\nX-shape1365.96 M 77.51 26.06 22.39 59.48 46.36 64.07 32.24 40.05 60.92 49.34 47.85\nUMMKD1439.34 M 83.65 50.34 50.27 77.21 65.37 75.58 49.63 64.97 61.08 62.82 64.09\nUNetDSBN2312.74 M 85.42 46.33 34.47 73.00 59.80 68.42 54.29 60.85 69.66 63.30 61.55\nUNetDSBN +MT23,4412.74 M 82.96 49.11 39.37 77.27 62.18 73.82 47.24 64.80 66.64 63.13 62.65\nOurs 23.59 M 87.51 69.09 82.50 88.30 81.85 72.54 66.83 67.07 75.63 70.52 76.18\nNote : The best results are indicated by bold fonts. “Labeling Ratio” is the labeling ratio of the training data, “Avg.” denotes the average DSC value of all r egions in a\nsingle modality, and “Overall Mean” indicates the overall average DSC value of testing CT and MR scans.\nusing only 25% labeled data is close to the performance\n(80.30%) of the fully-supervised U-Net (Single) method\nwith 100% labeled data, which also demonstrates the\neffectiveness of our proposed method. On the other\nhand, to qualitatively analyze the segmentation results\nof different methods, we compare the segmentation\nresults of different methods in unpaired CT and MR\nabdominal scans in Figure 6.The visualization results of\nFigure 6demonstrate that our proposed method is able\nto obtain more accurate and continuous abdominal seg-\nmentation results, especially for the L-kdy and Spleen\nregions.4.5 Ablation study\nTo evaluate the effectiveness of key components in\nour proposed method, as shown in Table 4, we per-\nform the ablation study on the unpaired multi-modal\nabdominal multi-organ dataset with a labeling ratio of\n25%. In Table 4, we compared the impacts of three\ncomponents on segmentation performance, including\nthe MSSC module, MIViT module, and MCPS method.\nSpeciﬁcally, we construct six methods with different\ncombinations of key components for experiments, and\nstatistical signiﬁcance analysis is performed by paired\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nMCTHNet 5473\nFIGURE 6 Qualitative comparison of different segmentation methods in unpaired multi-modal abdominal multi-organ segmentation, where\nthe labeling ratio is 25%. The Liver, R-kdy, L-kdy, and Spleen regions of abdominal multi-organs correspond to red, green, blue, and yellow colors,\nrespectively .\nTABLE 4 Ablation study of key components on the unpaired abdominal multi-organ segmentation dataset with a labeling ratio of 25%.\nKey components DSC (%) ↑ ASD (mm) ↓\nSettings MSSC MIViT MCPS CT MR Avg. CT MR Avg. p-Value Param\n1) Baseline ✗✗ ✗ 64.70 58.04 61.37 24.98 19.47 17.72 – 10.38 M\n2) Comb1 ✓✗✗ 72.81 60.61 66.71 8.709 13.38 11.04 0.027 11.90 M\n3) Comb2 ✗✓ ✗ 73.31 61.00 67.15 12.76 20.45 16.60 0.022 22.08 M\n4) Comb3 ✗✗ ✓ 73.02 68.21 70.61 14.68 16.19 15.43 0.004 10.38 M\n5) Comb4 ✓✓ ✗ 76.55 63.12 69.83 8.531 17.52 13.02 0.011 23.59 M\n6) Ours ✓✓ ✓ 81.85 70.52 76.18 8.129 8.174 8.15 0.003 23.59 M\nNote : Best results are marked in bold fonts. Statistical analysis is performed with paired t-tests between each method and the baseline in terms of the DSC metric on\ntesting CT and MR images, and p-value<0.05 denotes the signiﬁcant difference between the baseline method and each modiﬁed method.\nt-tests between each combination and the baseline\nmethod in terms of DSC values in testing CT and MR\nimages. It is observed that we get p-values<0.05\nin all combinations, indicating signiﬁcant improvements\nfor our proposed key components. Table 4shows that\nthe Baseline method without any key components,\nwhich only exploits modality-speciﬁc feature normaliza-\ntion layers for unpaired multi-modal learning, achieves\nthe basic segmentation performance (i.e., the average\nDSC value of 61.37% and the average ASD value of\n17.72 mm). Since the MSSC module could enhance\nmodality-speciﬁc representations and alleviate the scale\nvariation problem, the Comb1 method with the MSSC\nmodule improves the average DSC value by 5.34%\nand decreases the average ASD value by 6.68 mm\ncompared to the Baseline method. By combining con-\nvolutions with transformers for learning generalizable\nmodality-invariant representations with global contexts,the Comb2 method with the MIViT module achieves bet-\nter segmentation performance than the Comb1 method.\nMeanwhile, the MIViT module only increases 11.7 M\nparameters compared to the Baseline method, which\nis more lightweight and efﬁcient than the original ViT\nmodule25with 43.30 M parameters. By introducing\nthe MCPS method for semi-supervised learning, the\nComb3 method improves the average DSC values\nto 70.61% and decreases the average ASD values\nto 15.43 mm, outperforming Comb2 by a signiﬁcant\nmargin. Moreover, we constructed the Comb4 method\n(i.e., MCTHNet) by integrating the MSSC and MIViT\nmodules into the Baseline method, which effectively\nleverages modality-invariant and modality-speciﬁc rep-\nresentations for improving segmentation performance.\nFinally, we combined all the key components to con-\nstruct our proposed complete method with 23.59 M\nparameters, which obtains the highest average DSC\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n5474 MCTHNet\nTABLE 5 Quantitative comparison of different feature normalization layers and inference models on the unpaired abdominal multi-organ\nsegmentation dataset.\nDSC (%)↑\nAbdominal CT Abdominal MR\nMethod Inference models Norm. Liver R-kdy L-kdy Spleen Avg. Liver R-kdy L-kdy Spleen Avg.Overall\nmean\nOurs f𝜃1IN 70.43 28.11 39.19 12.77 37.62 59.15 17.56 55.81 40.74 43.32 40.47\nGN 87.24 58.87 58.91 73.91 69.73 77.64 61.86 65.62 82.20 71.83 70.78\nBN 87.51 69.09 82.50 88.30 81.85 72.54 66.83 67.07 75.63 70.52 76.18\nf𝜃2BN 87.73 68.01 82.26 87.96 81.49 71.69 66.91 66.76 76.20 70.39 75.94\nNote : Best results are marked in bold fonts, and “Norm.” denotes the feature normalization method.\nvalue of 76.18% and the lowest average ASD value of\n8.15 mm.\nTo investigate the impacts of feature normalization\nlayers on unpaired multi-modal learning, we compare\nthe segmentation results of different modality-speciﬁc\nfeature normalization methods50adopted in our pro-\nposed method in Table 5, including Instance Nor-\nmalization (IN), Group Normalization (GN), and Batch\nNormalization (BN). From Table 5, we observe a sig-\nniﬁcant performance degradation in the method using\nIN layers, demonstrating the importance of learning\nmodality-speciﬁc information and feature distributions\nfrom multiple samples. Although the method using\nGN layers gains a subtle performance improvement in\nabdominal MR scans compared to the approach using\nBN layers, the overall segmentation performance of\nthe method using BN layers outperforms the method\nusing GN layers by a large margin (from 70.78% to\n76.18%). Besides, we compared the segmentation per-\nformances of two perturbed networks in the inference\nprocedure in Table 5. Results of Table 5show that\nthe segmentation performances of two perturbed net-\nworks are close,and the p-value between two perturbed\nnetworks is 0.2155 ( >0.05) in our experiments, which\nmeans that the performances of two perturbed networks\nare not signiﬁcantly different. Thus, we choose the ﬁrst\nperturbed network f𝜃1as the inference model, which is\nslightly better than the second perturbed network f𝜃2in\nTable 5.\nOn the other hand, as shown in Figure 7,w e\ncompare the proposed MCPS method with three state-\nof-the-art semi-supervised learning methods, including\nSelf-Training (ST), Mean-Teacher (MT),44and UA-MT46\nmethods. Figure 7shows that our proposed MCPS\nmethod obtains more signiﬁcant performance improve-\nments in all modalities than other semi-supervised\nlearning methods, which is beneﬁcial for unpaired multi-\nmodal image segmentation with limited annotations.\nThe MIViT module with spatial inductive biases guides\nthe segmentation network to learn better modality-\ninvariant representations,but it also produces parameter\ngrowth with the increase of the transformer layers,\nresulting in overﬁtting. Thus, to balance the number of\nparameters and overall segmentation performance in\nFIGURE 7 Comparison of average DSC values in testing\nabdominal CT and MR images by using different semi-supervised\nlearning methods, where \"None\" denotes the method without\nsemi-supervised learning.\nour proposed method, in Figure 8, we compare the\nimpacts of different numbers of Transformer layers.\nThe results of Figure 8show that the MIViT module\nwith four layers transformer has the best segmen-\ntation performance and the appropriate number of\nparameters.\nIn the training procedure, the trade-off weight 𝜆mcps is\nused to control the balance between the supervised and\nunsupervised consistency losses. As shown in Figure 9,\nto evaluate the effects of different deﬁnitions of the\ntrade-off weight 𝜆mcps, we compare the performance\nchanges of the time-dependent Gaussian warming up\nfunction46,47and ﬁxed values (i.e., 0.1 and 0.5) dur-\ning 300 training epochs. Figure 9demonstrates that the\ntime-dependent Gaussian warming up function used in\nour method can help segmentation networks converge\nstably and obtain better overall performance compared\nto other ﬁxed values.\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nMCTHNet 5475\nFIGURE 8 Comparison of parameters and segmentation\nperformances for different numbers of transformer layers.\nFIGURE 9 Effects of different deﬁnitions of the trade-off weight\n𝜆mcps in the training procedure. (a) and (b) show the average DSC\nvalues of all regions in testing abdominal CT and MR images during\n300 training epochs, respectively . \"Ours\" denotes the time-dependent\nGaussian warming up function used in our method.\n4.6 Impacts of different labeling ratios\nTo evaluate the impacts of different labeling ratios, as\nshown in Figure 10, we vary the labeling ratio of train-\ning data to compare the segmentation performances\nof different methods in unpaired multi-modal abdominal\nsegmentation, including U-Net (Single)6without multi-\nmodal learning, UMMKD14with knowledge distillation\nloss, UNetDSBN +MT23,44with semi-supervised learn-\ning, and Ours. Speciﬁcally, we set the labeling ratio\nof training data at three different scales (25%, 50%,\nand 75%). Figure 10shows that our proposed method\nsigniﬁcantly outperforms other methods under 25%,\n50%, and 75% labeling ratios, and only requires 25%\nlabeled data to obtain better segmentation performance\nthan the U-Net (Single) method trained by 75% labeled\ndata,which demonstrates that our proposed method can\neffectively learn abundant label information from unla-\nbeled multi-modal unpaired images and signiﬁcantly\noutperforms other state-of-the-art unpaired multi-modal\nsegmentation methods under various labeling ratios.\nOverall, for the scenario with a labeling rate of 25%, our\nproposed method achieves a comparable performance\nclose to fully supervised single-modal segmentation\nmethods with 100% labeled data while reducing the cost\nof data annotation by 75%.5 DISCUSSION\nThis paper aims at constructing a robust and uni-\nﬁed segmentation method for automatically and accu-\nrately segmenting cardiac substructures and abdominal\norgans from unpaired CT and MR scans with lim-\nited annotations. The difﬁculties of segmentation tasks\nin this paper mainly arise from the heterogeneous\nvisual representation between different modalities, spa-\ntial misalignment between unpaired images, and lack\nof annotation information, which cause the signiﬁcant\nperformance degradation of traditional fully supervised\nsingle-modal methods and shared-weight methods in\nTables 2and3. To effectively learn from unpaired multi-\nmodal images, existing unpaired multi-modal learn-\ning methods13,14,23exploit multiple individual network\nmodules to learn modality-speciﬁc information and\nshared-weight modules for all modalities to capture\nmodality-invariant features, obtaining current state-of-\nthe-art segmentation results. Within these methods,\nmodality-speciﬁc feature normalization layers are widely\nused to efﬁciently learn different feature distributions\nacross modalities, which mainly focus on the inten-\nsity distribution gap between different modalities. Unlike\nexisting methods, our proposed method not only signif-\nicantly alleviates the lack of global contexts and scale\nvariation problems ignored by previous works but also\ncollaboratively learns modality-invariant and modality-\nspeciﬁc representations by fewer parameters. As shown\nin Tables 2and 3, for both the cardiac and abdominal\nsegmentation tasks with fully labeled data,our proposed\nmethod surpasses current state-of-the-art UMMKD14\nusing unpaired multi-modal learning,and the overall per-\nformance of it is better than single-modal methods by\nfewer parameters. On the other hand, existing unpaired\nmulti-modal learning methods only consider super-\nvised segmentation tasks with sufﬁcient labeled data,\nwhile our proposed method further focuses on semi-\nsupervised segmentation tasks with scarce annotations.\nFor the scenario when the labeling ratio is 25%, as\nshown in Tables 1and2,by automatically learning abun-\ndant annotation information from unlabeled unpaired\nmulti-modal scans, our proposed method signiﬁcantly\noutperforms other state-of-the-art methods by a large\nmargin and achieves similar performance as fully super-\nvised single-modal methods with 100% labeled data.\nAlthough our experiments are conducted on the car-\ndiac substructure and abdominal multi-organ datasets\nwith CT and MR modalities, our proposed approach\ncan be easily extended to other multi-modal analy-\nsis tasks or datasets in clinical practice. For example,\nby effectively learning modality-invariant and modality-\nspeciﬁc representations, our method allows employing\nmore modalities (e.g., multiple MR modalities, MR and\npathology, MR and PET) to segment various organs\n(e.g., prostate, brain, breast) and lesion areas (e.g., can-\ncers) from large-scale clinical datasets.56,57On the other\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n5476 MCTHNet\nFIGURE 10 Performance comparisons of different methods in unpaired multi-modal abdominal multi-organ segmentation under three\nlabeling ratios, that is, 25%, 50%, and 75%, where the center point indicates the average DSC value of all regions and the error line indicates the\nstandard deviation. (a) and (b) show the segmentation performances of different methods in testing CT and MR scans, respectively .\nhand, by simply adjusting the network architecture and\nnormalization parameters of our method, our method\ncan also be applied in the classiﬁcation, segmenta-\ntion, and reconstruction tasks of multi-site CT or MR\ndatasets.58–60In addition, our method could be further\nimproved by replacing 2D operations with 3D operations\nfor capturing the volumetric spatial information that is\ninherently available in medical images.8,53In the future,\nwe will explore these extensions and introduce more\nadvanced unpaired multi-modal learning strategies in\nour method.\n6 CONCLUSION\nIn this paper, we present an effective and robust\nunpaired multi-modal semi-supervised segmentation\nframework for cardiac substructure and abdominal\nmulti-organ segmentation, which collaboratively learns\nmodality-invariant and modality-speciﬁc representations\nfrom CT and MR scans. In our proposed method, to\nalleviate the intensity distribution gap and scale varia-\ntion problems, we propose a MSSC module that can\nadaptively adjust the receptive ﬁeld sizes and select\nthe most effective feature normalization parameters\naccording to the input. To capture generalizable rep-\nresentations between different modalities, we propose\na MIViT module as the shared bottleneck layer, which\nimplicitly incorporates convolution-like local operations\nwith the global processing of transformers. Moreover,\nwe design a simple but effective MCPS method to\nguide our proposed method to learn abundant anno-\ntation information from unlabeled multi-modal scans.\nExtensive experimental results show that our proposedmethod not only signiﬁcantly outperforms other unpaired\nmulti-modal learning methods, but also achieves a\ncomparable segmentation performance close to single-\nmodal fully-supervised methods with 100% labeled data\nby only leveraging 25% labeled data.\nACKNOWLEDGMENTS\nWe thank the organizers of the MMWHS-2017 dataset,\nthe BTCV dataset, and the CHAOS dataset for pro-\nviding benchmark datasets for validating the seg-\nmentation performance. This work was supported by\nthe Wuhan Major Science and Technology Project\nfor Breaking Through the Key Bottleneck Technolo-\ngies (Grant No.2021022002023426) and HUST-UIH-IRI\nJoint Laboratory Project for Medical Imaging Technol-\nogy Research and Medical Product Development (Grant\nNo. 2021420118000418).\nCONFLICT OF INTEREST STATEMENT\nNo conﬂict of interest exists in the submission of this\nmanuscript, and the manuscript is approved by all\nauthors for publication.\nREFERENCES\n1. Tempany CMC, Jayender J, Kapur T, et al. Multimodal imag-\ning for improved diagnosis and treatment of cancers. Cancer .\n2015;121(6):817-827.\n2. Skaarup M, Lundemann MJ, Darkner S, et al. A framework for\nvoxel-based assessment of biological effect after proton radio-\ntherapy in pediatric brain cancer patients using multi-modal\nimaging. Med Phys . 2021;48(7):4110-4121.\n3. Jiang J, Hu Y, Tyagi N, et al. Cross-modality (CT-MRI) prior aug-\nmented deep learning for robust lung tumor segmentation from\nsmall MR datasets. Med Phys . 2019;46(10):4392-4404.\n4. Yang X, Wang Z, Liu C, et al. Joint detection and diagnosis of\nprostate cancer in multi-parametric MRI based on multimodal\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nMCTHNet 5477\nconvolutional neural networks. In: International Conference on\nMedical Image Computing and Computer-Assisted Intervention.\nSpringer; 2017:426-434.\n5. Baltrušaitis T, Ahuja C, Morency L-P. Multimodal machine learn-\ning: a survey and taxonomy . IEEE Trans Pattern Anal Mach Intell .\n2018;41(2):423-443.\n6. Ronneberger O, Fischer P, Brox T, U-net: convolutional networks\nfor biomedical image segmentation. In: International Confer-\nence on Medical Image Computing and Computer-Assisted\nIntervention. Springer; 2015:234-241.\n7. Guo Z, Li X, Huang H, Guo N, Li Q. Deep learning-based image\nsegmentation on multimodal medical imaging. IEEE Trans Radiat\nPlasma Med Sci . 2019;3(2):162-169.\n8. Isensee F, Jaeger PF, Kohl SAA, Petersen J, Maier-Hein KH.\nnnU-Net: a self-conﬁguring method for deep learning-based\nbiomedical image segmentation. Nat Methods . 2021;18(2):203-\n211.\n9. Sun L, Ma W, Ding X, Huang Y, Liang D, Paisley J. A 3D spa-\ntially weighted network for segmentation of brain tissue from MRI.\nIEEE Trans Med Imaging . 2020;39(4):898-909. doi: 10.1109/TMI.\n2019.2937271\n10. Zhou T, Canu S, Vera P, Ruan S. Latent correlation represen-\ntation learning for brain tumor segmentation with missing MRI\nmodalities. IEEE Trans Image Process . 2021;30:4263-4274. doi:\n10.1109/TIP .2021.30707521\n11. Dolz J, Gopinath K, Yuan J, Lombaert H, Desrosiers C,\nBen A. HyperDense-Net: a hyper-densely connected CNN for\nmulti-modal image segmentation. IEEE Trans Med Imaging .\n2019;38(5):1116-1126. doi: 10.1109/TMI.2018.2878669\n12. Dou Q, Ouyang C, Chen C, et al. PnP-AdaNet: plug-and-play\nadversarial domain adaptation network at unpaired cross-\nmodality cardiac segmentation. IEEE Access . 2019;7:99065-\n99076.\n13. Valindria VV, Pawlowski N, Rajchl M, et al. Multi-modal learning\nfrom unpaired images:application to multi-organ segmentation in\nCT and MRI.In:2018 IEEE Winter Conference on Applications of\nComputer Vision (WACV). IEEE; 2018:547-556.\n14. Dou Q, Liu Q, Heng PA, Glocker B. Unpaired multi-modal seg-\nmentation via knowledge distillation. IEEE Trans Med Imaging .\n2020;39(7):2415-2425.\n15. Chen J, Li W, Li H, Zhang J, Deep class-speciﬁc afﬁnity-guided\nconvolutional network for multimodal unpaired image segmen-\ntation. In: International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention. Springer; 2020:187-\n196.\n16. Zhou P, Chen H, Li Y, Peng Y. Unpaired multi-modal tumor seg-\nmentation with structure adaptation. Appl Intell . 2022;53(4):1-13.\nPublished online.\n17. Yang J, Zhang R, Wang C, Li Z, Wan X, Zhang L, Toward unpaired\nmulti-modal medical image segmentation via learning structured\nsemantic consistency . arXiv Prepr arXiv220610571. Published\nonline 2022.\n18. Kavur AE,Gezer NS,Barı¸ s M,et al.CHAOS challenge - combined\n(CT-MR) healthy abdominal organ segmentation. Med Image\nAnal . 2021;69:101950.\n19. Zhuang X, Li L, Payer C, et al. Evaluation of algorithms for\nmulti-modality whole heart segmentation: an open-access grand\nchallenge. Med Image Anal . 2019;58:101537.\n20. Guan H, Liu M. Domain adaptation for medical image analysis: a\nsurvey. IEEE Trans Biomed Eng . 2021;69(3):1173-1185.\n21. Chang W-G, You T, Seo S, Kwak S, Han B, Domain-speciﬁc\nbatch normalization for unsupervised domain adaptation.In: Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition . 2019:7354-7362.\n22. Zhou Y, Huang L, Zhou T, Shao L, CCT-Net: category-invariant\ncross-domain transfer for medical single-to-multiple disease\ndiagnosis. In: Proceedings of the IEEE/CVF International Con-\nference on Computer Vision. 2021:8260-8270.23. Zhou Z, Qi L, Yang X, Ni D, Shi Y, Generalizable cross-modality\nmedical image segmentation via style augmentation and dual\nnormalization. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 2022:20856-20865.\n24. Zhou Q, Wang R, Zeng G, Fan H, Zheng G. Towards bridging the\ndistribution gap: instance to prototype earth mover’s distance for\ndistribution alignment. Med Image Anal . 2022;82:102607.\n25. Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth\n16x16 words: transformers for image recognition at scale. In:\nInternational Conference on Learning Representations. 2020.\n26. Chen J, Lu Y, Yu Q, et al. Transunet: Transformers make\nstrong encoders for medical image segmentation. arXiv Prepr\narXiv210204306. Published online 2021.\n27. Xiao T, Singh M, Mintun E, Darrell T, Dollár P, Girshick R. Early\nconvolutions help transformers see better. Adv Neural Inf Process\nSyst. 2021;34:30392-30400.\n28. Mehta S, Rastegari M, MobileViT: light-weight, general-purpose,\nand mobile-friendly vision transformer . In: International Confer-\nence on Learning Representations. 2021.\n29. Cheplygina V,de Bruijne M,Pluim JPW.Not-so-supervised:a sur-\nvey of semi-supervised, multi-instance, and transfer learning in\nmedical image analysis. Med Image Anal . 2019;54:280-296.\n30. Wang Z, Lin Y, Cheng K-TT, Yang X. Semi-supervised mp-\nMRI data synthesis with StitchLayer and auxiliary distance\nmaximization. Med Image Anal . 2020;59:101565.\n31. Zhao Z,Xu K,Li S,Zeng Z,Guan C,Mt-uda:towards unsupervised\ncross-modality medical image segmentation with limited source\nlabels. In: International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention. Springer; 2021:293-\n303.\n32. Zhu L, Yang K, Zhang M, Chan LL, Ng TK, Ooi BC, Semi-\nsupervised unpaired multi-modal learning for label-efﬁcient med-\nical image segmentation.In:International Conference on Medical\nImage Computing and Computer-Assisted Intervention.Springer;\n2021:394-404.\n33. Chen X, Zhou H-Y, Liu F, Guo J, Wang L, Yu Y. MASS:\nmodality-collaborative semi-supervised segmentation by exploit-\ning cross-modal consistency from unpaired CT and MRI images.\nMed Image Anal . 2022;80:102506. Published online.\n34. Zhang S, Zhang J, Tian B, Lukasiewicz T, Xu Z. Multi-modal\ncontrastive mutual learning and pseudo-label re-learning for\nsemi-supervised medical image segmentation. Med Image Anal .\n2023;83:102656. doi: 10.1016/j.media.2022.102656\n35. Chen X, Yuan Y, Zeng G, Wang J, Semi-supervised semantic\nsegmentation with cross pseudo supervision. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 2021:2613-2622.\n36. Chen C, Dou Q, Chen H, Qin J, Heng PA. Unsupervised bidirec-\ntional cross-modality adaptation via deeply synergistic image and\nfeature alignment for medical image segmentation. IEEE Trans\nMed Imaging . 2020;39(7):2494-2505.\n37. Wu F, Zhuang X. CF distance: a new domain discrepancy\nmetric and application to explicit domain adaptation for cross-\nmodality cardiac image segmentation. IEEE Trans Med Imaging .\n2020;39(12):4274-4285.\n38. Bousmalis K, Trigeorgis G, Silberman N, Krishnan D, Erhan\nD. Domain separation networks. Adv Neural Inf Process Syst .\n2016;29.\n39. Pei C, Wu F, Huang L, Zhuang X. Disentangle domain features\nfor cross-modality cardiac image segmentation. Med Image Anal .\n2021;71:102078.\n40. Feng C-M, Yan Y, Chen G, et al. Multi-modal transformer for\naccelerated MR imaging. IEEE Trans Med Imaging . 2022:1-1.\nPublished online.\n41. Zhang Y, He N, Yang J, et al. mmFormer: Multimodal Medical\nTransformer for Incomplete Multimodal Learning of Brain Tumor\nSegmentation. arXiv Prepr arXiv220602425. Published online\n2022.\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n5478 MCTHNet\n42. Luo X, Wang G, Liao W, et al. Semi-supervised medical image\nsegmentation via uncertainty rectiﬁed pyramid consistency. Med\nImage Anal . 2022;80:102517.\n43. Lee D-H, Pseudo-label: the simple and efﬁcient semi-supervised\nlearning method for deep neural networks. In: Workshop on\nChallenges in Representation Learning, ICML. 2013; 3:896.\n44. Tarvainen A, Valpola H. Mean teachers are better role models:\nweight-averaged consistency targets improve semi-supervised\ndeep learning results. Adv Neural Inf Process Syst . 2017:30.\n45. Cui W, Liu Y, Li Y, et al. Semi-supervised brain lesion segmen-\ntation with an adapted mean teacher model. In: International\nConference on Information Processing in Medical Imaging.\nSpringer; 2019:554-565.\n46. Yu L, Wang S, Li X, Fu C-W, Heng P-A, Uncertainty-aware self-\nensembling model for semi-supervised 3D left atrium segmenta-\ntion. In: International Conference on Medical Image Computing\nand Computer-Assisted Intervention. Springer; 2019:605-613.\n47. Luo X, Hu M, Song T, Wang G, Zhang S, Semi-supervised med-\nical image segmentation via cross teaching between CNN and\ntransformer . In: Medical Imaging with Deep Learning. 2021.\n48. Dorent R, Kujawa A, Ivory M, et al. CrossMoDA 2021 challenge:\nbenchmark of cross-modality domain adaptation techniques for\nvestibular schwannoma and cochlea segmentation. Med Image\nAnal . 2023;83:102628.\n49. Liu J, Desrosiers C, Zhou Y, Semi-supervised medical image\nsegmentation using cross-model pseudo-supervision with shape\nawareness and local context constraints. In: International Con-\nference on Medical Image Computing and Computer-Assisted\nIntervention. Springer; 2022:140-150.\n50. Wu Y, He K, Group normalization. In: Proceedings of the\nEuropean Conference on Computer Vision (ECCV). 2018:3-19.\n51. Li X, Wang W, Hu X, Yang J, Selective kernel networks. In: Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 2019:510-519.\n52. Ma N, Zhang X, Zheng H-T, Sun J, Shufﬂenet v2: practical\nguidelines for efﬁcient cnn architecture design. In: Proceed-\nings of the European Conference on Computer Vision (ECCV).\n2018:116-131.53. Hatamizadeh A, Tang Y, Nath V, et al. Unetr: transformers\nfor 3d medical image segmentation. In: Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision. 2022:574-584.\n54. Ouyang C, Bifﬁ C, Chen C, Kart T, Qiu H, Rueckert D, Self-\nsupervision with superpixels: training few-shot medical image\nsegmentation without annotation. In: European Conference on\nComputer Vision. Springer; 2020:762-780.\n55. He K, Zhang X, Ren S, Sun J, Delving deep into rectiﬁers: sur-\npassing human-level performance on imagenet classiﬁcation. In:\nProceedings of the IEEE International Conference on Computer\nVision. 2015:1026-1034.\n56. ran LiuP,L Lu,yao ZhangJ,tong HuoT,xiang LiuS,wei YeZ.Appli-\ncation of artiﬁcial intelligence in medicine: an overview. Curr Med\nSci. 2021;41(6):1105-1115. PMID: 34874486.\n57. Li C, Li W, Liu C, Zheng H, Cai J, Wang S. Artiﬁcial intelligence\nin multiparametric magnetic resonance imaging: a review. Med\nPhys . 2022;49(10):e1024-e1054.\n58. Liu Q, Dou Q, Yu L, Heng PA. MS-Net: multi-site network for\nimproving prostate segmentation with heterogeneous MRI data.\nIEEE Trans Med Imaging . 2020;39(9):2713-2724.\n59. Wang Z, Liu Q, Dou Q. Contrastive cross-site learning with\nredesigned net for COVID-19 CT classiﬁcation. IEEE J Biomed\nHeal Informatics . 2020;24(10):2806-2813.\n60. Liu P, Lu L, Liu S, et al. Mixed reality assists the ﬁght against\nCOVID-19. Intell Med . 2021;1(1):16-18.\nHow to cite this article: Liu H, Zhuang Y, Song\nE, et al. A modality-collaborative convolution and\ntransformer hybrid network for unpaired\nmulti-modal medical image segmentation with\nlimited annotations. Med Phys .\n2023;50:5460–5478.\nhttps://doi.org/10.1002/mp.16338\n 24734209, 2023, 9, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16338 by Shanghai Jiaotong University, Wiley Online Library on [11/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nYOLO-World: Real-Time Open-Vocabulary Object Detection\nTianheng Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang Wang3,✉, Ying Shan1,2\n∗equal contribution†project lead✉corresponding author\n1Tencent AI Lab2ARC Lab, Tencent PCG\n3School of EIC, Huazhong University of Science & Technology\nCode & Models: YOLO-World\nAbstract\nThe You Only Look Once (YOLO) series of detectors\nhave established themselves as efficient and practical tools.\nHowever, their reliance on predefined and trained ob-\nject categories limits their applicability in open scenar-\nios. Addressing this limitation, we introduce YOLO-World,\nan innovative approach that enhances YOLO with open-\nvocabulary detection capabilities through vision-language\nmodeling and pre-training on large-scale datasets. Specif-\nically, we propose a new Re-parameterizable Vision-\nLanguage Path Aggregation Network (RepVL-PAN) and\nregion-text contrastive loss to facilitate the interaction be-\ntween visual and linguistic information. Our method excels\nin detecting a wide range of objects in a zero-shot man-\nner with high efficiency. On the challenging LVIS dataset,\nYOLO-World achieves 35.4 AP with 52.0 FPS on V100,\nwhich outperforms many state-of-the-art methods in terms\nof both accuracy and speed. Furthermore, the fine-tuned\nYOLO-World achieves remarkable performance on several\ndownstream tasks, including object detection and open-\nvocabulary instance segmentation.\n1. Introduction\nObject detection has been a long-standing and fundamental\nchallenge in computer vision with numerous applications in\nimage understanding, robotics, and autonomous vehicles.\nTremendous works [16, 27, 43, 45] have achieved signif-\nicant breakthroughs in object detection with the develop-\nment of deep neural networks. Despite the success of these\nmethods, they remain limited as they only handle object de-\ntection with a fixed vocabulary, e.g., 80 categories in the\nCOCO [26] dataset. Once object categories are defined and\nlabeled, trained detectors can only detect those specific cat-\negories, thus limiting the ability and applicability of open\n20×SpeedupFigure 1. Speed-and-Accuracy Curve. We compare YOLO-\nWorld with recent open-vocabulary methods in terms of speed and\naccuracy. All models are evaluated on the LVIS minival and in-\nference speeds are measured on one NVIDIA V100 w/o TensorRT.\nThe size of the circle represents the model’s size.\nscenarios.\nRecent works [8, 13, 48, 53, 58] have explored the\nprevalent vision-language models [19, 39] to address open-\nvocabulary detection [58] through distilling vocabulary\nknowledge from language encoders, e.g., BERT [5]. How-\never, these distillation-based methods are much limited due\nto the scarcity of training data with a limited diversity of\nvocabulary, e.g., OV-COCO [58] containing 48 base cate-\ngories. Several methods [24, 30, 56, 57, 59] reformulate ob-\nject detection training as region-level vision-language pre-\ntraining and train open-vocabulary object detectors at scale.\nHowever, those methods still struggle for detection in real-\nworld scenarios, which suffer from two aspects: (1) heavy\ncomputation burden and (2) complicated deployment for\nedge devices. Previous works [24, 30, 56, 57, 59] have\n1arXiv:2401.17270v3  [cs.CV]  22 Feb 2024\n\ndemonstrated the promising performance of pre-training\nlarge detectors while pre-training small detectors to en-\ndow them with open recognition capabilities remains un-\nexplored.\nIn this paper, we present YOLO-World, aiming for\nhigh-efficiency open-vocabulary object detection, and ex-\nplore large-scale pre-training schemes to boost the tradi-\ntional YOLO detectors to a new open-vocabulary world.\nCompared to previous methods, the proposed YOLO-\nWorld is remarkably efficient with high inference speed\nand easy to deploy for downstream applications. Specif-\nically, YOLO-World follows the standard YOLO archi-\ntecture [20] and leverages the pre-trained CLIP [39] text\nencoder to encode the input texts. We further propose\nthe Re-parameterizable Vision-Language Path Aggregation\nNetwork (RepVL-PAN) to connect text features and im-\nage features for better visual-semantic representation. Dur-\ning inference, the text encoder can be removed and the\ntext embeddings can be re-parameterized into weights of\nRepVL-PAN for efficient deployment. We further inves-\ntigate the open-vocabulary pre-training scheme for YOLO\ndetectors through region-text contrastive learning on large-\nscale datasets, which unifies detection data, grounding data,\nand image-text data into region-text pairs. The pre-trained\nYOLO-World with abundant region-text pairs demonstrates\na strong capability for large vocabulary detection and train-\ning more data leads to greater improvements in open-\nvocabulary capability.\nIn addition, we explore a prompt-then-detect paradigm\nto further improve the efficiency of open-vocabulary object\ndetection in real-world scenarios. As illustrated in Fig. 2,\ntraditional object detectors [16, 20, 23, 41–43, 52] con-\ncentrate on the fixed-vocabulary (close-set) detection with\npredefined and trained categories. While previous open-\nvocabulary detectors [24, 30, 56, 59] encode the prompts of\na user for online vocabulary with text encoders and detect\nobjects. Notably, those methods tend to employ large de-\ntectors with heavy backbones, e.g., Swin-L [32], to increase\nthe open-vocabulary capacity. In contrast, the prompt-then-\ndetect paradigm (Fig. 2 (c)) first encodes the prompts of a\nuser to build an offline vocabulary and the vocabulary varies\nwith different needs. Then, the efficient detector can infer\nthe offline vocabulary on the fly without re-encoding the\nprompts. For practical applications, once we have trained\nthe detector, i.e., YOLO-World, we can pre-encode the\nprompts or categories to build an offline vocabulary and\nthen seamlessly integrate it into the detector.\nOur main contributions can be summarized into three\nfolds:\n• We introduce the YOLO-World, a cutting-edge open-\nvocabulary object detector with high efficiency for real-\nworld applications.\n• We propose a Re-parameterizable Vision-Language PANto connect vision and language features and an open-\nvocabulary region-text contrastive pre-training scheme\nfor YOLO-World.\n• The proposed YOLO-World pre-trained on large-scale\ndatasets demonstrates strong zero-shot performance and\nachieves 35.4 AP on LVIS with 52.0 FPS. The pre-trained\nYOLO-World can be easily adapted to downstream tasks,\ne.g., open-vocabulary instance segmentation and referring\nobject detection. Moreover, the pre-trained weights and\ncodes of YOLO-World will be open-sourced to facilitate\nmore practical applications.\n2. Related Works\n2.1. Traditional Object Detection\nPrevalent object detection research concentrates on fixed-\nvocabulary (close-set) detection, in which object detectors\nare trained on datasets with pre-defined categories, e.g.,\nCOCO dataset [26] and Objects365 dataset [46], and then\ndetect objects within the fixed set of categories. During\nthe past decades, the methods for traditional object de-\ntection can be simply categorized into three groups, i.e.,\nregion-based methods, pixel-based methods, and query-\nbased methods. The region-based methods [11, 12, 16, 27,\n44], such as Faster R-CNN [44], adopt a two-stage frame-\nwork for proposal generation [44] and RoI-wise (Region-\nof-Interest) classification and regression. The pixel-based\nmethods [28, 31, 42, 49, 61] tend to be one-stage detec-\ntors, which perform classification and regression over pre-\ndefined anchors or pixels. DETR [1] first explores object\ndetection through transformers [50] and inspires extensive\nquery-based methods [64]. In terms of inference speed,\nRedmon et al. presents YOLOs [40–42] which exploit sim-\nple convolutional architectures for real-time object detec-\ntion. Several works [10, 23, 33, 52, 55] propose various\narchitectures or designs for YOLO, including path aggrega-\ntion networks [29], cross-stage partial networks [51], and\nre-parameterization [6], which further improve both speed\nand accuracy. In comparison to previous YOLOs, YOLO-\nWorld in this paper aims to detect objects beyond the fixed\nvocabulary with strong generalization ability.\n2.2. Open-Vocabulary Object Detection\nOpen-vocabulary object detection (OVD) [58] has emerged\nas a new trend for modern object detection, which aims\nto detect objects beyond the predefined categories. Early\nworks [13] follow the standard OVD setting [58] by train-\ning detectors on the base classes and evaluating the novel\n(unknown) classes. Nevertheless, this open-vocabulary set-\nting can evaluate the capability of detectors to detect and\nrecognize novel objects, it is still limited for open scenar-\nios and lacks generalization ability to other domains due\nto training on the limited dataset and vocabulary. Inspired\n2\n\nDetectorText Encoder(a) Traditional Object Detector(b) PreivousOpen-V ocabulary Detector(c) YOLO-WorldObject DetectorFixedVocabulary\nText EncoderLarge Detector\nText EncoderLightweight DetectorOfflineVocabulary\nUserUserOnlineVocabularyRe-parameterize\nUserTrain-onlyFigure 2. Comparison with Detection Paradigms. (a) Traditional Object Detector : These object detectors can only detect objects\nwithin the fixed vocabulary pre-defined by the training datasets, e.g., 80 categories of COCO dataset [26]. The fixed vocabulary limits the\nextension for open scenes. (b) Previous Open-Vocabulary Detectors: Previous methods tend to develop large and heavy detectors for\nopen-vocabulary detection which intuitively have strong capacity. In addition, these detectors simultaneously encode images and texts as\ninput for prediction, which is time-consuming for practical applications. (c) YOLO-World: We demonstrate the strong open-vocabulary\nperformance of lightweight detectors, e.g., YOLO detectors [20, 42], which is of great significance for real-world applications. Rather than\nusing online vocabulary, we present a prompt-then-detect paradigm for efficient inference, in which the user generates a series of prompts\naccording to the need and the prompts will be encoded into an offline vocabulary. Then it can be re-parameterized as the model weights\nfor deployment and further acceleration.\nYOLO BackboneTextEncoder\nA manand a womanare skiing with a dogcaption, noun phrases, category…\nUserText Embeddings\nMulti-scale Image FeaturesTextContrastive HeadBox HeadmanwomandogV ocabulary Embeddingsmanwomandog\nUser’s  V ocabularyDogImage-aware Embeddings\nMulti-scaleImage FeaturesTraining: Online VocabularyDeployment: Offline VocabularyVision-Language PAN\nObject EmbeddingsRegion-Text Matching\nInputImageExtract Nouns\nFigure 3. Overall Architecture of YOLO-World. Compared to traditional YOLO detectors, YOLO-World as an open-vocabulary detector\nadopts text as input. The Text Encoder first encodes the input text input text embeddings. Then the Image Encoder encodes the input image\ninto multi-scale image features and the proposed RepVL-PAN exploits the multi-level cross-modality fusion for both image and text features.\nFinally, YOLO-World predicts the regressed bounding boxes and the object embeddings for matching the categories or nouns that appeared\nin the input text.\nby vision-language pre-training [19, 39], recent works [8,\n22, 53, 62, 63] formulate open-vocabulary object detection\nas image-text matching and exploit large-scale image-text\ndata to increase the training vocabulary at scale. OWL-\nViTs [35, 36] fine-tune the simple vision transformers [7]\nwith detection and grounding datasets and build the sim-\nple open-vocabulary detectors with promising performance.\nGLIP [24] presents a pre-training framework for open-\nvocabulary detection based on phrase grounding and eval-\nuates in a zero-shot setting. Grounding DINO [30] incor-\nporates the grounded pre-training [24] into detection trans-\nformers [60] with cross-modality fusions. Several meth-\nods [25, 56, 57, 59] unify detection datasets and image-textdatasets through region-text matching and pre-train detec-\ntors with large-scale image-text pairs, achieving promising\nperformance and generalization. However, these methods\noften use heavy detectors like ATSS [61] or DINO [60]\nwith Swin-L [32] as a backbone, leading to high com-\nputational demands and deployment challenges. In con-\ntrast, we present YOLO-World, aiming for efficient open-\nvocabulary object detection with real-time inference and\neasier downstream application deployment. Differing from\nZSD-YOLO [54], which also explores open-vocabulary de-\ntection [58] with YOLO through language model align-\nment, YOLO-World introduces a novel YOLO framework\nwith an effective pre-training strategy, enhancing open-\n3\n\nvocabulary performance and generalization.\n3. Method\n3.1. Pre-training Formulation: Region-Text Pairs\nThe traditional object detection methods, including the\nYOLO-series [20], are trained with instance annotations\nΩ ={Bi, ci}N\ni=1, which consist of bounding boxes {Bi}\nand category labels {ci}. In this paper, we reformulate the\ninstance annotations as region-text pairs Ω ={Bi, ti}N\ni=1,\nwhere tiis the corresponding text for the region Bi. Specif-\nically, the text tican be the category name, noun phrases,\nor object descriptions. Moreover, YOLO-World adopts both\nthe image Iand texts T(a set of nouns) as input and outputs\npredicted boxes {ˆBk}and the corresponding object embed-\ndings{ek}(ek∈RD).\n3.2. Model Architecture\nThe overall architecture of the proposed YOLO-World is il-\nlustrated in Fig. 3, which consists of a YOLO detector , a\nText Encoder , and a Re-parameterizable Vision-Language\nPath Aggregation Network (RepVL-PAN). Given the input\ntext, the text encoder in YOLO-World encodes the text into\ntext embeddings. The image encoder in the YOLO detector\nextracts the multi-scale features from the input image. Then\nwe leverage the RepVL-PAN to enhance both text and im-\nage representation by exploiting the cross-modality fusion\nbetween image features and text embeddings.\nYOLO Detector. YOLO-World is mainly developed\nbased on YOLOv8 [20], which contains a Darknet back-\nbone [20, 43] as the image encoder, a path aggregation net-\nwork (PAN) for multi-scale feature pyramids, and a head\nfor bounding box regression and object embeddings.\nText Encoder. Given the text T, we adopt the Trans-\nformer text encoder pre-trained by CLIP [39] to extract the\ncorresponding text embeddings W=TextEncoder (T)∈\nRC×D, where Cis the number of nouns and Dis the em-\nbedding dimension. The CLIP text encoder offers better\nvisual-semantic capabilities for connecting visual objects\nwith texts compared to text-only language encoders [5].\nWhen the input text is a caption or referring expression,\nwe adopt the simple n-gram algorithm to extract the noun\nphrases and then feed them into the text encoder.\nText Contrastive Head. Following previous works [20],\nwe adopt the decoupled head with two 3×3convs to regress\nbounding boxes {bk}K\nk=1and object embeddings {ek}K\nk=1,\nwhere Kdenotes the number of objects. We present a text\ncontrastive head to obtain the object-text similarity sk,jby:\nsk,j=α·L2-Norm (ek)·L2-Norm (wj)⊤+β, (1)where L2-Norm (·)is the L2 normalization and wj∈W\nis the j-th text embeddings. In addition, we add the affine\ntransformation with the learnable scaling factor αand shift-\ning factor β. Both the L2 norms and the affine transforma-\ntions are important for stabilizing the region-text training.\nTraining with Online Vocabulary. During training, we\nconstruct an online vocabulary Tfor each mosaic sample\ncontaining 4 images. Specifically, we sample all positive\nnouns involved in the mosaic images and randomly sam-\nple some negative nouns from the corresponding dataset.\nThe vocabulary for each mosaic sample contains at most M\nnouns, and Mis set to 80 as default.\nInference with Offline Vocabulary. At the inference\nstage, we present a prompt-then-detect strategy with an of-\nfline vocabulary for further efficiency. As shown in Fig. 3,\nthe user can define a series of custom prompts, which might\ninclude captions or categories. We then utilize the text en-\ncoder to encode these prompts and obtain offline vocabu-\nlary embeddings. The offline vocabulary allows for avoid-\ning computation for each input and provides the flexibility\nto adjust the vocabulary as needed.\n3.3. Re-parameterizable Vision-Language PAN\nFig. 4 shows the structure of the proposed RepVL-PAN\nwhich follows the top-down and bottom-up paths in [20, 29]\nto establish the feature pyramids {P3, P4, P5}with the\nmulti-scale image features {C3, C4, C5}. Furthermore,\nwe propose the Text-guided CSPLayer (T-CSPLayer) and\nImage-Pooling Attention (I-Pooling Attention) to further\nenhance the interaction between image features and text\nfeatures, which can improve the visual-semantic represen-\ntation for open-vocabulary capability. During inference, the\noffline vocabulary embeddings can be re-parameterized into\nweights of convolutional or linear layers for deployment.\nText-guided CSPLayer. As Fig. 4 illustrates, the cross-\nstage partial layers (CSPLayer) are utilized after the top-\ndown or bottom-up fusion. We extend the CSPLayer\n(also called C2f) of [20] by incorporating text guidance\ninto multi-scale image features to form the Text-guided\nCSPLayer. Specifically, given the text embeddings Wand\nimage features Xl∈RH×W×D(l∈ {3,4,5}), we adopt\nthemax-sigmoid attention after the last dark bottleneck\nblock to aggregate text features into image features by:\nX′\nl=Xl·δ( max\nj∈{1..C}(XlW⊤\nj))⊤, (2)\nwhere the updated X′\nlis concatenated with the cross-stage\nfeatures as output. The δindicates the sigmoid function.\n4\n\nmanwomandog=\nC3C4C5T-CSPLayerT-CSPLayerT-CSPLayerT-CSPLayerI-Pooling AttentionP3P4P5I-Pooling AttentionText Embeddings\n×12×12×2×2Image-aware Embeddings\nText to ImageImage to TextDark BottleneckMax-SigmoidCSTextSplitSCConcatT-CSPLayer (C2f Block)I-PoolingAttentionCMHCAText3×3Figure 4. Illustration of the RepVL-PAN. The proposed RepVL-\nPAN adopts the Text-guided CSPLayer (T-CSPLayer) for injecting\nlanguage information into image features and the Image Pooling\nAttention (I-Pooling Attention) for enhancing image-aware text\nembeddings.\nImage-Pooling Attention. To enhance the text embed-\ndings with image-aware information, we aggregate image\nfeatures to update the text embeddings by proposing the\nImage-Pooling Attention. Rather than directly using cross-\nattention on image features, we leverage max pooling on\nmulti-scale features to obtain 3×3regions, resulting in a\ntotal of 27 patch tokens ˜X∈R27×D. The text embeddings\nare then updated by:\nW′=W+MultiHead-Attention (W,˜X,˜X)(3)\n3.4. Pre-training Schemes\nIn this section, we present the training schemes for pre-\ntraining YOLO-World on large-scale detection, grounding,\nand image-text datasets.\nLearning from Region-Text Contrastive Loss. Given\nthe mosaic sample Iand texts T, YOLO-World outputs\nKobject predictions {Bk, sk}K\nk=1along with annotations\nΩ ={Bi, ti}N\ni=1. We follow [20] and leverage task-aligned\nlabel assignment [9] to match the predictions with ground-\ntruth annotations and assign each positive prediction with a\ntext index as the classification label. Based on this vocabu-\nlary, we construct the region-text contrastive loss Lconwith\nregion-text pairs through cross entropy between object-text\n(region-text) similarity and object-text assignments. In ad-\ndition, we adopt IoU loss and distributed focal loss for\nbounding box regression and the total training loss is de-\nfined as: L(I) =Lcon+λI·(Liou+Ldfl),where λIis\nan indicator factor and set to 1 when input image Iis from\ndetection or grounding data and set to 0 when it is fromthe image-text data. Considering image-text datasets have\nnoisy boxes, we only calculate the regression loss for sam-\nples with accurate bounding boxes.\nPseudo Labeling with Image-Text Data. Rather than di-\nrectly using image-text pairs for pre-training, we propose an\nautomatic labeling approach to generate region-text pairs.\nSpecifically, the labeling approach contains three steps: (1)\nextract noun phrases : we first utilize the n-gram algo-\nrithm to extract noun phrases from the text; (2) pseudo la-\nbeling : we adopt a pre-trained open-vocabulary detector,\ne.g., GLIP [24], to generate pseudo boxes for the given\nnoun phrases for each image, thus providing the coarse\nregion-text pairs. (3) filtering : We employ the pre-trained\nCLIP [39] to evaluate the relevance of image-text pairs and\nregion-text pairs, and filter the low-relevance pseudo an-\nnotations and images. We further filter redundant bound-\ning boxes by incorporating methods such as Non-Maximum\nSuppression (NMS). We suggest the readers refer to the ap-\npendix for the detailed approach. With the above approach,\nwe sample and label 246k images from CC3M [47] with\n821k pseudo annotations.\n4. Experiments\nIn this section, we demonstrate the effectiveness of the\nproposed YOLO-World by pre-training it on large-scale\ndatasets and evaluating YOLO-World in a zero-shot manner\non both LVIS benchmark and COCO benchmark (Sec. 4.2).\nWe also evaluate the fine-tuning performance of YOLO-\nWorld on COCO, LVIS for object detection.\n4.1. Implementation Details\nThe YOLO-World is developed based on the MMYOLO\ntoolbox [3] and the MMDetection toolbox [2]. Following\n[20], we provide three variants of YOLO-World for differ-\nent latency requirements, e.g., small (S), medium (M), and\nlarge (L). We adopt the open-source CLIP [39] text encoder\nwith pre-trained weights to encode the input text. Unless\nspecified, we measure the inference speeds of all models on\none NVIDIA V100 GPU without extra acceleration mecha-\nnisms, e.g., FP16 or TensorRT.\n4.2. Pre-training\nExperimental Setup. At the pre-training stage, we adopt\nthe AdamW optimizer [34] with an initial learning rate\nof 0.002 and weight decay of 0.05. YOLO-World is pre-\ntrained for 100 epochs on on 32 NVIDIA V100 GPUs with\na total batch size of 512. During pre-training, we follow\nprevious works [20] and adopt color augmentation, random\naffine, random flip, and mosaic with 4 images for data aug-\nmentation. The text encoder is frozen during pre-training.\n5\n\nDataset Type V ocab. Images Anno.\nObjects365V1 [46] Detection 365 609k 9,621k\nGQA [17] Grounding - 621k 3,681k\nFlickr [38] Grounding - 149k 641k\nCC3M †[47] Image-Text - 246k 821k\nTable 1. Pre-training Data. The specifications of the datasets\nused for pre-training YOLO-World.\nPre-training Data. For pre-training YOLO-World, we\nmainly adopt detection or grounding datasets including Ob-\njects365 (V1) [46], GQA [17], Flickr30k [38], as specified\nin Tab. 1. Following [24], we exclude the images from\nthe COCO dataset in GoldG [21] (GQA and Flickr30k).\nThe annotations of the detection datasets used for pre-\ntraining contain both bounding boxes and categories or\nnoun phrases. In addition, we also extend the pre-training\ndata with image-text pairs, i.e., CC3M†[47], which we have\nlabeled 246k images through the pseudo-labeling method\ndiscussed in Sec. 3.4.\nZero-shot Evaluation. After pre-training, we di-\nrectly evaluate the proposed YOLO-World on the LVIS\ndataset [14] in a zero-shot manner. The LVIS dataset\ncontains 1203 object categories, which is much more\nthan the categories of the pre-training detection datasets\nand can measure the performance on large vocabulary\ndetection. Following previous works [21, 24, 56, 57], we\nmainly evaluate on LVIS minival [21] and report the\nFixed AP [4] for comparison. The maximum number of\npredictions is set to 1000.\nMain Results on LVIS Object Detection. In Tab. 2, we\ncompare the proposed YOLO-World with recent state-of-\nthe-art methods [21, 30, 56, 57, 59] on LVIS benchmark in a\nzero-shot manner. Considering the computation burden and\nmodel parameters, we mainly compare with those methods\nbased on lighter backbones, e.g., Swin-T [32]. Remarkably,\nYOLO-World outperforms previous state-of-the-art meth-\nods in terms of zero-shot performance and inference speed.\nCompared to GLIP, GLIPv2, and Grounding DINO, which\nincorporate more data, e.g., Cap4M (CC3M+SBU [37]),\nYOLO-World pre-trained on O365 & GolG obtains bet-\nter performance even with fewer model parameters. Com-\npared to DetCLIP, YOLO-World achieves comparable per-\nformance (35.4 v.s.34.4) while obtaining 20×increase in\ninference speed. The experimental results also demonstrate\nthat small models, e.g., YOLO-World-S with 13M parame-\nters, can be used for vision-language pre-training and ob-\ntain strong open-vocabulary capabilities.4.3. Ablation Experiments\nWe provide extensive ablation studies to analyze YOLO-\nWorld from two primary aspects, i.e., pre-training and ar-\nchitecture. Unless specified, we mainly conduct ablation\nexperiments based on YOLO-World-L and pre-train Ob-\njects365 with zero-shot evaluation on LVIS minival .\nPre-training Data. In Tab. 3, we evaluate the perfor-\nmance of pre-training YOLO-World using different data.\nCompared to the baseline trained on Objects365, adding\nGQA can significantly improve performance with an 8.4\nAP gain on LVIS. This improvement can be attributed to\nthe richer textual information provided by the GQA dataset,\nwhich can enhance the model’s ability to recognize large\nvocabulary objects. Adding part of CC3M samples (8%\nof the full datasets) can further bring 0.5 AP gain with 1.3\nAP on rare objects. Tab. 3 demonstrates that adding more\ndata can effectively improve the detection capabilities on\nlarge-vocabulary scenarios. Furthermore, as the amount of\ndata increases, the performance continues to improve, high-\nlighting the benefits of leveraging larger and more diverse\ndatasets for training.\nAblations on RepVL-PAN. Tab. 4 demonstrates the ef-\nfectiveness of the proposed RepVL-PAN of YOLO-World,\nincluding Text-guided CSPLayers and Image Pooling At-\ntention, for the zero-shot LVIS detection. Specifically, we\nadopt two settings, i.e., (1) pre-training on O365 and (2)\npre-training on O365 & GQA. Compared to O365 which\nonly contains category annotations, GQA includes rich\ntexts, particularly in the form of noun phrases. As shown\nin Tab. 4, the proposed RepVL-PAN improves the base-\nline (YOLOv8-PAN [20]) by 1.1 AP on LVIS, and the im-\nprovements are remarkable in terms of the rare categories\n(APr) of LVIS, which are hard to detect and recognize. In\naddition, the improvements become more significant when\nYOLO-World is pre-trained with the GQA dataset and ex-\nperiments indicate that the proposed RepVL-PAN works\nbetter with rich textual information.\nText Encoders. In Tab. 5, we compare the performance\nof using different text encoders, i.e., BERT-base [5] and\nCLIP-base (ViT-base) [39]. We exploit two settings dur-\ning pre-training, i.e., frozen and fine-tuned, and the learn-\ning rate for fine-tuning text encoders is a 0.01×factor of\nthe basic learning rate. As Tab. 5 shows, the CLIP text\nencoder obtains superior results than BERT (+10.1 AP for\nrare categories in LVIS), which is pre-trained with image-\ntext pairs and has better capability for vision-centric embed-\ndings. Fine-tuning BERT during pre-training brings signifi-\ncant improvements (+3.7 AP) while fine-tuning CLIP leads\nto a severe performance drop. We attribute the drop to that\n6\n\nMethod Backbone Params Pre-trained Data FPS AP AP rAPcAPf\nMDETR [21] R-101 [15] 169M GoldG - 24.2 20.9 24.3 24.2\nGLIP-T [24] Swin-T [32] 232M O365,GoldG 0.12 24.9 17.7 19.5 31.0\nGLIP-T [24] Swin-T [32] 232M O365,GoldG,Cap4M 0.12 26.0 20.8 21.4 31.0\nGLIPv2-T [59] Swin-T [32] 232M O365,GoldG 0.12 26.9 - - -\nGLIPv2-T [59] Swin-T [32] 232M O365,GoldG,Cap4M 0.12 29.0 - - -\nGrounding DINO-T [30] Swin-T [32] 172M O365,GoldG 1.5 25.6 14.4 19.6 32.2\nGrounding DINO-T [30] Swin-T [32] 172M O365,GoldG,Cap4M 1.5 27.4 18.1 23.3 32.7\nDetCLIP-T [56] Swin-T [32] 155M O365,GoldG 2.3 34.4 26.9 33.9 36.3\nYOLO-World-S YOLOv8-S 13M (77M) O365,GoldG 74.1 (19.9) 26.2 19.1 23.6 29.8\nYOLO-World-M YOLOv8-M 29M (92M) O365,GoldG 58.1 (18.5) 31.0 23.8 29.2 33.9\nYOLO-World-L YOLOv8-L 48M (110M) O365,GoldG 52.0 (17.6) 35.0 27.1 32.8 38.3\nYOLO-World-L YOLOv8-L 48M (110M) O365,GoldG,CC3M†52.0 (17.6) 35.4 27.6 34.1 38.0\nTable 2. Zero-shot Evaluation on LVIS. We evaluate YOLO-World on LVIS minival [21] in a zero-shot manner. We report the Fixed\nAP[4] for a fair comparison with recent methods.†denotes the pseudo-labeled CC3M in our setting, which contains 246k samples.\nThe FPS is evaluated on one NVIDIA V100 GPU w/o TensorRT. The parameters and FPS of YOLO-World are evaluated for both the\nre-parameterized version (w/o bracket) and the original version (w/ bracket).\nPre-trained Data AP AP rAPcAPf\nO365 23.5 16.2 21.1 27.0\nO365,GQA 31.9 22.5 29.9 35.4\nO365,GoldG 32.5 22.3 30.6 36.0\nO365,GoldG,CC3M†33.0 23.6 32.0 35.5\nTable 3. Ablations on Pre-training Data. We evaluate the zero-\nshot performance on LVIS of pre-training YOLO-World with dif-\nferent amounts of data.\nGQA T →I I→T AP AP rAPcAPf\n✗ ✗ ✗ 22.4 14.5 20.1 26.0\n✗✓ ✗ 23.2 15.2 20.6 27.0\n✗✓ ✓ 23.5 16.2 21.1 27.0\n✓ ✗ ✗ 29.7 21.0 27.1 33.6\n✓ ✓ ✓ 31.9 22.5 29.9 35.4\nTable 4. Ablations on Re-parameterizable Vision-Language\nPath Aggregation Network. We evaluate the zero-shot perfor-\nmance on LVIS of the proposed Vision-Language Path Aggrega-\ntion Network. T →I and I→T denote the Text-guided CSPLayers\nand Image-Pooling Attention, respectively.\nfine-tuning on O365 may degrade the generalization ability\nof the pre-trained CLIP, which contains only 365 categories\nand lacks abundant textual information.\n4.4. Fine-tuning YOLO-World\nIn this section, we further fine-tune YOLO-World for close-\nset object detection on the COCO dataset and LVIS datasetText Encoder Frozen? AP AP rAPcAPf\nBERT-base Frozen 14.6 3.4 10.7 20.0\nBERT-base Fine-tune 18.3 6.6 14.6 23.6\nCLIP-base Frozen 22.4 14.5 20.1 26.0\nCLIP-base Fine-tune 19.3 8.6 15.7 24.8\nTable 5. Text Encoder in YOLO-World. We ablate different text\nencoders in YOLO-World through the zero-shot LVIS evaluation.\nto demonstrate the effectiveness of the pre-training.\nExperimental Setup. We use the pre-trained weights to\ninitialize YOLO-World for fine-tuning. All models are fine-\ntuned for 80 epochs with the AdamW optimizer and the ini-\ntial learning rate is set to 0.0002. In addition, we fine-tune\nthe CLIP text encoder with a learning factor of 0.01. For the\nLVIS dataset, we follow previous works [8, 13, 63] and fine-\ntune YOLO-World on the LVIS-base (common & frequent)\nand evaluate it on the LVIS-novel (rare).\nCOCO Object Detection. We compare the pre-trained\nYOLO-World with previous YOLO detectors [20, 23, 52]\nin Tab. 6. For fine-tuning YOLO-World on the COCO\ndataset, we remove the proposed RepVL-PAN for fur-\nther acceleration considering that the vocabulary size of\nthe COCO dataset is small. In Tab. 6, it’s evident that\nour approach can achieve decent zero-shot performance on\nthe COCO dataset, which indicates that YOLO-World has\nstrong generalization ability. Moreover, YOLO-World af-\nter fine-tuning on the COCO train2017 demonstrates\n7\n\nMethod Pre-train AP AP 50AP75FPS\nTraining from scratch.\nYOLOv6-S [23] ✗ 43.7 60.8 47.0 442\nYOLOv6-M [23] ✗ 48.4 65.7 52.7 277\nYOLOv6-L [23] ✗ 50.7 68.1 54.8 166\nYOLOv7-T [52] ✗ 37.5 55.8 40.2 404\nYOLOv7-L [52] ✗ 50.9 69.3 55.3 182\nYOLOv7-X [52] ✗ 52.6 70.6 57.3 131\nYOLOv8-S [20] ✗ 44.4 61.2 48.1 386\nYOLOv8-M [20] ✗ 50.5 67.3 55.0 238\nYOLOv8-L [20] ✗ 52.9 69.9 57.7 159\nZero-shot transfer.\nYOLO-World-S O+G 37.6 52.3 40.7 -\nYOLO-World-M O+G 42.8 58.3 46.4 -\nYOLO-World-L O+G 44.4 59.8 48.3 -\nYOLO-World-L O+G+C 45.1 60.7 48.9 -\nFine-tuned w/ RepVL-PAN.\nYOLO-World-S O+G 45.9 62.3 50.1 -\nYOLO-World-M O+G 51.2 68.1 55.9 -\nYOLO-World-L O+G+C 53.3 70.1 58.2 -\nFine-tuned w/o RepVL-PAN.\nYOLO-World-S O+G 45.7 62.3 49.9 373\nYOLO-World-M O+G 50.7 67.2 55.1 231\nYOLO-World-L O+G+C 53.3 70.3 58.1 156\nTable 6. Comparison with YOLOs on COCO Object Detec-\ntion. We fine-tune the YOLO-World on COCO train2017 and\nevaluate on COCO val2017 . The results of YOLOv7 [52] and\nYOLOv8 [20] are obtained from MMYOLO [3]. ‘O’, ‘G’, and ‘C’\ndenote pertaining using Objects365, GoldG, and CC3M†, respec-\ntively. The FPS is measured on one NVIDIA V100 w/ TensorRT.\nhigher performance compared to previous methods trained\nfrom scratch.\nLVIS Object Detection. In Tab. 7, we evaluate the fine-\ntuning performance of YOLO-World on the standard LVIS\ndataset. Firstly, compared to the oracle YOLOv8s [20]\ntrained on the full LVIS datasets, YOLO-World achieves\nsignificant improvements, especially for larger models, e.g.,\nYOLO-World-L outperforms YOLOv8-L by 7.2 AP and\n10.2 AP r. The improvements can demonstrate the effec-\ntiveness of the proposed pre-training strategy for large-\nvocabulary detection. Moreover, YOLO-World, as an effi-\ncient one-stage detector, outperforms previous state-of-the-\nart two-stage methods [8, 13, 22, 53, 63] on the overall per-\nformance without extra designs, e.g., learnable prompts [8]\nor region-based alginments [13].Method AP AP r APcAPf\nViLD [13] 27.8 16.7 26.5 34.2\nRegionCLIP [62] 28.2 17.1 - -\nDetic [63] 26.8 17.8 - -\nFVLM [22] 24.2 18.6 - -\nDetPro [8] 28.4 20.8 27.8 32.4\nBARON [53] 29.5 23.2 29.3 32.5\nYOLOv8-S 19.4 7.4 17.4 27.0\nYOLOv8-M 23.1 8.4 21.3 31.5\nYOLOv8-L 26.9 10.2 25.4 35.8\nYOLO-World-S 23.9 12.8 20.4 32.7\nYOLO-World-M 28.8 15.9 24.6 39.0\nYOLO-World-L 34.1 20.4 31.1 43.5\nTable 7. Comparison with Open-Vocabulary Detectors on\nLVIS. We train YOLO-World on the LVIS-base (including com-\nmon and frequent) report the bbox AP . The YOLO-v8 are trained\non the full LVIS datasets (including base and novel) along with the\nclass balanced sampling.\n4.5. Open-Vocabulary Instance Segmentation\nIn this section, we further fine-tune YOLO-World for\nsegmenting objects under the open-vocabulary setting,\nwhich can be termed open-vocabulary instance segmenta-\ntion (OVIS). Previous methods [18] have explored OVIS\nwith pseudo-labelling on novel objects. Differently, con-\nsidering that YOLO-World has strong transfer and gener-\nalization capabilities, we directly fine-tune YOLO-World\non a subset of data with mask annotations and evaluate the\nsegmentation performance under large-vocabulary settings.\nSpecifically, we benchmark open-vocabulary instance seg-\nmentation under two settings:\n• (1) COCO to LVIS setting, we fine-tune YOLO-World on\nthe COCO dataset (including 80 categories) with mask\nannotations, under which the models need to transfer\nfrom 80 categories to 1203 categories ( 80→1203 );\n• (2) LVIS-base to LVIS setting, we fine-tune YOLO-World\non the LVIS-base (including 866 categories, common &\nfrequent) with mask annotations, under which the models\nneed to transfer from 866 categories to 1203 categories\n(866→1203 ).\nWe evaluate the fine-tuned models on the standard LVIS\nval2017 with 1203 categories, in which 337 rare cate-\ngories are unseen and can be used to measure the open-\nvocabulary performance.\nResults. Tab. 8 shows the experimental results of extend-\ning YOLO-World for open-vocabulary instance segmenta-\ntion. Specifically, we adopt two fine-tuning strategies: (1)\nonly fine-tuning the segmentation head and (2) fine-tuning\n8\n\nall modules. Under strategy (1), the fine-tuned YOLO-\nWorld still retains the zero-shot capabilities acquired from\nthe pre-training stage, allowing it to generalize to unseen\ncategories without additional fine-tuning. Strategy (2) en-\nables YOLO-World fit the LVIS dataset better, but it may\nresult in the degradation of the zero-shot capabilities.\nTab. 8 shows the comparisons of fine-tuning YOLO-\nWorld with different settings (COCO or LVIS-base) and\ndifferent strategies (fine-tuning seg. head or fine-tuning\nall). Firstly, fine-tuning on LVIS-base obtains better perfor-\nmance compared to that based on COCO. However, the ra-\ntios between AP and AP r(APr/AP) are nearly unchanged,\ne.g., the ratios of YOLO-World on COCO and LVIS-base\nare 76.5% and 74.3%, respectively. Considering that the\ndetector is frozen, we attribute the performance gap to the\nfact that the LVIS dataset provides more detailed and denser\nsegmentation annotations, which are beneficial for learn-\ning the segmentation head. When fine-tuning all mod-\nules, YOLO-World obtains remarkable improvements on\nLVIS, e.g., YOLO-World-L achieves 9.6 AP gain. However,\nthe fine-tuning might degrade the open-vocabulary perfor-\nmance and lead to a 0.6 box AP rdrop for YOLO-World-L.\n4.6. Visualizations\nWe provide the visualization results of pre-trained YOLO-\nWorld-L under three settings: (a) we perform zero-shot\ninference with LVIS categories; (b) we input the custom\nprompts with fine-grained categories with attributes; (c) re-\nferring detection. The visualizations also demonstrate that\nYOLO-World has a strong generalization ability for open-\nvocabulary scenarios along with referring ability.\nZero-shot Inference on LVIS. Fig. 5 shows the visual-\nization results based on the LVIS categories which are gen-\nerated by the pre-trained YOLO-World-L in a zero-shot\nmanner. The pre-trained YOLO-World exhibits strong zero-\nshot transfer capabilities and is able to detect as many ob-\njects as possible within the image.\nInference with User’s Vocabulary. In Fig. 6, we explore\nthe detection capabilities of YOLO-World with our defined\ncategories. The visualization results demonstrate that the\npre-trained YOLO-World-L also exhibits the capability for\n(1) fine-grained detection ( i.e., detect the parts of one ob-\nject) and (2) fine-grained classification ( i.e., distinguish dif-\nferent sub-categories of objects.).\nReferring Object Detection. In Fig. 7, we leverage some\ndescriptive (discriminative) noun phrases as input, e.g., the\nstanding person, to explore whether the model can locate\nregions or objects in the image that match our given in-\nput. The visualization results display the phrases and theircorresponding bounding boxes, demonstrating that the pre-\ntrained YOLO-World has the referring or grounding capa-\nbility. This ability can be attributed to the proposed pre-\ntraining strategy with large-scale training data.\n5. Conclusion\nWe present YOLO-World, a cutting-edge real-time open-\nvocabulary detector aiming to improve efficiency and open-\nvocabulary capability in real-world applications. In this pa-\nper, we have reshaped the prevalent YOLOs as a vision-\nlanguage YOLO architecture for open-vocabulary pre-\ntraining and detection and proposed RepVL-PAN, which\nconnects vision and language information with the network\nand can be re-parameterized for efficient deployment. We\nfurther present the effective pre-training schemes with de-\ntection, grounding and image-text data to endow YOLO-\nWorld with a strong capability for open-vocabulary de-\ntection. Experiments can demonstrate the superiority of\nYOLO-World in terms of speed and open-vocabulary per-\nformance and indicate the effectiveness of vision-language\npre-training on small models, which is insightful for future\nresearch. We hope YOLO-World can serve as a new bench-\nmark for addressing real-world open-vocabulary detection.\nReferences\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In ECCV , pages\n213–229, 2020. 2\n[2] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,\nJifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChen Change Loy, and Dahua Lin. MMDetection: Open\nmmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155 , 2019. 5\n[3] MMYOLO Contributors. MMYOLO: OpenMMLab YOLO\nseries toolbox and benchmark. https://github.com/\nopen-mmlab/mmyolo , 2022. 5, 8\n[4] Achal Dave, Piotr Doll ´ar, Deva Ramanan, Alexander Kir-\nillov, and Ross B. Girshick. Evaluating large-vocabulary\nobject detectors: The devil is in the details. CoRR ,\nabs/2102.01066, 2021. 6, 7\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT , pages\n4171–4186, 2019. 1, 4, 6\n[6] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,\nGuiguang Ding, and Jian Sun. Repvgg: Making vgg-style\nconvnets great again. In CVPR , pages 13733–13742, 2021.\n2\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n9\n\nModel Fine-tune Data Fine-tune Modules AP AP rAPcAPfAPbAPb\nr\nYOLO-World-M COCO Seg Head 12.3 9.1 10.9 14.6 22.3 16.2\nYOLO-World-L COCO Seg Head 16.2 12.4 15.0 19.2 25.3 18.0\nYOLO-World-M LVIS-base Seg Head 16.7 12.6 14.6 20.8 22.3 16.2\nYOLO-World-L LVIS-base Seg Head 19.1 14.2 17.2 23.5 25.3 18.0\nYOLO-World-M LVIS-base All 25.9 13.4 24.9 32.6 32.6 15.8\nYOLO-World-L LVIS-base All 28.7 15.0 28.3 35.2 36.2 17.4\nTable 8. Open-Vocabulary Instance Segmentation. We evaluate YOLO-World for open-vocabulary instance segmentation under the two\nsettings. We fine-tune the segmentation head or all modules of YOLO-World and report Mask AP for comparison. APbdenotes the box\nAP.\nFigure 5. Visualization Results on Zero-shot Inference on LVIS. We adopt the pre-trained YOLO-World-L and infer with the LVIS\nvocabulary (containing 1203 categories) on the COCO val2017 .\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR , 2021. 3\n[8] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao,\nand Guoqi Li. Learning to prompt for open-vocabulary ob-\nject detection with vision-language model. In CVPR , pages\n14064–14073, 2022. 1, 3, 7, 8\n[9] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R. Scott,\nand Weilin Huang. TOOD: task-aligned one-stage object de-\ntection. In ICCV , pages 3490–3499. IEEE, 2021. 5\n[10] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\nSun. YOLOX: exceeding YOLO series in 2021. CoRR ,\nabs/2107.08430, 2021. 2\n[11] Ross B. Girshick. Fast R-CNN. In ICCV , pages 1440–1448,\n2015. 2\n[12] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In CVPR , pages 580–587, 2014.\n2\n[13] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.Open-vocabulary object detection via vision and language\nknowledge distillation. In ICLR , 2022. 1, 2, 7, 8\n[14] Agrim Gupta, Piotr Doll ´ar, and Ross B. Girshick. LVIS: A\ndataset for large vocabulary instance segmentation. In CVPR ,\npages 5356–5364, 2019. 6\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR ,\npages 770–778, 2016. 7\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B.\nGirshick. Mask R-CNN. In ICCV , pages 2980–2988, 2017.\n1, 2\n[17] Drew A. Hudson and Christopher D. Manning. GQA: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR , pages 6700–6709, 2019. 6\n[18] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan\nElhamifar. Open-vocabulary instance segmentation via ro-\nbust cross-modal pseudo-labeling. In CVPR , pages 7010–\n7021, 2022. 8\n[19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc V . Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\n10\n\n{men, women, boy, girl}{golden dog, black dog, spotted dog}{elephant, ear, leg, trunk, ivory}{grass, sky, zebra, trunk, tree}Figure 6. Visualization Results on User’s Vocabulary. We define the custom vocabulary for each input image and YOLO-World can\ndetect the accurate regions according to the vocabulary. Images are obtained from COCO val2017 .\nthe person in redthe brown animalthe tallest personperson with a white shirt\nmoonthe standing personperson holding a toyperson holding a baseball batthe jumping person\nFigure 7. Visualization Results on Referring Object Detection. We explore the capability of the pre-trained YOLO-World to detect\nobjects with descriptive noun phrases. Images are obtained from COCO val2017 .\nlearning with noisy text supervision. In ICML , pages 4904–\n4916, 2021. 1, 3\n[20] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralyt-\nics yolov8. https://github.com/ultralytics/\nultralytics , 2023. 2, 3, 4, 5, 6, 7, 8\n[21] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion. MDETR - mod-\nulated detection for end-to-end multi-modal understanding.\nInICCV , pages 1760–1770, 2021. 6, 7\n[22] Weicheng Kuo, Yin Cui, Xiuye Gu, A. J. Piergiovanni,\nand Anelia Angelova. F-VLM: open-vocabulary object de-\ntection upon frozen vision and language models. CoRR ,\nabs/2209.15639, 2022. 3, 8\n[23] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei\nGeng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\nWeiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang, Linyuan\nZhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, and\nXiaolin Wei. Yolov6: A single-stage object detection frame-work for industrial applications. CoRR , abs/2209.02976,\n2022. 2, 7, 8\n[24] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and\nJianfeng Gao. Grounded language-image pre-training. In\nCVPR , pages 10955–10965, 2022. 1, 2, 3, 5, 6, 7, 13\n[25] Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gho-\nlamreza Haffari, Zehuan Yuan, and Jianfei Cai. Learning\nobject-language alignments for open-vocabulary object de-\ntection. In ICLR , 2023. 3\n[26] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC. Lawrence Zitnick. Microsoft COCO: common objects\nin context. In Proceedings of the European Conference on\nComputer Vision (ECCV) , pages 740–755, 2014. 1, 2, 3, 13\n[27] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He,\nBharath Hariharan, and Serge J. Belongie. Feature pyramid\n11\n\nnetworks for object detection. In CVPR , pages 936–944,\n2017. 1, 2\n[28] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,\nand Piotr Doll ´ar. Focal loss for dense object detection. In\nICCV , pages 2999–3007, 2017. 2\n[29] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\nPath aggregation network for instance segmentation. In\nCVPR , pages 8759–8768, 2018. 2, 4\n[30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, and Lei Zhang. Grounding DINO: marrying DINO with\ngrounded pre-training for open-set object detection. CoRR ,\nabs/2303.05499, 2023. 1, 2, 3, 6, 7\n[31] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C.\nBerg. SSD: single shot multibox detector. In ECCV , pages\n21–37, 2016. 2\n[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV , pages 9992–10002, 2021. 2, 3, 6, 7\n[33] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang,\nQingqing Dang, Yuan Gao, Hui Shen, Jianguo Ren, Shumin\nHan, Errui Ding, and Shilei Wen. PP-YOLO: an effec-\ntive and efficient implementation of object detector. CoRR ,\nabs/2007.12099, 2020. 2\n[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR , 2019. 5\n[35] Matthias Minderer, Alexey A. Gritsenko, Austin Stone,\nMaxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy,\nAravindh Mahendran, Anurag Arnab, Mostafa Dehghani,\nZhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and\nNeil Houlsby. Simple open-vocabulary object detection with\nvision transformers. In ECCV , 2022. 3\n[36] Matthias Minderer, Alexey A. Gritsenko, and Neil Houlsby.\nScaling open-vocabulary object detection. In NeurIPS , 2023.\n3\n[37] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In NeurIPS , pages 1143–1151, 2011. 6\n[38] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,\nJuan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.\nFlickr30k entities: Collecting region-to-phrase correspon-\ndences for richer image-to-sentence models. Int. J. Comput.\nVis., pages 74–93, 2017. 6\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In ICML , pages\n8748–8763, 2021. 1, 2, 3, 4, 5, 6, 13\n[40] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\nstronger. In CVPR , pages 6517–6525, 2017. 2\n[41] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\nimprovement. CoRR , abs/1804.02767, 2018. 2\n[42] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick,\nand Ali Farhadi. You only look once: Unified, real-time ob-\nject detection. In CVPR , pages 779–788, 2016. 2, 3[43] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick,\nand Ali Farhadi. You only look once: Unified, real-time ob-\nject detection. In CVPR , pages 779–788, 2016. 1, 2, 4\n[44] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\nFaster R-CNN: towards real-time object detection with re-\ngion proposal networks. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence , pages 1137–1149, 2017. 2\n[45] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\nFaster R-CNN: towards real-time object detection with re-\ngion proposal networks. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence , pages 1137–1149, 2017. 1\n[46] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:\nA large-scale, high-quality dataset for object detection. In\nICCV , pages 8429–8438, 2019. 2, 6\n[47] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In ACL,\npages 2556–2565, 2018. 5, 6, 13\n[48] Cheng Shi and Sibei Yang. Edadet: Open-vocabulary ob-\nject detection using early dense alignment. In ICCV , pages\n15678–15688, 2023. 1\n[49] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\nfully convolutional one-stage object detection. In ICCV ,\npages 9626–9635, 2019. 2\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS , pages\n5998–6008, 2017. 2\n[51] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A\nnew backbone that can enhance learning capability of CNN.\nInCVPRW , pages 1571–1580, 2020. 2\n[52] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\nYuan Mark Liao. Yolov7: Trainable bag-of-freebies sets\nnew state-of-the-art for real-time object detectors. In CVPR ,\npages 7464–7475, 2023. 2, 7, 8\n[53] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and\nChen Change Loy. Aligning bag of regions for open-\nvocabulary object detection. In CVPR , pages 15254–15264,\n2023. 1, 3, 8\n[54] Johnathan Xie and Shuai Zheng. ZSD-YOLO: zero-shot\nYOLO detection using vision-language knowledgedistilla-\ntion. CoRR , 2021. 3\n[55] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,\nCheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing\nDang, Shengyu Wei, Yuning Du, and Baohua Lai.\nPP-YOLOE: an evolved version of YOLO. CoRR ,\nabs/2203.16250, 2022. 2\n[56] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan\nXu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu.\nDetclip: Dictionary-enriched visual-concept paralleled pre-\ntraining for open-world detection. In NeurIPS , 2022. 1, 2, 3,\n6, 7\n[57] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei\nZhang, Zhenguo Li, and Hang Xu. Detclipv2: Scal-\nable open-vocabulary object detection pre-training via word-\n12\n\nregion alignment. In CVPR , pages 23497–23506, 2023. 1, 3,\n6\n[58] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-\nFu Chang. Open-vocabulary object detection using captions.\nInCVPR , pages 14393–14402, 2021. 1, 2, 3\n[59] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu\nYuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Uni-\nfying localization and vision-language understanding. In\nNeurIPS , 2022. 1, 2, 3, 6, 7\n[60] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\nZhu, Lionel M. Ni, and Heung-Yeung Shum. DINO: DETR\nwith improved denoising anchor boxes for end-to-end object\ndetection. In ICLR , 2023. 3\n[61] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\nStan Z. Li. Bridging the gap between anchor-based and\nanchor-free detection via adaptive training sample selection.\nInCVPR , pages 9756–9765, 2020. 2, 3\n[62] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan\nLi, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang\nDai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip:\nRegion-based language-image pretraining. In CVPR , pages\n16772–16782, 2022. 3, 8\n[63] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\nKr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand\nclasses using image-level supervision. In ECCV , pages 350–\n368, 2022. 3, 7, 8\n[64] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable DETR: deformable transformers\nfor end-to-end object detection. In ICLR , 2021. 2\nA. Additional Details\nA.1. Re-parameterization for RepVL-PAN\nDuring inference on an offline vocabulary, we adopt re-\nparameterization for RepVL-PAN for faster inference speed\nand deployment. Firstly, we pre-compute the text embed-\ndings W∈RC×Dthrough the text encoder.\nRe-parameterize T-CSPLayer. For each T-CSPLayer in\nRepVL-PAN, we can re-parameterize and simplify the pro-\ncess of adding text guidance by reshaping the text embed-\ndings W∈RC×D×1×1into the weights of a 1×1convo-\nlution layer (or a linear layer), as follows:\nX′=X⊙Sigmoid (max(Conv (X, W ),dim=1 )),(4)\nwhere X× ∈RB×D×H×WandX′∈RB×D×H×Ware\nthe input and output image features. ⊙is the matrix multi-\nplication with reshape ortranspose .\nRe-parameterize I-Pooling Attention. The I-Pooling\nAttention can be re-parameterize or simplified by:\n˜X=cat(MP(X3,3),MP(X4,3),MP(X5,3)),(5)where cat is the concentration and MP(·, 3) denotes the\nmax pooling for 3×3output features. {X3, X4, X5}are\nthe multi-scale features in RepVL-PAN. ˜Xis flattened and\nhas the shape of B×D×27. Then we can update the text\nembeddings by:\nW′=W+Softmax (W⊙˜X),dim=-1 )⊙W, (6)\nA.2. Fine-tuning Details.\nWe remove all T-CSPLayers and Image-Pooling Atten-\ntion in RepVL-PAN when transferring YOLO-World to\nCOCO [26] object detection, which only contains 80 cat-\negories and has a relatively low dependency on visual-\nlanguage interaction. During fine-tuning, we initialize\nYOLO-World using pre-trained weights. The learning rate\nof fine-tuning is set to 0.0002 with the weight decay set to\n0.05. After fine-tuning, we pre-compute the class text em-\nbeddings with given COCO categories and store the embed-\ndings into the weights of the classification layers.\nB. Automatic Labeling on Large-scale Image-\nText Data\nIn this section, we add details procedures for labeling\nregion-text pairs with large-scale image-text data, e.g.,\nCC3M [47]. The overall labeling pipeline is illustrated in\nFig. 8, which mainly consists of three procedures, i.e., (1)\nextract object nouns, (2) pseudo labeling, and (3) filtering.\nAs discussed in Sec. 3.4, we adopt the simple n-gram algo-\nrithm to extract nouns from captions.\nRegion-Text Proposals. After obtaining the set of object\nnouns T={tk}Kfrom the first step, we leverage a pre-\ntrained open-vocabulary detector, i.e., GLIP-L [24], to gen-\nerate pseudo boxes {Bi}along with confidence scores {ci}:\n{Bi, ti, ci}N\ni=1=GLIP-Labeler (I, T), (7)\nwhere {Bi, ti, ci}N\ni=1are the coarse region-text proposals.\nCLIP-based Re-scoring & Filtering. Considering the\nregion-text proposals containing much noise, we present\na restoring and filtering pipeline with the pre-trained\nCLIP [39]. Given the input image I, caption T, and\nthe coarse region-text proposals {Bi, ti, ci}N\ni=1, the specific\npipeline is listed as follows:\n• (1) Compute Image-Text Score: we forward the image I\nwith its caption Tinto CLIP and obtain the image-text\nsimilarity score simg.\n• (2) Compute Region-Text Score: we crop the region im-\nages from the input image according to the region boxes\n{Bi}. Then we forward the cropped images along with\ntheir texts {ti}into CLIP and obtain the region-text simi-\nlarity Sr={sr\ni}N\ni=1.\n13\n\nOpen-V ocabularyLabelerCLIP LabelerAutomatic Labeling Pipeline\nimagePseudo LabelingCLIP-based Filteringimage, object nounsboxes\nn-gramcaptionExtracting Object Nouns“A photography of a manand a woman”nouns\nobjectsFigure 8. Labeling Pipeline for Image-Text Data We first leverage the simple n-gram to extract object nouns from the captions. We adopt\na pre-trained open-vocabulary detector to generate pseudo boxes given the object nouns, which forms the coarse region-text proposals.\nThen we use a pre-trained CLIP to rescore or relabel the boxes along with filtering.\n• (3) [Optional] Re-Labeling: we can forward each\ncropped image with all nouns and assign the noun with\nmaximum similarity, which can help correct the texts\nwrongly labeled by GLIP.\n• (4) Rescoring: we adopt the region-text similarity Srto\nrescore the confidence scores ˜ci=pci∗sr\ni.\n• (5) Region-level Filtering: we first divide the region-text\nproposals into different groups according to the texts and\nthen perform non-maximum suppression (NMS) to fil-\nter the duplicate predictions (the NMS threshold is set to\n0.5). Then we filter out the proposals with low confidence\nscores (the threshold is set to 0.3).\n• (6) Image-level Filtering: we compute the image-level\nregion-text scores sregionby averaging the kept region-\ntext scores. Then we obtain the image-level confidence\nscore by s=√\nsimg∗sregion and we keep the images\nwith scores larger than 0.3.\nThe thresholds mentioned above are empirically set accord-\ning to the part of labeled results and the whole pipeline is\nautomatic without human verification. Finally, the labeled\nsamples are used for pre-training YOLO-World. We will\nprovide the pseudo annotations of CC3M for further re-\nsearch.\nC. Pre-training YOLO-World at Scale\nWhen pre-training small models, e.g., YOLO-World-S, a\nnatural question we have is: how much capacity does a\nsmall model have, and how much training data or what kind\nof data does a small model need? To answer this question,\nwe leverage different amounts of pseudo-labeled region-text\npairs to pre-train YOLO-World. As shown in Tab. 9, adding\nmore image-text samples can increase the zero-shot per-\nformance of YOLO-World-S. Tab. 9 indicates: (1) adding\nimage-text data can improve the overall zero-shot perfor-\nmance of YOLO-World-S; (2) using an excessive amount\nof pseudo-labeled data may have some negative effects for\nsmall models (YOLO-World-S), though it can improve theon rare categories (AP r). However, using fine-grained an-\nnotations (GoldG) for small models can provide significant\nimprovements, which indicates that large-scale high-quality\nannotated data can significantly enhance the capabilities of\nsmall models. And Tab. 3 in the main text has shown that\npre-training with the combination of fine-annotated data\nand pseudo-annotated data can perform better. We will ex-\nplore more about the data for pre-training small models or\nYOLO detectors in future work.\n14\n\nMethod Pre-trained Data Samples AP AP rAPcAPf\nYOLO-World-S O365 0.61M 16.3 9.2 14.1 20.1\nYOLO-World-S O365+GoldG 1.38M 24.2 16.4 21.7 27.8\nYOLO-World-S O365+CC3M-245k 0.85M 16.5 10.8 14.8 19.1\nYOLO-World-S O365+CC3M-520k 1.13M 19.2 10.7 17.4 22.4\nYOLO-World-S O365+CC3M-750k 1.36M 18.2 11.2 16.0 21.1\nTable 9. Zero-shot Evaluation on LVIS. We evaluate the performance of pre-training YOLO-World-S with different amounts of data, the\nimage-text data.\n15\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nCitation: Li, Z.; Yao, Y.; Wen, R.;\nLiu, Q. Dual-Modal Illumination\nSystem for Defect Detection of\nAircraft Glass Canopies. Sensors 2024 ,\n24, 6717. https://doi.org/10.3390/\ns24206717\nAcademic Editor: Iñigo Barandiaran\nReceived: 22 September 2024\nRevised: 13 October 2024\nAccepted: 16 October 2024\nPublished: 18 October 2024\nCopyright: ©2024 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nsensors\nArticle\nDual-Modal Illumination System for Defect Detection of Aircraft\nGlass Canopies\nZijian Li, Yong Yao *\n , Runyuan Wen and Qiyang Liu\nSchool of Computer Science and Technology, Xidian University, Xi’an 710126, China;\nzjian.li@stu.xidian.edu.cn (Z.L.); wenrunyuan@stu.xidian.edu.cn (R.W.); 22031212452@stu.xidian.edu.cn (Q.L.)\n*Correspondence: yaoyong@xidian.edu.cn\nAbstract: Defect detection in transparent materials typically relies on specific lighting conditions.\nHowever, through our work on defect detection for aircraft glass canopies, we found that using\na single lighting condition often led to missed or false detections. This limitation arises from the\noptical properties of transparent materials, where certain defects only become sufficiently visible\nunder specific lighting angles. To address this issue, we developed a dual-modal illumination system\nthat integrates both forward and backward lighting to capture defect images. Additionally, we\nintroduced the first dual-modal dataset for defect detection in aircraft glass canopies. Furthermore,\nwe proposed an attention-based dual-branch modal fusion network (ADMF-Net) to enhance the\ndetection process. Experimental results show that our system and model significantly improve\nthe detection performance, with the dual-modal approach increasing the mAP by 5.6% over the\nsingle-modal baseline, achieving a mAP of 98.4%. Our research also provides valuable insights for\ndefect detection in other transparent materials.\nKeywords: aircraft glass canopy; defect detection; image fusion; technical diagnostics\n1. Introduction\nTransparent materials are widely used in canopies, windshields, and windows of\naircraft. As external structural components of the aircraft and vision windows for pilots,\nthese materials must possess excellent mechanical strength and optical properties to en-\nsure operational safety. However, due to the complexities inherent in the manufacturing\nprocesses and the continuous exposure to environmental factors, such as high speed dust\nimpact and abrasion during flight [ 1], surface defects, such as cracks, scratches and pits,\ninevitably develop over time. These defects can interfere with the pilot’s field of vision and,\nin more severe cases, present significant structural safety risks [2,3].\nCurrently, the inspection and maintenance of aircraft transparent materials rely heav-\nily on manual visual inspections. This approach is not only labor-intensive and time-\nconsuming but also prone to human error, resulting in inconsistent outcomes and the\npotential for missed defects [ 4]. Therefore, the development of an automated, accurate and\nefficient defect detection system is crucial for improving the quality control and mainte-\nnance processes for aircraft transparent materials.\nIn recent years, machine vision technology has been increasingly applied to defect\ndetection and quality control in industrial manufacturing. This paper seeks to bridge the\ngap between research and industrial application by introducing a novel defect detection\nsystem for aircraft glass canopies, along with a defect dataset and detection model.\nOur contributions can be summarized as follows:\n1. An automated prototype system with a dual-modal illumination structure is proposed.\nFor the same sampling point, the system sequentially uses forward lighting and\nbackward lighting to capture defect images. By integrating the information from both\ntypes of images, more accurate defect detection is achieved.\nSensors 2024 ,24, 6717. https://doi.org/10.3390/s24206717 https://www.mdpi.com/journal/sensors\n\nSensors 2024 ,24, 6717 2 of 15\n2. A curated dataset is presented, which, to the best of our knowledge, is the first\nbenchmark in the field of defect detection for aircraft transparent materials.\n3. A dual-modal baseline is designed and proved to be competitive, in which we designed\ntwo fusion detection methods: a data-level fusion method named RGB Channel Fusion,\nand an attention-based dual-branch modal fusion network using feature-level fusion.\nBoth methods achieved excellent results on our dataset.\n2. Related Work\nProduct defect detection is a crucial aspect of quality control in the manufacturing\nindustry. Automated Optical Inspection (AOI), as a non-contact testing technique, is widely\nused across various product production lines [ 5]. However, research on defect detection\nsystems for aircraft transparent materials lags far behind. Defect detection techniques in\nother fields can serve as valuable references for this study.\n2.1. Traditional Machine Vision Method for Defect Detection in Transparent Materials\nTraditional defect detection for transparent materials often relies on specialized optical\nor acoustic instruments [ 6,7]. With the development of image processing technology and\nmachine learning, machine vision methods have started to be applied. These methods\nuse conventional image processing techniques, such as edge detection, texture analysis, or\nshape matching, to extract and identify defect objects.\nFor instance, Zhou X et al. [ 8] proposed a defect detection strategy for glass bottle\nbottoms based on wavelet transform and multi-scale filtering algorithms, which effectively\nreduces the impact of texture on defect detection. Jian C et al. [ 9] captured images of\nmobile phone cover glass under coaxial parallel lighting and used template subtraction\nand projection methods for defect detection.\nTraditional machine learning-based defect detection methods often rely on the com-\nparison between defect images and defect-free templates [ 10], making them very sensitive\nto noise, background interference, and uneven lighting. These methods are mainly suitable\nfor defect detection in controlled scenarios.\n2.2. Deep Learning-Based Visual Defect Detection\nIn recent years, deep learning-based machine vision methods have been widely ap-\nplied in the field of defect detection. Compared to traditional machine learning methods,\ndeep learning offers more complex model fitting capabilities, allowing it to learn high-\nlevel abstract features [ 11]. This results in better accuracy and generalization in defect\ndetection tasks.\nFor instance, Park J et al. [ 12] proposed a defect detection system for curved glass\nof mobile phones based on weighted multi-neural networks, effectively utilizing multi-\nchannel measurement data. Cheng L et al. [ 13] introduced an infrared thermography-based\ngenerative adversarial network (IRT-GAN) for non-destructive testing of carbon fiber and\nglass fiber polymer composites. Zhu Y et al. [ 14] introduced a ResNeSt network based\non deformable convolution to detect fine features of surface defects on mobile phones,\naddressing various defect shapes. They also used RoI Align to reduce localization errors,\nparticularly for scratch defects with high aspect ratios.\nR-CNN algorithms, including Fast R-CNN and Faster R-CNN, are well-established\ntwo-stage object detection algorithms. Pan Z et al. [ 15] developed an automatic surface\nscratch inspection system using a rail-based microscopic camera scanning system for\ndetecting defects in architectural glass curtain walls. They used a Mask R-CNN network\nmodel to achieve pixel-level instance segmentation.\nThe YOLO series algorithms, as the most typical representatives of one-stage object\ndetection algorithms, transform the object detection process into a regression task, sig-\nnificantly speeding up detection and maintaining a leading position in real-time object\ndetection tasks [ 16]. Zhang M et al. [ 17] adopted an improved YOLOv5 algorithm for\ndefect detection in solar cells, incorporating deformable convolutions, ECA-Net attention\n\nSensors 2024 ,24, 6717 3 of 15\nmechanism, Mosaic and MixUp data augmentation, and K-means++ for anchor box cluster-\ning, improving the algorithm’s mAP by 7.85 %and achieving a detection speed of 36.24 FPS,\nmeeting real-time detection requirements. Xie Y et al. [ 18] proposed a feature-enhanced\nsurface defect detection algorithm (FE-YOLO) for practical industrial applications, combin-\ning depthwise separable convolution with dense connections and introducing an improved\nfeature pyramid network to enhance spatial position correlation in multi-scale detection\nlayers, achieving high accuracy while reducing model weight.\n2.3. Multi-Modal Fusion in Object Detection\nMulti-modal information fusion has gained attention in the field of object detection.\nThese methods leverage the complementary information from different modalities to con-\nstruct more comprehensive and enriched features, thereby improving detection accuracy.\nIn multi-modal image object detection, fusion methods are generally categorized into\nthree types [ 19]: data-level fusion (early fusion), feature-level fusion (intermediate fusion),\nand decision-level fusion (late fusion).\nA typical example is the fusion of visible light and infrared images for nighttime\npedestrian detection. Data-level fusion involves combining visible light and infrared\nimages into a single image using specific algorithms or multiple data channels before\nfeeding them into the detection model. Decision-level fusion uses two separate models to\ndetect visible and infrared images independently, focusing on post-processing the outputs\nof the two models. Feature-level fusion uses parallel branches to extract independent\nfeatures from images at different scales, embedding a feature fusion layer between the\nfeature layers, and using the fused features for detection.\nCompared to decision-level fusion, a well-designed feature-level fusion method en-\nables the model to learn the correlations and differences between different modalities, and\nreduces redundant computations.\nMulti-modal fusion object detection has been applied and validated in various fields.\nExamples include RGB and infrared thermal image fusion for object detection [ 20,21], RGB\nand radar information fusion for assisted driving [ 22], polarization and infrared imaging\nfusion for PCB defect detection [ 23], and multi-spectral image fusion for medical image\nsegmentation [24].\n3. Methodology\n3.1. Dual-Modal Illumination Structure\nIn industrial machine vision defect detection, an appropriate illumination structure\ndesign is crucial for capturing defect images. Among various illumination structures,\nforward lighting is the most widely used. Depending on whether the light is reflected\ndirectly into the camera lens, which is usually controlled by the incident angle of the\nlight source, forward lighting can be divided into bright field forward lighting and dark\nfield forward lighting, as shown in Figure 1a. Additionally, for highly reflective surfaces,\nscattering forward lighting can effectively reduce the strong reflections [ 25]. This type of\nlighting is usually achieved using a dome structure or multi-angle light source to provide\nuniform illumination, as shown in Figure 1b.\nIn contrast to forward lighting, the backward lighting method places the light source\nbehind the object, as depicted in Figure 1c. Backward lighting is commonly used for defect\ndetection in transparent materials as it can highlight internal defects, but it is less sensitive\nto surface defects [26].\nTo design an appropriate illumination structure for this study, we captured images\nof typical defects under different lighting conditions, with some of the results shown in\nFigure 2.\nThrough the analysis of these images collected, we found that bright field forward\nlighting easily captures surface scratch-type defects but is susceptible to reflections and\nuneven lighting. Backward lighting effectively reveals the depth information of defects,\naiding in the analysis of defect severity and classification. However, it tends to lose surface\n\nSensors 2024 ,24, 6717 4 of 15\ndetail and some boundary contour information. The effect of dark field forward lighting is\nsimilar to backward lighting, resembling an inverted operation.\nCamera \nBright field \nDark field \nDetected object Light source \nLight \nsource Camera \nDetected object Light \nsource Camera \nDetected \nobject \nLight source \n(a) (b) (c) \nFigure 1. Schematic diagram of the light path of common illumination structures. ( a) Bright field and\ndark field forward lighting. ( b) Scattering forward lighting. ( c) Backward lighting.\nbright feld forward lighting backward  lighting dark feld  forward lighting \nFigure 2. Typical defect samples under different illumination structures.\nWe realized that the information provided by multiple modalities is complemen-\ntary, contributing to a more accurate assessment of defect conditions. Our subsequent\nexperiments in this paper also verified that the multi-modal defect detection method has\nsignificantly improved performance compared to the single-modal method.\n3.2. Image Acquisition Platform\nConsidering the practical feasibility in industrial inspection environments, we ulti-\nmately chose to use a dual-modal illumination structure, employing both forward lighting\nand backward lighting to capture and detect defects. For the implementation of forward\nlighting, we use a ring light source to achieve more uniform illumination and reduce\nreflections. For backward lighting, we use a planar light source.\n\nSensors 2024 ,24, 6717 5 of 15\nWe built a prototype dual-modal illumination image acquisition platform, as shown\nin Figure 3. This platform mainly consists of a digital microscope, a flexible arm holder, a\nbacklight panel, and a ring light source.\nFigure 3. Prototype dual-modal illumination image acquisition platform.\nThe defects on glass canopies mostly range in diameter from 80 µm to 500 µm, making\nit impractical to use conventional CCD cameras for imaging. Therefore, we use a digital\nmicroscope as the image acquisition device. This device features an adjustable focal length\nranging from 8 mm to 60 mm, with a magnification range of 50 ×to 1000 ×, and a photo\nresolution of 1280×720. During image acquisition, the focal length was set to 20 mm, with\nan approximate magnification of 400 ×.\nDue to the necessity of maintaining a fixed position between the camera lens and\nthe sample when capturing images under two lighting conditions, we used a flexible arm\nholder as the support device. This device has a length of 60 cm and a maximum load\ncapacity of 1.0 kg, providing sufficient stability for the image acquisition process.\nThe forward lighting method uses 8 ring-shaped LED lights positioned around the\ncamera lens. The backward lighting method uses a 30 cm ×42 cm backlight panel with a\nbrightness of 500 nits and a color temperature of 6500 K. The two light sources are enabled\nalternately by an electrically controlled switch during image acquisition. See Figure 4.\nFigure 4. Forward lighting and backward lighting implementation.\n\nSensors 2024 ,24, 6717 6 of 15\n3.3. Dataset Construction\nDue to the specialized nature of the research, publicly available defect sample data\nfor aircraft glass canopies are not currently accessible. Therefore, we collected data from\nour own samples of real aircraft cockpit canopies, which served as the source for our\nsubsequent experiments. See Figure 5.\nFigure 5. Aircraft glass canopy samples.\nBased on the real aircraft glass canopies samples, we collected a total of 438 raw images,\ncovering 219 defect points using two different lighting methods. Through image analysis\nand relevant experience, we categorized the collected defects into four types: contusion,\nscratches, crack, and spot. These categories essentially cover all possible defect types that\nmay appear on the aircraft glass canopy. Typical images are shown in Figure 6.\nforward \nlighting \nbackward \nlighting \ncontusion \n scratches crack spot \nFigure 6. Typical images of four types of defects.\n\nSensors 2024 ,24, 6717 7 of 15\nConsidering that many of the detected target objects are slender, that is, they have\nhigh aspect ratios, large image spans, and may be interlaced, we choose to use oriented\nbounding boxes (OBB) for annotation and detection.\nWe use X-AnyLabeling [ 27] (version 2.3.5) for data annotation, which allows the\nuse of existing segmentation models to improve annotation efficiency. We choose the\nSegment Anything (ViT-Large) model for pre-labeling, which provides preparatory polygon\nboundaries of the selected defects. Then, a script is written to calculate the minimum\nenclosing rectangle for each defect, resulting in a more precise rotated bounding box.\nManual adjustments are also necessary. This is because some defects are only clearly\noutlined under a specific lighting condition. For example, scratch defects are usually\neasier to observe under forward lighting. However, some small spot-type defects that\nare not apparent under forward lighting are very prominent under backward lighting.\nAdditionally, there are also subtle differences in the contour boundaries of the defects\nunder the two types of lighting, and in that case, the union outer contour will be selected.\nWe check each annotation file to ensure the accuracy and usability of the dataset.\nTo address the limited number of defect images available for aviation transparent\nparts, we also adopted several data augmentation methods. In order to improve the defect\ndetection performance under any rotation angle, we performed pre-rotation augmentation\non the defect samples. In this process, rotation and mirroring operations are simultaneously\napplied to the images and their labels to avoid redundant annotation work. By adjusting\nthe step size of the rotation angle, the augmentation rate can be controlled. For example,\nusing 90-degree rotations and mirroring operations, an 8 ×augmentation can be achieved\nwithout information loss. For general angles, the background color RGB(114, 114, 0) is\nfilled in the blank areas to reduce interference. Moreover, since the standard OBB format\ndoes not allow point coordinates to be located outside the image boundaries, labels that\noverflow during the rotation process need to be adjusted or deleted. The correction method\nfor overflow labels is as follows:\n1. If all four points of the bounding box are outside the field of view, delete the target\n(process ends), otherwise go to step 2.\n2. If all four points of the bounding box are within the field of view, no adjustment is\nrequired (process ends), otherwise go to step 3.\n3. Select a point that is outside the field of view. Calculate the distance between the point\nand its adjacent two points to find the long and short sides connected to the point on\nthe rectangle. Translate the short side along the direction of the long side until the\npoint falls on the image boundary. The translation amount is determined by solving a\nlinear programming problem. After adjustment, return to step 2 to re-evaluate.\nBy following these steps, we ensure that the labels are correctly normalized. Addition-\nally, during the training process, we employed the Mosaic-4 data augmentation method,\nwhich includes random image combinations, random scaling and translation, contrast\nadjustments, exposure adjustments, and noise addition.\nThe final dataset of aircraft glass canopy defects, named ag_dual_obb, includes four\ntypes of defects, totaling 1752 defect points and 4784 defect objects. Each defect point\nincludes two images: a forward lighting image and a backward lighting image, which are\nstrictly spatially aligned. The dataset is divided into a training set with 1576 image pairs\nand a test set with 176 image pairs, following a 9:1 split ratio. See Table 1.\nTable 1. Defect quantity statistics.\nIndex Defect Type Name Count\n0 contusion 1104\n1 scratches 1728\n2 crack 232\n3 spot 1720\n\nSensors 2024 ,24, 6717 8 of 15\nWe also created some variant datasets for extra experiments, which follow similar nam-\ning. For example, the dataset labeled with standard rectangular box is named ag_dual_rect,\nand the dataset obtained through RGB Channel Fusion is named ag_composite_obb.\n3.4. RGB Channel Fusion Method\nTo initially validate the effectiveness of fusion detection for this task, we first designed\na data-level fusion method named RGB Channel Fusion, as shown in Figure 7. In the RGB\nChannel Fusion method, the forward lighting image and the backward lighting image are\neach converted to grayscale. These grayscale images are then assigned to the red channel\nand green channel of the fused image, respectively, while the blue channel is obtained by\naveraging the two grayscale images.\ngray \ngray red channel \ngreen channel \nblue channel mean gray merge \nFigure 7. RGB Channel Fusion method.\nThe design rationale for the RGB Channel Fusion method is based on the fact that, for\nthis specific task, grayscale conversion hardly diminishes the image information. Moreover,\nthe averaging of images can serve as a simple yet effective image fusion technique. In\naddition, the resulting color of the fused image can provide a reference for our data\nannotation work. For example, a dark red color usually means deeper damage, which\nhelps distinguish surface scratches from more serious contusions.\n3.5. Attention-Based Dual-Branch Modal Fusion Network (ADMF-Net)\nTo fully utilize the multimodal information obtained under different lighting condi-\ntions, this paper proposes an attention-based dual-branch modal fusion network (ADMF-\nNet) for defect detection. The overall architecture of the network is shown in Figure 8. It\nconsists of two independent feature extraction backbones, feature fusion modules (AMFF,\nAttention-based Multi-modal Feature Fusion Module), a multi-scale feature pyramid pool-\ning module (FSPPF, Fusion-based Spatial Pyramid Pooling - Fast), and detection layers.\nIn this network, we use two CSPDarknet-like feature extraction backbones, as pro-\nposed in YOLOv8, to extract multi-scale features from the forward lighting and backward\nlighting modalities separately. Next, two AMFF modules are inserted between the two\nbackbones, applied to the P3 and P4 layers, to obtain fused features with different receptive\nfields. Then, the P5 layers from both modalities are connected to a spatial pyramid pooling\nlayer, FSPPF. Finally, the three scales of features are fed into the detection layers, using an\nOBB detection head to predict the defects with rotated bounding boxes.\n\nSensors 2024 ,24, 6717 9 of 15\nCBS \nFSPPF C2fCBS \nCBS \nC2f\nCBS \nC2f\nCBS \nC2fAMFF \nAMFF CBS \nC2fCBS \nCBS \nC2f\nCBS \nC2f\nCBS \nC2fNeck OBB-Detect\nOBB-Detect\nOBB-Detect\nCBS Conv BN SiLU =\nAMFF ConcatCBAM \nFSPPF Concat CBAM SPPF =\n=P1 \nP2 \nP3 \nP4 \nP5 \nFigure 8. The architecture of the proposed ADMF-Net.\nThe AMFF module consists of a Concat module and a convolutional block attention\nmodule (CBAM). The feature fusion process involves three steps.\nFirst, the Concat operation is applied to the feature maps of the two modalities to\ncreate the initial fused feature.\nF=\u0010\n[Fforward;Fbackward]\u0011\n(1)\nSecond, a channel attention module (CAM) is used to adjust the channel weights\nbetween and within the two modalities. The input feature map F∈R(C×H×W)is trans-\nformed into C×1×1using maximum pooling and average pooling, respectively. These\ntwo feature vectors are then fed into a shared MLP for learning the attention weights of\nchannels. The results are combined using the addition operation and normalized by the\nsigmoid function. Multiplying the final weight coefficient Mc∈R(C×1×1)with the original\nfeature map gives the new feature map F′.\nMc(F) =σ(MLP(AvgPool (F)) + MLP(MaxPool (F)))\n=σ\u0010\nW1\u0010\nW0\u0010\nFc\navg\u0011\u0011\n+W1(W0(Fc\nmax))\u0011 (2)\nF′=Mc(F)⊗F (3)\nThird, the fused feature is fed into the spatial attention module (SAM) to enhance the\nattention on the important defect areas and to reduce the weight of background areas. In this\nprocess, maximum and average pooling are applied to each channel to obtain two feature\nmaps of 1×H×W, then concatenated by channel and sent into a 7×7convolution layer.\nAfter use of the sigmoid activation function, the spatial attention map Ms∈R(1×H×W)is\nobtained. Lastly, the Msand F′are multiplied to obtain the final fused feature map F′′.\nMs(F) =σ\u0010\nf7×7([AvgPool (F); MaxPool (F)])\u0011\n=σ\u0010\nf7×7\u0010\n[Fs\navg;Fs\nmax]\u0011\u0011 (4)\n\nSensors 2024 ,24, 6717 10 of 15\nF′′=Ms(F′)⊗F′(5)\nThe FSPPF module first concatenates the outputs of the P5 layers from the two feature\nextraction backbones, and then passes it into the SPPF (Spatial Pyramid Pooling-Fast)\nmodule. Then, similarly, the attention mechanism is applied to refine and produce the final\nfeature map. The output of FSPPF, along with the outputs from the P3 and P5 layers’ AMFF\nmodules, serves as the input to the neck layer.\n4. Experiments\n4.1. Evaluation Metrics\nWe evaluate the model’s detection performance using the mean average precision\n(mAP). Precision is the ratio of true positives (TP) to the sum of true positives and false\npositives (FP), indicating the accuracy of positive predictions. Recall is the ratio of true\npositives to the sum of true positives and false negatives (FN), reflecting how well the\nmodel detects all relevant instances. Average precision (AP) measures the area under the\nprecision-recall curve, denoted as p(r), which describes the relationship between precision\nand recall as the confidence threshold changes. The final mAP is obtained by averaging\nthe AP values across all classes. In this study, we use both the conventional mAP50, which\nevaluates at an intersection over union (IoU) threshold of 0.5, and the stricter mAP50-95,\nwhich averages over multiple IoU thresholds from 0.5 to 0.95 in steps of 0.05.\nPrecision =TP\nTP+FP\nRecall =TP\nTP+FN\nAP=Z1\n0p(r)dr\nmAP =1\nN∑APi(6)\n4.2. Implementation Details\nThe training environment for the experiments was configured with Python version\n3.10.14 and PyTorch version 2.2.2, with CUDA version 11.8 for GPU acceleration. The\nUltralytics version 8.2.2 was employed to support model implementation. The hardware\nsetup included an AMD Ryzen ™7 7800X3D CPU with 8 cores and 16 threads, 48GB of\nmemory, and a NVIDIA ®GeForce RTX ™4080 GPU with 16GB of VRAM. During the\ntraining phase, we adopted the hyperparameter settings shown in Table 2.\nTable 2. Hyperparametric configuration.\nParameter Name Parameter Value\nEpoch 300\nBatch size 16\nImage size 640 ×640\nData augmentation Mosaic-4\nOptimizer SGD\nLearning rate 0.01\nMomentum 0.937\nWeight decay 0.0005\n4.3. Results and Discussion\n4.3.1. Baseline and Dual-Modal Validity Discussion\nFirst, we used YOLOv8m as the baseline model, training on the single-modal datasets\nto test the baseline performance. The results show that using only forward lighting images\nachieved a mAP50 of 92.8%, while using only backward lighting images achieved a mAP50\n\nSensors 2024 ,24, 6717 11 of 15\nof 79% (see Table 3). However, as seen in the detailed results for each defect type, backward\nlighting significantly outperforms forward lighting in detecting contusions and spot defects.\nIn contrast, its AP for scratches is only 23%, which considerably lowers its overall mAP\n(see Table 4).\nTable 3. Experimental results overview.\nIndex Dataset Name Model mAP50 mAP50-95\n1 forward_only YOLOv8m 0.928 0.641\n2 backward_only YOLOv8m 0.790 0.538\n3 ag_composite_obb RGB Channel Fusion (ours)10.979 0.691\n4 ag_dual_obb ADMF-Net (ours) 0.984 0.669\n1Data-level fusion, using YOLOv8m as the back end. Note: In all experiments, the model scale was set to m\nif optional. Bold values indicate the best performance for each metric.\nTable 4. Detailed AP results for the four types of defect.\nIndex Contusion Scratches Crack Spot\n1 0.908/0.649 0.900/0.517 0.995/ 0.743 0.908/0.652\n2 0.951/0.679 0.233/0.118 0.995/0.685 0.980/0.669\n3 0.980/ 0.716 0.957 /0.556 0.995/0.737 0.984/0.754\n4 0.995 /0.686 0.954/0.520 0.995/0.714 0.993 /0.756\nNote: For each data item, the values from left to right represent AP50 and AP50-95. Bold values indicate the best\nperformance for each metric.\nNext, we trained the model on the ag_composite_obb dataset, which is created using\nthe RGB Channel Fusion method. The results show that our simple channel fusion method\nachieved impressive improvements. The overall mAP reached 97.9%, which is close to\nthe natural error margin of manual annotations. Furthermore, the AP for each defect type\nsurpassed the highest levels achieved using a single lighting method, with only a slight\ndecrease in AP50-95 for the crack defect category.\nAlthough the overall performance of forward lighting is barely satisfactory, it cannot\nfully replace the detection capabilities of backward lighting. Combining both methods for\ndefect detection is obviously more effective and holds potential for further improvement.\n4.3.2. ADMF-Net Experiment\nOur fusion model achieved excellent results on the ag_dual_obb dataset. The detection\nrates for the contusion, crack, and spot categories all exceeded 99%. The detection rate\nfor scratches was 95.4%, which is consistent with the results of the RGB Channel Fusion\nmethod. Overall, compared to the RGB Channel Fusion method, the total mAP50 improved\nfrom 97.9% to 98.4%. As shown in Figure 9, our fusion model demonstrated a better\nprecision-confidence curve and precision-recall curve, although there was a slight decrease\nin mAP50-95.\n\nSensors 2024 ,24, 6717 12 of 15\nFigure 9. A comparison of the two proposed methods. First row: RGB Channel Fusion. Second\nrow: ADMF-Net.\n4.3.3. Interpretability Experiment\nA confusion matrix is a table used to evaluate classification models by comparing\npredicted and actual values. As shown in Figure 10, the model trained with forward\nlighting tends to misclassify lighter scratch defects as contusions and struggles to detect\nspot defects. For backward lighting, a significant number of scratch defects are incorrectly\nclassified as background, which is the primary factor limiting its overall performance.\nThese issues were significantly improved with the adoption of the fusion detection method.\n4.3.4. Comparative Analysis of Results\nWe conducted several comparative experiments to verify the effectiveness of our\nproposed method. See Table 5. For the selection of the third channel in the RGB Channel\nFusion method, we experimented with using the difference image of the two modalities\ninstead of the mean value. This resulted in a slight decrease in mAP .\nTable 5. Additional experimental results.\nIndex Dataset Name Model mAP50 mAP50-95\n5 ag_composite_diff_obb RGB Channel Fusion10.977 0.684\n6 ag_dual_obb ADMF-Net20.983 0.639\n7 ag_dual_obb ADMF-Net30.976 0.668\n8 ag_dual_rect ICAFusion [28]40.963 0.640\n1Using differential image as the third channel.2Without attention mechanism, using only Concat operation.\n3Using EMA attention [29].4Using transformer cross-attention.\nTo validate the effectiveness of our fusion model, we conducted ablation experiments.\nThe results showed that after removing the attention mechanism, mAP50 remained rela-\ntively stable, but mAP50-95 significantly dropped by 5.2%.\nAdditionally, we conducted further experiments to evaluate the impact of different\nattention mechanisms on detection performance, such as EMA attention and a state-of-the-\nart model based on transformer cross-attention. The experimental results indicate that these\nmore complex attention mechanisms could have negative effects. Our proposed model\nproves to be more effective for defect detection in aircraft glass canopies.\n\nSensors 2024 ,24, 6717 13 of 15\n(a) (b)\n(c) (d)\n(a) (b)\n(c) (d)\nFigure 10. Normalized confusion matrix of single lighting methods and our fusion methods.\n(a) Forward lighting. ( b) Backward lighting. ( c) RGB Channel Fusion. ( d) ADMF-Net.\n5. Conclusions\nTo address the issue of surface defect detection on aircraft glass canopies, where\nrelying on a single-modal lighting condition often leads to missed or false detections, we\nhave proposed a dual-modal illumination system for transparent material defect detection.\nThis system utilizes both forward lighting and backward lighting to capture defect images,\nfacilitating a more comprehensive and accurate defect detection process.\nIn addition, based on real aircraft glass canopy samples, we have collected the first\ndataset for defect detection in aircraft glass canopies. This dataset includes 1752 defect\npoints captured under both forward and backward lighting conditions, totaling\n4784 defect samples. These samples are categorized into four classes and annotated using\nthe OBB format.\nFurthermore, we proposed a data-level fusion method named RGB Channel Fusion,\nand a feature-level fusion model, the attention-based dual-branch modal fusion network\n(ADMF-Net). The model uses two feature extraction backbones to independently extract\nmulti-scale features from the forward and backward lighting modalities. Next, through the\nattention-based multi-modal feature fusion block (AMFF) and improved SPPF module, the\n\nSensors 2024 ,24, 6717 14 of 15\nnetwork adaptively integrates the multi-scale features from the two modalities. Finally, the\nfeatures are fed into the detection layers, where an OBB detection head is used to provide\nthe rotated bounding boxes of defects.\nExperiments demonstrate the effectiveness of our proposed system and dual-modal\ndetect methods. Compared to the baseline model YOLOv8m, which individually detects\nforward lighting and backward lighting images, our ADMF-Net model improved the de-\ntection accuracy by 5.6% and 19.4%, reaching a mAP of 98.4%. The proposed RGB Channel\nFusion method also achieved a strong mAP of 97.9%. The comparative experiments also\nconfirmed the superiority of our model.\nOur research effectively addresses the issue of defect detection in aircraft glass canopies.\nMoreover, it provides a new methodological support for defect detection in other transparent\nmaterials. Exploring specific multi-modal detection networks remains a worthwhile issue.\nAuthor Contributions: Conceptualization: Y.Y. and Z.L.; methodology: Z.L. and Y.Y.; data curation:\nR.W., Z.L. and Q.L.; original draft: Z.L.; review and editing: R.W. and Q.L.; software: Z.L. and Q.L.;\nvalidation: Q.L. and R.W. All authors have read and agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The data that support the findings of this study are publicly available\nfor research purposes at the following GitHub repository: https://github.com/core128/AGDD\n(accessed on 14 October 2024).\nConflicts of Interest: The authors declare no conflicts of interest.\nReferences\n1. Towsyfyan, H.; Biguri, A.; Boardman, R.; Blumensath, T. Successes and challenges in non-destructive testing of aircraft composite\nstructures. Chin. J. Aeronaut. 2020 ,33, 771–791. [CrossRef]\n2. Keršien ˙e, N.; Raslaviˇ cius, L.; Keršys, A.; Kažys, R.; Žukauskas, E. Energo-mechanical evaluation of damage growth and fracture\ninitiation in aviation composite structures. Polym.-Plast. Technol. Eng. 2016 ,55, 1137–1144. [CrossRef]\n3. Zimmermann, N.; Wang, P .H. A review of failure modes and fracture analysis of aircraft composite materials. Eng. Fail. Anal.\n2020 ,115, 104692. [CrossRef]\n4. Kopardekar, P .; Mital, A.; Anand, S. Manual, Hybrid and Automated Inspection Literature and Current Research. Integr. Manuf.\nSyst. 1993 ,4, 18–29. [CrossRef]\n5. Ebayyeh, A.A.R.M.; Mousavi, A. A Review and Analysis of Automatic Optical Inspection and Quality Monitoring Methods in\nElectronics Industry. IEEE Access 2020 ,8, 183192–183271. [CrossRef]\n6. Costa, R.D.; Maia, J.M.; Assef, A.A.; Pichorim, S.F.; Gewehr, P .M.; Costa, E.T. Defect Detection in Aluminum Bars Using Impedance\nand Ultrasonic Attenuation. IEEE Sens. J. 2020 ,20, 7400–7413. [CrossRef]\n7. Yao, H.B.; Ping, J.; Ma, G.D.; Li, L.W.; Gu, J.N. The system research on automatic defect detection of glasses. Appl. Mech. Mater.\n2013 ,437, 362–365. [CrossRef]\n8. Zhou, X.; Wang, Y.; Zhu, Q.; Mao, J.; Xiao, C.; Lu, X.; Zhang, H. A Surface Defect Detection Framework for Glass Bottle Bottom\nUsing Visual Attention Model and Wavelet Transform. IEEE Trans. Ind. Inform. 2020 ,16, 2189–2201. [CrossRef]\n9. Jian, C.; Gao, J.; Ao, Y. Automatic surface defect detection for mobile phone screen glass based on machine vision. Appl. Soft\nComput. J. 2017 ,52, 348–358. [CrossRef]\n10. Nand, G.K.; Noopur; Neogi, N. Defect detection of steel surface using entropy segmentation. In Proceedings of the 11th IEEE\nIndia Conference: Emerging Trends and Innovation in Technology, INDICON 2014, Pune, India, 11–13 December 2014. [CrossRef]\n11. Czimmermann, T.; Ciuti, G.; Milazzo, M.; Chiurazzi, M.; Roccella, S.; Oddo, C.M.; Dario, P . Visual-based defect detection and\nclassification approaches for industrial applications—A SURVEY. Sensors 2020 ,20, 1459. [CrossRef]\n12. Park, J.; Riaz, H.; Kim, H.; Kim, J. Advanced cover glass defect detection and classification based on multi-DNN model. Manuf.\nLett. 2020 ,23, 53–61. [CrossRef]\n13. Cheng, L.; Tong, Z.; Xie, S.; Kersemans, M. IRT-GAN: A generative adversarial network with a multi-headed fusion strategy for\nautomated defect detection in composites using infrared thermography. Compos. Struct. 2022 ,290, 115543. [CrossRef]\n14. Zhu, Y.; Ding, R.; Huang, W.; Wei, P .; Yang, G.; Wang, Y. HMFCA-Net: Hierarchical multi-frequency based Channel attention net\nfor mobile phone surface defect detection. Pattern Recognit. Lett. 2022 ,153, 118–125. [CrossRef]\n15. Pan, Z.; Yang, J.; Wang, X.; Wang, F.; Azim, I.; Wang, C. Image-based surface scratch detection on architectural glass panels using\ndeep learning approach. Constr. Build. Mater. 2021 ,282, 122717. [CrossRef]\n\nSensors 2024 ,24, 6717 15 of 15\n16. Hussain, M. YOLO-v1 to YOLO-v8, the Rise of YOLO and Its Complementary Nature toward Digital Manufacturing and\nIndustrial Defect Detection. Machines 2023 ,11, 677. [CrossRef]\n17. Zhang, M.; Yin, L. Solar Cell Surface Defect Detection Based on Improved YOLO v5. IEEE Access 2022 ,10, 80804–80815.\n[CrossRef]\n18. Xie, Y.; Hu, W.; Xie, S.; He, L. Surface Defect Detection Algorithm Based on Feature-Enhanced YOLO. Cogn. Comput. 2023 ,15,\n565–579. [CrossRef]\n19. Zhang, Y.; Sidibé, D.; Morel, O.; Mériaudeau, F. Deep multimodal fusion for semantic image segmentation: A survey. Image Vis.\nComput. 2021 ,105, 104042. [CrossRef]\n20. Chen, H.; Li, Y.; Su, D. Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient\nobject detection. Pattern Recognit. 2019 ,86, 376–385. [CrossRef]\n21. Wang, Q.; Chi, Y.; Shen, T.; Song, J.; Zhang, Z.; Zhu, Y. Improving RGB-Infrared Object Detection by Reducing Cross-Modality\nRedundancy. Remote Sens. 2022 ,14, 2020. [CrossRef]\n22. Feng, D.; Haase-Schutz, C.; Rosenbaum, L.; Hertlein, H.; Glaser, C.; Timm, F.; Wiesbeck, W.; Dietmayer, K. Deep Multi-Modal\nObject Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges. IEEE Trans. Intell.\nTransp. Syst. 2021 ,22, 1341–1360. [CrossRef]\n23. Li, M.; Yao, N.; Liu, S.; Li, S.; Zhao, Y.; Kong, S.G. Multisensor Image Fusion for Automated Detection of Defects in Printed\nCircuit Boards. IEEE Sens. J. 2021 ,21, 23390–23399. [CrossRef]\n24. Zhou, T.; Ruan, S.; Canu, S. A review: Deep learning for medical image segmentation using multi-modality fusion. Array 2019 ,\n3–4, 100004. [CrossRef]\n25. Ren, Z.; Fang, F.; Yan, N.; Wu, Y. State of the Art in Defect Detection Based on Machine Vision. Int. J. Precis. Eng. and Manuf.-Green\nTech. 2022 ,9, 661–691. [CrossRef]\n26. Ming, W.; Shen, F.; Li, X.; Zhang, Z.; Du, J.; Chen, Z.; Cao, Y. A comprehensive review of defect detection in 3C glass components.\nMeasurement 2020 ,158, 107722. [CrossRef]\n27. Wang, W. Advanced Auto Labeling Solution with Added Features. 2023. Available online: https://github.com/CVHub520/X-\nAnyLabeling (accessed on 14 October 2024).\n28. Shen, J.; Chen, Y.; Liu, Y.; Zuo, X.; Fan, H.; Yang, W. ICAFusion: Iterative cross-attention guided feature fusion for multispectral\nobject detection. Pattern Recognit. 2024 ,145, 109913. [CrossRef]\n29. Ouyang, D.; He, S.; Zhang, G.; Luo, M.; Guo, H.; Zhan, J.; Huang, Z. Efficient Multi-Scale Attention Module with Cross-Spatial\nLearning. In Proceedings of the ICASSP 2023—2023 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), Rhodes Island, Greece, 4–10 June 2023; pp. 1–5. [CrossRef]\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nComputer Vision and Image Understanding 202 (2021) 103085\nContents lists available at ScienceDirect\nComputer Vision and Image Understanding\njournal homepage: www.elsevier.com/locate/cviu\nMulti-modal semantic image segmentation\nAkila Pemasiri∗, Kien Nguyen, Sridha Sridharan, Clinton Fookes\nImage and Video Research Lab, Queensland University of Technology, 2 George Street, GPO Box 2434, Brisbane, QLD 4001, Australia\nA R T I C L E I N F O\nCommunicated by Smith John\nKeywords:\nSegmentation\nX-ray\nMask R-CNN\nNeural networksA B S T R A C T\nWe propose a modality invariant method to obtain high quality semantic object segmentation of human body\nparts, for four imaging modalities which consist of visible images, X-ray images, thermal images (heatmaps)\nand infrared radiation (IR) images. We first consider two modalities (i.e. visible and X-ray images) to develop\nan architecture suitable for multi-modal semantic segmentation. Due to the intrinsic difference between images\nfrom the two modalities, state-of-the-art approaches such as Mask R-CNN do not perform satisfactorily. Insights\nfrom analysing how the intermediate layers within Mask R-CNN work on both visible and X-ray modalities\nhave led us to propose a new and efficient network architecture which yields highly accurate semantic\nsegmentation results across both visible and X-ray domains. We design multi-task losses to train the network\nacross different modalities. By conducting multiple experiments across visible and X-ray images of the human\nupper extremity, we validate the proposed approach, which outperforms the traditional Mask R-CNN method\nthrough better exploiting the output features of CNNs. Based on the insights gained on these images from visible\nand X-ray domains, we extend the proposed multi-modal semantic segmentation method to two additional\nmodalities; (viz. heatmap and IR images). Experiments conducted on these two modalities, further confirm\nour architecture’s capacity to improve the segmentation by exploiting the complementary information in the\ndifferent modalities of the images. Our method can also be applied to include other modalities and can be\neffectively utilized for several tasks including medical image analysis tasks such as image registration and 3D\nreconstruction across modalities.\n1. Introduction\nSemantic segmentation underpins many computer vision applica-\ntions across several domains since region of interest identification\nuplifts the accuracy of the end results (Pemasiri et al., 2019b). Many\nof these applications utilize in addition to visible images, other imag-\ning modalities such as X-ray images and IR images. Algorithms for\ncomputer vision applications, including human action recognition and\npatient analysis (Pemasiri et al., 2019a; Ji et al., 2018; Khowaja and\nLee, 2019), 3D reconstruction (Elanattil et al., 2018; Kundu et al., 2014)\nand object detection (Salscheider, 2019) based on visible images use\nsemantic segmentation as a subroutine of the process. When consider-\ning the X-ray images, establishing semantic segmentation is vital for\nprocesses such as joint space width assessment, bone age assessment\nand preoperative planning (Bullock et al., 2019; Wu and Mahfouz,\n2016; Hržić et al., 2019). Thermal images (heatmaps) and infrared\n(IR) images are also employed in computer vision applications across\nmany domains including industrial inspection, medical image analy-\nsis and surveillance where these applications also require semantic\nsegmentation as part of the process (Duarte et al., 2014; Qiao et al.,\n2017).\n∗Corresponding author.\nE-mail address: akila.10@cse.mrt.ac.lk (A. Pemasiri).Recent advances in neuro-computing have motivated the applica-\ntion of neural networks for semantic segmentation tasks, resulting in\nrobust and impressive baseline frameworks including Fast R-CNN (Gir-\nshick, 2015), Faster R-CNN (Ren et al., 2015), and Mask R-CNN (He\net al., 2017). However, the existing work on semantic segmentation\nhas been aimed only on a single selected image modality (Bates et al.,\n2019; Hu et al., 2019; Li et al., 2018a; Wollmann et al., 2019; Bullock\net al., 2019).\nMulti-modal information processing is beneficial in many computer\nvision applications as the information available in different modalities\ncomplement each other (Pemasiri et al., 0000; Jian et al., 2017; Naka-\nmura et al., 2013). For an example, in medical image analysis, visible\nimages capture the external features of an organ (e.g., skin information)\nwhere as X-ray images capture the information about internal parts\n(e.g., bones and muscles) and the combined information is useful\nfor medical analysis. In addition to these modalities, thermal images\npresent temperature distribution of the surface which can capture\npathophysiologic information related to physical conditions (Duarte\net al., 2014; Sonkusare et al., 2019). However, the development of\nthese cross modality applications demand robust lower-level computer\nvision algorithms which can be applied on multi-modal images. With\nhttps://doi.org/10.1016/j.cviu.2020.103085\nReceived 5 November 2019; Received in revised form 24 June 2020; Accepted 25 August 2020\nAvailable online 5 September 2020\n1077-3142/ ©2020 Published by Elsevier Inc.\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nthis requirement the question to be addressed is how to combine the\ncomplementary information from multiple sources considering the in-\ntrinsic differences in their nature. In the case of semantic segmentation,\nthe open question is how to distill information from multiple imaging\nmodalities in a single and unified architecture.\nCompared with single-modality image processing, multi-modal im-\nage processing exhibits some inherent challenges, arising from poten-\ntial dissimilarity where the regions in two images can be differently\ntextured or one image may be textured while the other is homoge-\nneous (Torabi and Bilodeau, 2013). In addition, the level of information\npresented in each image modality is different. For instance, considering\nimages of a hand from visible, X-ray, heatmap and IR modalities, visible\nimages contain details of skin, nails, clothes and accessories, whereas X-\nray images contain details of bones and muscles, while heatmap images\ncontain the temperature distribution of the surface and IR images\ncontains the details of the degree of infrared wave penetration through\nthe objects in the image (Fig. 1).\nIn this paper, we present an architecture which uses a single model\nto establish pixel wise semantic object segmentation of human body\nparts (i.e. multi-class), regardless of the modality, (i.e. a visible or\nan X-ray image). In the context of this paper we use the term ‘‘Se-\nmantic segmentation’’ to associate the pixels in the image, which\nbelong to the object of interest (e.g., identifying components of human\nupper extremity with the class labels of ‘‘elbow’’, ‘‘forearm’’, ‘‘hand’’,\n‘‘humerus’’, ‘‘shoulder’’ and ‘‘wrist’’) with the correct class labels. Using\na single model for semantic segmentation of multiple modalities, not\nonly seamlessly incorporates all sub-tasks into one, but also reduces the\nnumber of parameters and improves the accuracy by enabling end-to-\nend training and inference. Training a single network on multi-modal\nimages will integrate complementary information that is presented in\neach modality, and thus will result in robust kernels which are capable\nof capturing information from all the modalities under consideration.\nIn developing our multi-modal semantic segmentation framework, we\nexploit the convolution neural networks’ capability of learning patterns\nat early stages (Li et al., 2018b; Long et al., 2014), including the pat-\nterns in different modalities. Our proposed approach has the advantage\nthat the models trained on modalities where the samples are abundant\n(e.g., visible images) can be used on other image modalities (e.g., X-ray)\nwhich are hard to acquire. In our approach, we utilize the capabilities\nof Mask R-CNN by exploiting the kernels which generate the most\ninformative features to achieve effective multi-modal and robust object\nsegmentation.\nWe also demonstrate the effectiveness of our model in exploit-\ning complementary information across other imaging modalities (i.e.\nheatmaps and IR images), for single-class image segmentation. The\nreason to use single-class semantic segmentation for this evaluation as\nopposed to multi-class, multi-modal segmentation which we demon-\nstrate with visible and X-ray images, is the limited availability of\ndatasets, which we will discuss in detail in Section ‘‘Experiments’’ of\nthis paper. Our proposed architectural innovation however, is generic\nand can be used to perform semantic multi-class image segmentation\nwith any combination of multi-modal data, using a single deep learning\nmodel.\nMedical images of human upper extremity are used in detecting\nmany pathological conditions including Rheumatoid arthritis, upper\nlimb fractures (e.g., proximal humeral fracture, humeral shaft fracture,\nsupracondylar fracture), dislocations (e.g., shoulder dislocation, radial\nhead dislocation and carpal dislocation) and deformities (e.g., radial\nclub hand, ulnar dysplasia and radiohumeral synostosis). Due to these\nwide range of applications of human upper extremity for which we\nhave large amounts of X-ray, heatmap and IR images available, we have\nchosen this body part to demonstrate the efficacy of our method; the\nmethod we propose however, can easily be applied to the other body\nparts as well.\nFor the X-ray images, we use the musculoskeletal radiograph\n(MURA) dataset (Rajpurkar et al., 2017). For visible images, we have\nFig. 1. Sample images from visible (first row), X-ray (second row), heatmap (third\nrow) and IR (fourth row) images, illustrate the difference in visual texture and level\nof information.\ncollected our own dataset, which contains the corresponding visible\nhuman body parts (i.e. elbow, forearm, hand, humerus, shoulder and\nwrist). To ensure that our database is representative of a typical health-\ncare environment, we have collected visible images under challenging\nconditions including varying lighting conditions, different resolutions\nand different backgrounds. This database will be made publicly avail-\nable to enable researchers to replicate our results in this paper. In\naddition to releasing the database, code for the robust object segmenta-\ntion method introduced in this paper, as well as the groundtruth region\nannotation for the MURA X-ray image dataset for the upper extremity\nof the human body which we have manually annotated will also be\navailable for researchers to follow-up on our work. In addition using\nthe HandNet dataset (Wetzler et al., 2015), we obtained the heatmap\nimages and IR images of human hands. As this dataset does not contain\nother components of the upper extremity (i.e. elbow, forearm, humerus,\nshoulder and wrist), we use the HandNet dataset (Wetzler et al., 2015)\nto validate our method for other modalities and to demonstrate its\napplicability in other modalities.\nThe major contributions of our paper are summarized below:\n1. We have developed a CNN based modality invariant semantic\nimage segmentation framework, where the most informative\nlayers of CNNs are effectively utilized as the features for class\nprediction, region identification and mask identification across\nmulti-modal images.\n2. Our multimodal semantic segmentation implementation has out-\nperformed the widely used segmentation framework ‘‘Mask R-\nCNN’’ for the visible, X-ray, thermal and IR images, and this\nmethod can be extended to include other image modalities such\nas CT scan and MRI.\n3. Our code and the dataset we used in this work are released so\nthat the researchers are able to replicate our results and further\nadvance the area of multi-modal semantic image segmentation\nof medical images.1\n1https://github.com/PemasiriAkila/Multi-modal-Semantic-Image-\nSegmentation.\n2\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nFig. 2. Results of traditional segmentation methods and semantic segmentation for an\ninput X-ray image.\nThe remainder of the paper is organized as follows. In Section 2\nwe analyse the recent literature on object detection and segmentation.\nSection 3 describes the proposed system and its subsections elaborate\non each component of the system. It should be noted that in the\nmethod development we used only the visible and X-ray images, which\nwas then validated for other imaging modalities. In Section 4, we\npresent our experimental results for multi-class multi-modal semantic\nsegmentation using visible and X-ray images as well single-class multi-\nmodal semantic segmentation using visible, X-ray, heatmap and IR\nimages. In Section 5, we conclude the paper with a discussion of the\nresults on the presented method.\n2. Related work\nObject detection and segmentation literature has progressed\nthrough the decades. Early stages of image segmentation include ap-\nproaches that use the low level image features such as brightness and\ntexture gradient (Martin et al., 2003). However, when considering the\nobject segmentation in contrast to the image segmentation, the former\nrequires more details on the structure and the shape of the object\ncategory. Many of the recent approaches for object segmentation are\npreceded by object detection, where the object boundary identification\nis used as a prior for the detection task (Elgammal et al., 2000; Gonfaus\net al., 2010; Mittal and Paragios, 2004). Therefore efforts on improving\nthe detection process have a paramount effect on the improvement of\nthe segmentation process.\nIt is notable that neural network based approaches have produced\nsignificant improvement for object detection and segmentation tasks\n(Rowley et al., 1998; Sermanet et al., 2013; Vaillant et al., 1994).\nHowever, early approaches based on CNNs dealt with specific object\ncategories in constrained applications such as faces (Rowley et al.,\n1998; Vaillant et al., 1994) and pedestrians (Sermanet et al., 2013). In\nthe more recent literature, rich feature hierarchies for accurate object\ndetection and semantic segmentation (Girshick et al., 2014), have been\nintroduced such as the concept of Regions with CNN features coined as\n‘‘R-CNN’’ with the aim to accomplish higher accuracy values for object\ndetection. They have first extracted the region proposals for the whole\nimage, following which CNN based features are extracted on each of\nthese regions which are then subjected to SVM based classification.\nOne major drawback with R-CNN is that to extract features of each\ncandidate region proposal, it requires a forward pass of the CNN. To\novercome this a number of approaches known as ‘‘Fast R-CNN’’ (Gir-\nshick, 2015) has been suggested, where a Region of Interest (RoI)\npooling layer is used to extract feature vectors for object proposals.\nFeature vectors use the feature map generated by the forward pass\nof the input image over the CNN. To alleviate the bottleneck caused\nby having to obtain the candidate region proposals using a separate\nselective search process in Fast R-CNN, ‘‘Faster R-CNN’’ (Ren et al.,\n2015) has been introduced. In Faster R-CNN, a single CNN is used to\nestimate the candidate proposals as well as to extract features for the\nclassification.The main objective in developing the R-CNN, Fast R-CNN and Faster\nR-CNN architectures are to perform effective object detection and\nclassification. Mask R-CNN (He et al., 2017) has extended the Faster\nR-CNN by adding another branch for object mask detection resulting\nin state-of-art accuracy for object segmentation. However, all these\napproaches suggested in the literature for detection and segmentation\nhave mainly focused on visible images.\nWhen considering the other imaging modalities such as X-ray im-\nages, the existing methods for segmentation are limited to those which\nhave targeted on identifying the components of the musculoskeletal sys-\ntem (e.g., identifying bone regions of a given X-ray image or extracting\nthe whole body part that has been captured in the X-ray image) (Ali\net al., 2006; Feng, 2006; Isard and Blake, 1998; Manos et al., 1994;\nStolojescu-CriŞan and Holban, 2013) (See for example Fig. 2(b), where\ndifferent colours indicate the different regions resulting from segmen-\ntation). They do not segment all the components of a particular body\npart (e.g., identifying all the components of the wrist) (Fig. 2), which\nlimits the semantic nature of the segmentation results. The existing\nIR and heatmap segmentation methods are based on hand engineered\nfeatures (Duarte et al., 2014; Qiao et al., 2017) such as pixel intensities\nwhich make the algorithms vulnerable to noise while generating less\nrobust results. In addition, all these methods are targeted on modality\nspecific features which limit their ability to be used with other imaging\nmodalities.\nIn contrast to these previous approaches, we develop an architecture\nto detect the objects and carry out the segmentation while being\ninvariant to the modality of the image, whether they are visible, X-ray,\nthermal or IR. Our method enables a deeper semantic segmentation\nwhich can provide better localization of regions of interest, as illus-\ntrated in Fig. 2c for the X-ray image. To the best of our knowledge, this\nis the first work to consider model invariant semantic segmentation for\nmulti-modal images.\n3. Methodology\nIn this section we provide an overview of Mask R-CNN and then\nelaborate the insight of deriving our method based on the observations\non visible and X-ray image segmentation.\n3.1. Mask R-CNN\nAs described in Section 2, state-of-the-art object detection and seg-\nmentation are driven by CNNs and Mask R-CNN has depicted robust\nresults for both these tasks. In this section, we will elaborate strengths\nand weaknesses of Mask R-CNN and how we can exploit the Mask R-\nCNN’s strengths to make it invariant for object segmentation for visible\nand X-ray images.\nMask R-CNN has (He et al., 2017) been extended from Faster R-\nCNN (Ren et al., 2015), where the objective of the Faster R-CNN is\nto generate accurate object bounding boxes, Mask R-CNN provides a\nrefined segmentation of the objects. In addition to the added branch for\nsegmentation, Mask R-CNN introduces the RoIAlign layer which aligns\nthe regions of the feature map along with the regions of the original\nimage.\nThe foundation of our approach is based on the observations that\nwe encountered while attempting to use Mask R-CNN for object seg-\nmentation. When the original Mask R-CNN was trained on a dataset\nwhich consisted of human body parts in the visible image modality, it\nwas identified that though it can generalize well for visible images, it\ndoes not generate masks of adequate quality for X-ray images (Fig. 3).\nIn most of the cases, it was not detecting all the components of the\nX-ray object. Conversely, when a model trained on X-ray images was\ntested on a set of visible images, one major observation that was made\nis that it was erroneously detecting other objects in the environment as\nhuman body parts (Fig. 4).\n3\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nFig. 3. Results that were obtained from the model trained on visible images illustrating\nits poor performance in mask identification in X-ray images.\nFig. 4. Results that were obtained from the model trained on X-ray images illustrating\nits poor performance in mask identification in visible images.\nFig. 5. Results on images which were tested on the model trained on multi-modal\nimage dataset.\nFig. 6. Visualization of layer outputs of Mask R-CNN for a visible image and for an\nX-ray image, the first column depicts the input images. The 2nd,3rd,4th and 5th\ncolumns show the layer outputs where informative features have been generated for\nboth X-ray and visible images, 6𝑡ℎcolumn shows an example for a layer output where\nfor the X-ray image the generated feature is informative but for the visible image it\ndoes not, and the 7th column shows an example for the opposite scenario.\nAn intuitively obvious solution to overcome the above problem is\nto use a dataset, which contains the images from both the modalities.\nHowever, though this approach results in the increased quality of the\noutput compared to the models that were trained on a single modality\n(Figs. 3 and 4), the newly generated masks still suffer from the prob-\nlems of not recognizing the complete object or mistakenly identifying\nthe parts in the image that do not belong to the object (Fig. 5).\nOur next step in finding the solution to this problem was to gain a\ngreater insight into the architecture of Mask R-CNN. When the outputof the layers of Mask R-CNN were inspected, a simple yet crucial\nobservation was made. That is, only some of the kernels in Mask R-\nCNN generates discerning features for both the image modalities. A\nsubset of feature maps, extracted from some random kernels of the\nMask R-CNN is depicted in Fig. 6. The first column of Fig. 6 shows\nthe input image, where the first row of it is the input X-ray image\nand the second row shows the input visible image. The rest of the\ncolumns depict the feature output from some random kernels. It can\nbe seen that some kernels have generated features for both modalities,\nwhereas other kernels generated sensible features for only one of the\ntwo modalities. The kernels that were generating sensible features for\nboth image modalities were consistent for all the body parts under\nour consideration. Furthermore, to validate our observation, Mask R-\nCNN was trained 3 times separately, with the pretrained weights on the\nCOCO dataset (Lin et al., 2014) for 100 epochs, we observed that the\nlayers which generated features for the modalities remained the same.\n3.2. Modality invariant segmentation\nMotivated by the observations described in Section 3.1, we intro-\nduce a new architecture, which generates robust results for visible\nimages as well as X-ray images.\nAs described above, for a set of initial images we identified the\nkernels in the network that created discerning features for both visible\nand X-ray images. Based on these kernel numbers, our architecture in-\ncorporates a new feature concatenation layer where the resized feature\noutputs of the layers that generate features for both the modalities from\nMask R-CNN are concatenated. The input to the our model (Fig. 7)\nis images (either visible or X-ray) and the input images are passed\nthrough the network, where the features are generated at every level.\nThen using bilinear interpolation, the features are resized to the size\nof 224 ×224 which is the input image size, and the feature outputs\nthat are related to the kernels which generated discerning features are\nconcatenated together before feeding to the RoI pooling layer.\nDepending on the backbone architecture used in the Mask R-CNN,\nthe dimension of the feature concatenation layer vary. The architecture\nthat is depicted in Fig. 7 corresponds to the ResNet101-C4 (He et al.,\n2016) architecture. Following the previous work on Faster R-CNN we\nemployed ResNet architecture and ResNeXt architecture (Xie et al.,\n2017) with depth of 50 or 101 layers. Following the commonly adopted\nnotation (Dai et al., 2016; He et al., 2016; Huang et al., 2017b;\nShrivastava et al., 2016) we denote the ResNet architecture with 50\nlayers where the features are extracted from 4th stage convolution layer\nas ‘‘ResNet-50-C4’’. In addition, we analyse Feature Pyramid Network\n(FPN) (Lin et al., 2017) in combination with ResNet architecture.\nWhen the features generated by Resnet-50-C4, Resnet-101-C4,\nResNet-50-FPN, ResNet-101-FPN, ResNeXt-50-C4, ResNeXt-101-C4,\nResNeXt-50-FPN and ResNeXt-101-FPN were investigated it was iden-\ntified that there are 2100, 3435, 2421, 3606, 2241, 3527, 2627, and\n3677 feature outputs that create features for both visible and X-ray\nimages. These layers were automatically selected by thresholding the\npixel values of output features. In the current implementation, to\nselect the kernels which generate the discerning features, first we crop\nthe images based on the bounding boxes and then pass through the\nnetwork. From the features generated by each kernel we eliminate\nthe kernels which have generated features with either 0s or 1s. The\nconcatenation layer is followed by a convolutional layer, which is then\nsubjected to RoI align layer, which then get segregated into mask\nprediction branch and to class and bounding box prediction branch.\nWe use a multi-task loss function to train the network,\n𝐿=𝐿𝑐𝑙𝑎𝑠𝑠+𝜆𝐿𝑏𝑜𝑥+𝜇𝐿𝑚𝑎𝑠𝑘, (1)\nwhere𝐿𝑐𝑙𝑎𝑠𝑠denotes the classification loss, 𝐿𝑏𝑜𝑥denotes the loss de-\nfined on bounding box identification and 𝐿𝑚𝑎𝑠𝑘denotes the loss defined\non mask identification. The combined loss is calculated as in Eq. (1),\nwhere𝜆and𝜇are the hyper-parameters that are used for maintaining\n4\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nFig. 7. Our architecture, where the selected layer outputs are resized and fused to generate the new layer, which is then subjected for RoI extraction, mask extraction and class\nlabel extraction.\nthe balance between the three losses that are aggregating for the final\nloss value.\nThe𝐿𝑐𝑙𝑎𝑠𝑠 is computed based on the softmax over 7 outputs of\nthe fully connected layer in the classification branch. The last layer\nof the classification branch contains 7 outputs as the dataset we have\nused 6 classes and the default background layer is also taken into\nconsideration. For each given Region of Interest (RoI), a probability\ndistribution is associated using a softmax over the 7 outputs of the fully\nconnected layer. For a given image, for a given Region of Interest (RoI)\nwith groundtruth class label 𝑢, the𝐿𝑐𝑙𝑎𝑠𝑠is the negative log softmax\nvalue for the groundtruth class label Eq. (2),\n𝐿𝑐𝑙𝑎𝑠𝑠= −𝑙𝑜𝑔(𝑝𝑢). (2)\nA bounding box is defined using four parameters 𝑡𝑥,𝑡𝑦,𝑡𝑤and𝑡ℎ\nwhich denote the 𝑥coordinate of the right corner of the bounding\nbox,𝑦coordinate of the right corner of the bounding box, width of\nthe bounding box and height of the bounding box respectively. Let\n𝑡𝑢=(\n𝑡𝑢\n𝑥,𝑡𝑢\n𝑦,𝑡𝑢\n𝑤,𝑡ℎ\n𝑥)\nbe the bounding box parameter tuple of instance 𝑢\nof groundtruth annotation, and 𝑡𝑣be the predicted tuple the 𝐿𝑏𝑜𝑥is\ndefined as,\n𝐿𝑏𝑜𝑥=∑\n𝑖𝜖𝑥,𝑦,𝑤,ℎ𝑠𝑚𝑜𝑜𝑡ℎ𝐿1(𝑡𝑣\n𝑖−𝑡𝑢\n𝑖). (3)\nIn Eq. (3), the term 𝑠𝑚𝑜𝑜𝑡ℎ𝐿1is defined as,\n𝑠𝑚𝑜𝑜𝑡ℎ𝐿1(𝑟)={0.5𝑞2𝑖𝑓|𝑞|<1\n|𝑞|− 0.5 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒.(4)\nIt should be noted that the 𝐿𝑏𝑜𝑥is not calculated for the 𝑏𝑎𝑐𝑘𝑔𝑟𝑜𝑢𝑛𝑑\nclass.\nIn this work, the mask is defined as a binary array where the\nforeground pixels of the objects are marked. The mask branch output\nconsists of𝐾masks in𝑚×𝑚resolutions, where for each of the entries in\n𝑚×𝑚array a per-pixel sigmoid is applied and when an object belongs\nto the class 𝑘, the𝐿𝑚𝑎𝑠𝑘is defined as in Eq. (5),\n𝐿𝑚𝑎𝑠𝑘= −1\n𝑚2∑\n1≤𝑖,𝑗≤𝑚[\n𝑦𝑖𝑗𝑙𝑜𝑔(̂ 𝑦𝑘\n𝑖𝑗) +(1 −𝑦𝑖𝑗)𝑙𝑜𝑔(\n1 −̂ 𝑦𝑘\n𝑖𝑗)]\n. (5)\nIn Eq. (5),𝑦𝑖𝑗is the groundtruth label of the cell (𝑖,𝑗)where as the\n̂ 𝑦𝑖𝑗is the predicted mask associated with the same cell.\n4. Experiments\nThe experiments that we have conducted are divided into three\nmain sections: (1) Multi-class semantic segmentation for visible and\nX-ray images, (2) Single-class segmentation for multi-modal images\n(i.e. visible, X-ray, heatmap and IR images) and (3) Segmentation on CT\nimages. The first set of experiments (multi-class semantic segmentation\non visible and X-ray images) are conducted to demonstrate our model’s\ncapacity to exploit features presented in multimodal images and the\nsecond set of experiments are conducted to validate our method onother imaging modalities (i.e. heatmap images and IR images). How-\never, it should be noted that the second set of experiments were limited\nto the single class segmentation due to limited availability of datasets.\nThirdly, we conducted the experiments to evaluate our method’s ability\nin handling CT images (Yang et al., 2018), using the AAPM Thoracic\nAuto-segmentation datasets which contains 5 classes (i.e. Esophagus,\nHeart, Left lung, Right lung and Spinal cord) and their train/test split\nwas used for the evaluations.\nFor the training of all the networks, we use the same learning rates\nand the weight decay values that have been used in the preceding\napproaches including Fast R-CNN (Girshick, 2015; Ren et al., 2015).\nWe use the pretrained backbone architectures where the training has\nbeen performed on COCO (Lin et al., 2014) dataset and the convolution\nlayer was initiated with random values. The learning rate, weight decay\nand the momentum were set to 0.02, 0.00001 and 0.9 accordingly.\nTo benchmark our method, we used Mask R-CNN (He et al., 2017).\nEvaluation Metrics\nFor the first and second evaluations as the evaluation metrics, we\nused average precision (AP), which is calculated using Intersection over\nUnion (IoU),\n𝐼𝑜𝑈 =𝐴𝑟𝑒𝑎(𝑏𝑝∩𝑏𝑡)\n𝐴𝑟𝑒𝑎(𝑏𝑝∪𝑏𝑡), (6)\nbased on a defined threshold.\nIn Eq. (6),𝑏𝑝is the prediction, which is the bounding box prediction\nor the mask prediction, and 𝑏𝑡is the corresponding groundtruth value.\nThe precision is calculated as,\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =𝑇𝑃\n𝑇𝑃+𝐹𝑃, (7)\nwhere a detection is defined as a true positive (TP), if 𝐼𝑂𝑈≥𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑\nand as a false positive (FP), if 𝐼𝑂𝑈 <𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 . In this evaluation we\nused the thresholds of 0.5 and 0.75 and 𝐴𝑃50denotes the calculated\naverage precision when the 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 is set to 50%.\nFor the experiments on the CT image dataset we used Dice score\nEq. (8) which is the most common evaluation protocol used for the CT\nsegmentation accuracy (Yang et al., 2018).\n𝐷𝑖𝑐𝑒 =2 ∗𝐴𝑟𝑒𝑎(𝑏𝑝∩𝑏𝑡)\n𝐴𝑟𝑒𝑎(𝑏𝑝)+𝐴𝑟𝑒𝑎(𝑏𝑡), (8)\nIn Eq. (8), the numerator estimates the area of overlap between the\ngroundtruth segmentation ( 𝑏𝑡) and the predicted segmentation ( 𝑏𝑝),\nwhile the denominator estimates the total number of pixels in both the\nsegmentations.\n4.1. Multi-class segmentation on X-ray and visible images\nThe evaluations that we describe in this section were performed\nwith two objectives. Firstly, selecting the optimum backbone for feature\nextraction is a critical design decision that affects the accuracy level.\n5\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nFig. 8. Training and validation losses for the our method when it was trained on a dataset which contained both visible and X-ray images.\nTherefore, in this section we employee a number of backbone archi-\ntectures (Resnet-50, Resnet-101, ResNeXt-50 and ResNeXt-101) and\nestimate the corresponding precision values, to select which backbone\narchitecture yields the best accuracy values. Secondly, we assess our\nmodel’s capability in robustly segmenting different parts in human\nupper extremity, and evaluate the benefits of using multi-modal images\nin a single network modal. In this section we also evaluate the effec-\ntiveness of using different values for the parameters in the loss function\nEq. (1).\nDatasets\nFor the X-ray image dataset we used the MURA dataset (Rajpurkar\net al., 2017) which contains 8 bit X-ray images of the human upper\nextremity. The MURA dataset contains X-ray images of elbow, forearm,\nhand, humerus, shoulder and wrist. For this dataset we generated\nmanually annotated mask groundtruth. For the visible images, we gen-\nerated a dataset which contains visible images of the same body parts\nthat are included in MURA dataset and the corresponding groundtruth\nmasks. The visible image dataset contains the images captured under\nchallenging conditions including different backgrounds, resolutions,\nocclusions and illumination conditions as well as the subjects wearing\ndifferent accessories such as wrist watches, jackets and bangles. The\ntotal number of images that were captured is 1400, where there were\n817 annotations of the hand, 908 annotations for the wrist, 1023 anno-\ntations for the forearm, 986 annotations for the elbow, 807 annotations\nfor the humerus and 722 annotations for the shoulder. However, it\nshould be noted that the visible images and X-ray images are not\ncaptured on the same subjects as these two sets were constructed\nindependently, and thus the training on multi-modal images is not\naffected by the subject related features. The training that we have\ncarried out can easily be adapted to the simpler case, where the images\ncaptured are on the same subjects. From the annotated X-ray images,\nwe extracted a subset such that it contains a comparable number of\nannotations to the number of annotations for each class in the visible\nimage dataset. For each of the classes we randomly divided our dataset\ninto 60%, 20% and 20%, which we used as the training set, validation\ntest and the test set respectively.\nTraining and Testing on Single Modalities: First, we trained Mask R-\nCNN and our model separately on the visible image dataset and the\nX-ray image dataset, and were tested on a dataset of the same modality.\nIt should be noted that the 𝜆and𝜇which are the hyper-parameters of\nthe loss function Eq. (1) were set to 1 following (He et al., 2017). The\nobtained results are recorded in Table 1, and it can be observed that our\nmethod surpasses the Mask R-CNN in small margins, for visible images,\nand for the X-ray images our method outperforms Mask R-CNN by large\nmargins.When the results and the intermediate layers which generated dis-\ncerning features were analysed, it was identified that for X-ray images\nthere is a large percentage of kernels which do not generate discerning\nfeatures in Mask R-CNN. However, for the visible images Mask R-CNN\nhas a high percentage of layers which generate discerning features.\nHowever, Mask R-CNN utilizes all the layers for the classification and\nmask identification tasks. In contrast our method uses the features\nwhich encode distinctive features. The difference in the two implemen-\ntations has given a high marginal improved accuracy for X-ray images\nin our method, whereas the improvement of the accuracy for the visible\nimages is limited.\nTraining and Testing on Multi-modal Datasets: We then trained both\nthe modals (Mask R-CNN and our method) on the multi-modal dataset,\nwhich contains both X-ray and Visible images. Similar to the single\nmodality training, we have divided the combined dataset for each body\npart into 60%, 20% and 20%, for training, validation and testing. The\ntraining losses and the validation losses that we obtained for when\ntraining our architecture on the multi-modal dataset are depicted in\nFig. 8.\nSome qualitative results that we obtained for both visible and X-ray\nimages are depicted in Fig. 9. This illustrates the effect of selecting the\ndiscerning features in contrast to using all the features for successive\noperations. Moreover in work by Pemasiri et al. (2019a), the benefits\nof cross-dataset learning are demonstrated with possible applications\nwhile elaborating on how the architecture of the network can easily\nbe fine-tuned according to the application under the consideration,\nspecifically for static and dynamic analysis in a clinical context.\nWhen the original Mask R-CNN and the our method were tested\non the same datasets for mask identification, the obtained average\nprecision values are indicated in Table 2. By analysing the table it can\nbe understood that the we outperform the Mask R-CNN for both the X-\nray and visible images. In addition, it can be identified that the features\nextracted from deeper and advanced networks can perform better\ncomparatively. From Table 2 it can be understood that ResNeXt-101-\nFPN has yielded the highest average precision for both the modalities\nand for both architectures, except for the 𝐴𝑃50in visible modality on\nwhich the Mask R-CNN with backbone ResNeXt-101-C4 has performed\nat the best. However, when comparing the results of Mask R-CNN and\nour architecture it was identified that the later with all the backbone\narchitectures have performed better than the best performing setting of\nMask R-CNN.\nThe average precision for bounding box identification using the\noriginal Mask R-CNN and our-method, which have been trained on the\nsame datasets is recorded in Table 3. It can be identified that the same\n6\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nTable 1\nAverage precision on single image modality segmentation on Mask R-CNN and our method for mask identification.\nMethod Backbone Trained & tested\non visible imagesTrained & tested\non X-ray images\n𝐴𝑃50 𝐴𝑃75 𝐴𝑃50 𝐴𝑃75\nMask R-CNN Resnet-50-C4 58.48 34.49 38.08 22.08\nMask R-CNN Resnet-101-C4 61.18 34.78 44.73 24.20\nMask R-CNN ResNet-50-FPN 59.83 32.75 40.81 23.03\nMask R-CNN ResNet-101-FPN 64.51 32.36 46.74 25.92\nMask R-CNN ResNeXt-50-C4 65.59 35.90 42.75 22.99\nMask R-CNN ResNeXt-101-C4 67.39 35.56 45.47 23.93\nMask R-CNN ResNeXt-50-FPN 68.61 39.23 46.49 24.72\nMask R-CNN ResNeXt-101-FPN 70.44 40.85 47.13 25.87\nOur method Resnet-50-C4 59.32 35.46 54.45 29.65\nOur method Resnet-101-C4 61.81 35.52 56.98 35.26\nOur method ResNet-50-FPN 60.44 33.26 57.15 33.50\nOur method ResNet-101-FPN 64.94 32.58 59.50 33.04\nOur method ResNeXt-50-C4 66.02 36.05 61.41 35.90\nOur method ResNeXt-101-C4 67.78 35.66 62.52 31.79\nOur method ResNeXt-50-FPN 68.98 39.30 64.25 39.00\nOur method ResNeXt-101-FPN 70.73 40.86 65.60 33.73\nTable 2\nAverage precision on visible images and X-ray images for image segmentation, where the models have been trained on multi-modal dataset.\nBackbone Trained on multi-modal\ntested on visible imagesTrained on multi-modal\ntested on X-ray imagesAverage\n𝐴𝑃50 𝐴𝑃75 𝐴𝑃50 𝐴𝑃75 𝐴𝑃50 𝐴𝑃75\nMask R-CNN Resnet-50-C4 39.1 27.4 37.7 21.8 38.4 24.6\nMask R-CNN Resnet-101-C4 43 29.1 44.3 23.3 43.7 26.2\nMask R-CNN ResNet-50-FPN 41.6 28.3 40.3 22.4 41.0 25.4\nMask R-CNN ResNet-101-FPN 43.8 29.5 46.1 25 45.0 27.3\nMask R-CNN ResNeXt-50-C4 41.9 28.7 42.1 22.8 42.0 25.8\nMask R-CNN ResNeXt-101-C4 45.6 29.3 44.8 23.9 45.2 26.6\nMask R-CNN ResNeXt-50-FPN 44.1 29.4 45.7 24.3 44.9 26.9\nMask R-CNN ResNeXt-101-FPN 44.7 29.6 46.2 25.3 45.5 27.5\nOur method Resnet-50-C4 57.3 31.3 53.4 27.7 55.4 29.5\nOur method Resnet-101-C4 59.7 33.1 54.4 28.8 57.1 31.0\nOur method ResNet-50-FPN 58.2 32.4 53.1 28.2 55.7 30.3\nOur method ResNet-101-FPN 62.6 34.6 54.9 29.3 58.8 32.0\nOur method ResNeXt-50-C4 62.9 34.9 55.8 30 59.4 32.5\nOur method ResNeXt-101-C4 64.1 35.3 56.9 31.7 60.5 33.5\nOur method ResNeXt-50-FPN 65.1 35.7 58.2 32.4 61.7 34.1\nOur method ResNeXt-101-FPN 66.8 37.1 58.9 33.5 62.9 35.3\nTable 3\nAverage precision on visible images and X-ray images for bounding box identification, where the models have been trained on multi-modal dataset.\nBackbone Trained on multi-modal\ntested on visible imagesTrained on multi-modal\ntested on X-ray imagesAverage\n𝐴𝑃50 𝐴𝑃75 𝐴𝑃50 𝐴𝑃75 𝐴𝑃50 𝐴𝑃75\nMask R-CNN Resnet-50-C4 42.10 29.05 42.05 25.46 42.07 27.26\nMask R-CNN Resnet-101-C4 46.92 34.98 49.34 28.36 48.13 31.67\nMask R-CNN ResNet-50-FPN 42.61 29.94 44.06 27.54 43.34 28.74\nMask R-CNN ResNet-101-FPN 48.90 36.62 51.80 33.03 50.35 34.83\nMask R-CNN ResNeXt-50-C4 44.62 34.50 48.63 27.83 46.63 31.17\nMask R-CNN ResNeXt-101-C4 50.48 35.48 50.34 31.13 50.41 33.30\nMask R-CNN ResNeXt-50-FPN 49.25 36.07 50.40 33.02 49.83 34.54\nMask R-CNN ResNeXt-101-FPN 49.54 39.33 52.95 33.62 51.24 36.48\nOur method Resnet-50-C4 63.29 39.75 57.28 30.09 60.28 34.92\nOur method Resnet-101-C4 65.56 38.99 61.70 33.01 63.63 36.00\nOur method ResNet-50-FPN 63.34 33.13 56.44 30.81 59.89 31.97\nOur method ResNet-101-FPN 65.81 38.70 61.75 33.86 63.78 36.28\nOur method ResNeXt-50-C4 68.40 44.18 62.49 34.26 65.44 39.22\nOur method ResNeXt-101-C4 69.13 42.84 63.58 36.39 66.35 39.62\nOur method ResNeXt-50-FPN 69.27 41.36 63.95 36.91 66.61 39.13\nOur method ResNeXt-101-FPN 69.80 46.31 64.16 36.94 66.98 41.63\nobservations that were made for the mask prediction are presented in\nbounding box identification as well.\nHyper Parameter Selection for the Loss Function: In the loss function\nEq. (1),𝜆and𝜇were set to 1, which is the parameter value that\nhas been used in the original work of Mask R-CNN (He et al., 2017).\nFurthermore from the evaluations that were conducted it was identifiedthat the deeper backbone architectures favourably affect the accuracy\nvalues. Then, to evaluate the effect of 𝜆and𝜇values we conducted an\nablative study by changing the parameter values. Since the bounding\nbox identification and mask identification are interrelated, in this study\nwe assigned the same value for 𝜆and𝜇. The obtained results are\n7\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nFig. 9. Qualitative results on visible images and X-ray images on object identification,\nmask identification and bounding box identification. The first column indicates the\ninput images, second column indicates the results from Mask R-CNN and the third\ncolumn indicates the results from our method.\nrecorded in Table 4. It can be seen that assigning higher values to the\n𝜆and𝜇has favourably affected the results.\n4.2. Single-class segmentation on multi-modal images\nThe evaluations that we describe in this section were carried out\nto validate our method on other imaging modalities, which includes\nheatmap images and IR images.\nDatasets\nUsing the HandNet (Wetzler et al., 2015) dataset, we obtained IR\nimages and heatmap images along with their groundtruth annotation of\nthe mask. HandNet dataset contains 214 971 images in total. However\nas the groundtruth annotations for hand masks in X-ray images were\nlimited to 4200 images, to eliminate the class imbalance we randomly\nselected 8400 images from HandNet dataset, and for 4200 of them\nwe obtained the IR images and for the rest we obtained the heatmap\nimages. For the visible images, the dataset we collected has 918 hand\nannotations. Therefore we used images from Rendered Hand PoseDataset (RHD dataset Zimmermann and Brox, 2017) along with our\nown dataset to make a dataset with 4200 visible images.\nArchitecture Setting\nBy the ablative studies that we discussed in the previous section,\nit was identified that feature extraction network of ResNeXt-101-FPN\nproduced the best accuracy values for majority of experiments. There-\nfore, in this evaluation we have used ResNeXt-101-FPN as the backbone\narchitecture. Similar to the layer identification that we conducted\nfor visible and X-ray images, when the features generated for some\nheatmap images and IR images from Mask R-CNN were analysed, it\ncould be identified that only some kernels generates features for these\ntwo modalities, yet these kernel numbers were consistent among the\nimages from a single modality. For the ResNeXt-101-FPN architecture,\nit was identified that there are 3011 feature outputs which generated\ndiscerning features for all the modalities under consideration, which\nare visible, X-ray, heatmap and IR images. Therefore, in the archi-\ntecture that was used for this evaluation 3011 feature outputs were\nconcatenated.\nFirst we trained both the model (the Mask R-CNN and our model)\nusing heatmap image dataset and IR image dataset separately and eval-\nuated using a test set from the same imaging modality. The results that\nwe obtained for mask identification and bounding box identification\nare recorded in Table 5. It can be seen that our model is capable of\nobtaining better results for both imaging modalities, when compared\nwith Mask R-CNN.\nThen we trained both Mask R-CNN and our method on multi-modal\nimage dataset, which includes visible, X-ray, visible, heatmap and IR\nimages. The obtained average precision values for this setting, are\ndepicted in Table 6. From the results it can be observed that our method\noutperforms Mask R-CNN by larger margins. Compared to the single\nmodality training Mask R-CNN demonstrates a significant decrease in\nthe accuracy values, which demonstrates the adverse effect of incorpo-\nrating the feature outputs which are generating discerning features only\nfor some imaging modalities. Qualitative results for the IR images and\nheatmap images are depicted in Fig. 10 and the observations that were\nmade in quantitative results are reflected by the qualitative results as\nwell.\nIt should be noted that the traditional segmentation methods (Ali\net al., 2006; Feng, 2006; Isard and Blake, 1998; Manos et al., 1994)\nwere not evaluated in this paper, as the objective of this work is\nto obtain accurate semantic segmentation (Fig. 2c) in contrast to the\ntraditional segmentation process (Fig. 2b).\n4.3. Multi-class segmentation on CT images\nDataset\nUsing the CT image dataset (Yang et al., 2018), we obtained CT\nimages along with their groundtruth annotation of the mask. This\ndataset contains free-breathing (FB) CT images from 60 patients and\nannotations for five organs (i.e. Esophagus, Heart, Left lung (Lung_L),\nRight lung (Lung_R) and Spinal cord).\nResults\nWe used the standard train/test split of the dataset and the evalua-\ntions were conducted using the Dice score Eq. (8). For the benchmark-\ning, we compared the best network configuration of Yang, Jinzhong,\net al. (Yang et al., 2018) with ours. The obtained results are recorded\nin Table 7 and some qualitative results are depicted in Fig. 11.\n4.4. Discussion on the generalization of the proposed approach\nGeneralization capacity in terms of imaging modalities: The pro-\nposed method has been validated on four imaging modalities (i.e. vis-\nible, X-ray, heatmap and IR images), and extending the method into\nother imaging modalities does not demand much effort. Kernel selec-\ntion needs to be carried out by passing a sample of images across the\n8\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nTable 4\nAverage precision on visible images and X-ray images for different 𝜆and𝜇values in Eq. (1) for image segmentation, where the models have been trained on multi-modal dataset.\nMethod Backbone 𝜆 𝜇 Trained on multi-modal\ntested on visible imagesTrained on multi-modal\ntested on X-ray imagesAverage\n𝐴𝑃50 𝐴𝑃75 𝐴𝑃50 𝐴𝑃75 𝐴𝑃50 𝐴𝑃75\nMask R-CNN ResNeXt-101-C4 1 1 50.48 35.48 50.34 31.13 50.41 33.30\nMask R-CNN ResNeXt-101-C4 2 2 52.36 37.73 52.80 33.38 52.58 35.56\nMask R-CNN ResNeXt-101-C4 3 3 52.81 39.27 53.90 35.05 53.36 37.16\nMask R-CNN ResNeXt-101-FPN 1 1 49.54 39.33 52.95 33.62 51.25 36.48\nMask R-CNN ResNeXt-101-FPN 2 2 51.42 41.58 55.41 35.87 53.42 38.73\nMask R-CNN ResNeXt-101-FPN 3 3 51.87 43.12 56.51 37.54 54.19 40.33\nOur method ResNeXt-101-C4 1 1 69.13 42.84 63.58 36.39 66.36 39.62\nOur method ResNeXt-101-C4 2 2 71.01 45.09 66.04 38.64 68.53 41.87\nOur method ResNeXt-101-C4 3 3 71.46 46.63 67.14 40.31 69.30 43.47\nOur method ResNeXt-101-FPN 1 1 69.80 46.31 64.16 36.94 66.98 41.63\nOur method ResNeXt-101-FPN 2 2 72.05 48.77 66.41 39.11 69.23 43.94\nOur method ResNeXt-101-FPN 3 3 73.59 49.87 68.08 39.89 70.84 44.88\nTable 5\nAverage precision on IR images and heatmap images for bounding box identification and segmentation, where the models have been trained\non individual image modality.\nTask Method 𝜆 𝜇 Trained & tested\non IR imagesTrained & tested on\nHeatmap images\n𝐴𝑃50 𝐴𝑃75 𝐴𝑃50 𝐴𝑃75\nSegmentationMask R-CNN1 1 49.21 28.32 44.13 25.68\n2 2 51.38 30.42 46.21 27.41\n3 3 51.88 30.86 46.57 27.75\nOur method1 1 58.53 31.19 61.33 43.47\n2 2 60.44 33.23 63.49 45.12\n3 3 60.70 33.57 63.83 45.36\nBounding box identificationMask R-CNN1 1 52.77 31.29 48.19 27.36\n2 2 54.49 33.35 50.26 28.87\n3 3 54.83 33.63 50.50 29.09\nOur method1 1 61.18 34.52 64.56 47.08\n2 2 62.90 36.35 66.51 48.47\n3 3 63.21 36.60 66.73 48.65\nTable 6\nAverage precision on IR images and heatmap images for bounding box identification and segmentation, where the models have been trained\non multi-modal images.\nTask Method 𝜆 𝜇 Trained on multimodal\n& tested on IR imagesTrained on multimodal &\ntested on Heatmap images\n𝐴𝑃50𝐴𝑃75 𝐴𝑃50𝐴𝑃75\nSegmentationMask R-CNN1 1 39.33 21.83 32.62 18.66\n2 2 41.43 23.71 34.51 20.38\n3 3 41.8 24.08 34.83 20.7\nOur method1 1 56.12 43.38 59.87 47.34\n2 2 58.16 45.47 62.04 49.04\n3 3 60.71 48.06 64.78 51.16\nBounding box IdentificationMask R-CNN1 1 43.54 31.04 37.09 23.42\n2 2 45.36 33.06 39.34 25.2\n3 3 46.65 34.61 41.06 26.61\nOur method1 1 60.11 45.9 64.01 51.51\n2 2 62.36 48.46 66.73 53.76\n3 3 63.94 50.36 68.84 55.52\nnetwork, which is followed by updating the model to extract features\nfrom the selected kernels, and then the training can be performed using\nthe loss functions mentioned in Section in this paper.\nGeneralization capacity in terms of architectures: In the current\nimplementation variations of ResNet architecture (Resnet-50, Resnet-\n101, ResNeXt-50 and ResNeXt-101) have been used. The methodology\ncan be used with any other backbone (e.g., DenseNet Huang et al.,\n2017a and MobileNet Howard et al., 2017) architectures by replacing\nthe feature extraction component of the architecture depicted in Fig. 7,\nwith the corresponding feature extraction framework. Since the fea-\ntures generated by the kernels depend on the backbone architecture in\nthe process of adopting the proposed method to a different architecture,the kernel selection needs to be performed by passing sample images\nacross the network.\nGeneralization capacity in terms of classes: By the performed ex-\nperiments, it is evident that the proposed method has the ability to\ncapture the semantic segments on human upper extremity. Extending\nthe model to other body parts does not require much effort. By the\nempirical studies, we observed that for a given modality the kernels\nwhich generated discerning features for all the body parts under the\nconsideration of this paper (those body parts are elbow, forearm,\nhand, humerus, shoulder and wrist) remained the same. Therefore the\nexisting models can be used directly for other body parts which are\nfrom the same modality.\n9\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nFig. 10. Qualitative results on IR and heatmap images on segmentation. The first\ncolumn indicates the input images, second column indicates the groundtruth masks,\nthird column indicates the results obtained by our method and the fourth column\nindicates the results that were obtained from Mask R-CNN.\nFig. 11. Qualitative results on CT image dataset. The first column indicates the input\nimages, second column indicates the groundtruth masks and third column indicates the\nresults obtained by our method. Spinal cord, Esophagus, Heart, Lung_L and Lung_R are\ndenoted by red, blue, green, yellow and cyan colours respectively. (For interpretation\nof the references to colour in this figure legend, the reader is referred to the web\nversion of this article.)\n5. Conclusion\nIn this paper we have presented a modality invariant segmentation\nframework, where the most informative layers of CNNs are effectively\nutilized as the features for class prediction, region identification and\nmask identification across multi-modal images. The different textures\nof imaging modalities and the features that are generated by the\nconvolutional neural networks for these different textures are takenTable 7\nDice score values obtained for CT image dataset.\nLeft lung Right lung Heart Esophagus Spinal cord\nYang et al. (2018) 0.97 0.97 0.93 0.72 0.88\nOurs 0.97 0.97 0.96 0.78 0.91\ninto consideration when devising our architecture. In contrast to the\nprevious approaches where the successive layers of the network use\nall the outputs from the previous layers, and the features of the last\nconvolutional layer are subjected for the mask extraction, we filter the\nmost informative outputs of all the layers in the network. The current\nimplementation has outperformed the widely used segmentation frame-\nwork ‘‘ Mask R-CNN’’ for the visible, X-ray, heatmap and IR images, and\nthis method can be extended to other image modalities such as CT scan\nand MRI by observing the convolutional neural network’s behaviour on\nthe modalities under consideration. Using a single model for semantic\nsegmentation of multiple modalities, not only seamlessly incorporates\nall sub-tasks into one, but also reduces the number of parameters and\nimproves the accuracy by enabling end-to-end training and inference.\nThis is meaningful in the real world, since many modalities (e.g., X-ray)\nare quite hard to acquire and by using a single model, we can train\nthe network with the other modalities (e.g., visible) where samples are\nabundant. Our code and the dataset we used in this work are released\nso that the broader research community is able to replicate our results\nand further advance the area of multi-modal image segmentation of\nmedical images.\nCRediT authorship contribution statement\nAkila Pemasiri: Conception of ideas, Methodology development,\nData preparation, Implementation, Validation, Analysis of results, Writ-\ning - original draft. Kien Nguyen: Conception of ideas, Supervision,\nMethodology development, Analysis of results, Writing, Editing. Sridha\nSridharan: Conception of ideas, Supervision, Methodology develop-\nment, Analysis of results, Writing, Editing. Clinton Fookes: Conception\nof ideas, Supervision, Methodology development, Analysis of results,\nWriting, Editing.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAcknowledgement\nThe research presented in this paper was supported by an Australian\nResearch Council (ARC) grant DP170100632.\nReferences\nAli, M.A., Dooley, L.S., Karmakar, G.C., 2006. Object based image segmentation using\nfuzzy clustering. In: Acoustics, Speech and Signal Processing, 2006. ICASSP 2006\nProceedings. 2006 IEEE International Conference on, Vol. 2. IEEE, p. II.\nBates, R., Irving, B., Markelc, B., Kaeppler, J., Brown, G., Muschel, R.J., Brady, M.,\nGrau, V., Schnabel, J.A., 2019. Segmentation of vasculature from fluorescently\nlabeled endothelial cells in multi-photon microscopy images. In: IEEE Transactions\non Medical Imaging, Vol. 38. IEEE, pp. 1–10.\nBullock, J., Cuesta-Lázaro, C., Quera-Bofarull, A., 2019. XNet: a convolutional neural\nnetwork (CNN) implementation for medical x-ray image segmentation suitable for\nsmall datasets. In: Medical Imaging 2019: Biomedical Applications in Molecular,\nStructural, and Functional Imaging, Vol. 10953. International Society for Optics\nand Photonics, 109531Z.\nDai, J., He, K., Sun, J., 2016. Instance-aware semantic segmentation via multi-task\nnetwork cascades. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. pp. 3150–3158.\n10\n\nA. Pemasiri, K. Nguyen, S. Sridharan et al. Computer Vision and Image Understanding 202 (2021) 103085\nDuarte, A., Carrão, L., Espanha, M., Viana, T., Freitas, D., Bártolo, P., Faria, P.,\nAlmeida, H., 2014. Segmentation algorithms for thermal images. Proc. Technol.\n16, 1560–1569.\nElanattil, S., Moghadam, P., Sridharan, S., Fookes, C., Cox, M., 2018. Non-rigid\nreconstruction with a single moving RGB-D camera. In: 2018 24th International\nConference on Pattern Recognition. ICPR, IEEE, pp. 1049–1055.\nElgammal, A., Harwood, D., Davis, L., 2000. Non-parametric model for background\nsubtraction. In: European Conference on Computer Vision. Springer, pp. 751–767.\nFeng, D., 2006. Segmentation of Bone Structures in X-ray Images (Ph.D. thesis). School\nof Computing National University of Singapore, Superviser Dr. Leow Wee Kheng.\nGirshick, R., 2015. Fast r-cnn. arXiv preprint arXiv:1504.08083.\nGirshick, R., Donahue, J., Darrell, T., Malik, J., 2014. Rich feature hierarchies for\naccurate object detection and semantic segmentation. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. pp. 580–587.\nGonfaus, J.M., Boix, X., Van de Weijer, J., Bagdanov, A.D., Serrat, J., Gonzalez, J., 2010.\nHarmony potentials for joint classification and segmentation. In: Computer Vision\nand Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, pp. 3280–3287.\nHe, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask r-cnn. In: Computer Vision\n(ICCV), 2017 IEEE International Conference on. IEEE, pp. 2980–2988.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recog-\nnition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. pp. 770–778.\nHoward, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-\ndreetto, M., Adam, H., 2017. Mobilenets: Efficient convolutional neural networks\nfor mobile vision applications. arXiv preprint arXiv:1704.04861.\nHržić, F., Štajduhar, I., Tschauner, S., Sorantin, E., Lerga, J., 2019. Local-entropy based\napproach for X-ray image segmentation and fracture detection. Entropy 21 (4), 338.\nHu, J., Chen, Y., Yi, Z., 2019. Automated segmentation of macular edema in OCT using\ndeep neural networks. Med. Image Anal. 55, 216–227.\nHuang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017a. Densely connected\nconvolutional networks. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. pp. 4700–4708.\nHuang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Wojna, Z.,\nSong, Y., Guadarrama, S., et al., 2017b. Speed/accuracy trade-offs for modern\nconvolutional object detectors. In: IEEE CVPR.\nIsard, M., Blake, A., 1998. Condensation—conditional density propagation for visual\ntracking. Int. J. Comput. Vis. 29 (1), 5–28.\nJi, J., Buch, S., Soto, A., Carlos Niebles, J., 2018. End-to-end joint semantic segmen-\ntation of actors and actions in video. In: Proceedings of the European Conference\non Computer Vision, ECCV. pp. 702–717.\nJian, B.-L., Chen, C.-L., Chu, W.-L., Huang, M.-W., 2017. The facial expression of\nschizophrenic patients applied with infrared thermal facial image sequence. BMC\nPsychiatry 17 (1), 229.\nKhowaja, S.A., Lee, S.-L., 2019. Semantic image networks for human action recognition.\narXiv preprint arXiv:1901.06792.\nKundu, A., Li, Y., Dellaert, F., Li, F., Rehg, J.M., 2014. Joint semantic segmentation\nand 3d reconstruction from monocular video. In: European Conference on Computer\nVision. Springer, pp. 703–718.\nLi, X., Chen, H., Qi, X., Dou, Q., Fu, C.-W., Heng, P.A., 2018a. H-DenseUNet: Hybrid\ndensely connected UNet for liver and tumor segmentation from CT volumes. In:\nIEEE Transactions on Medical Imaging, Vol. 37. IEEE, pp. 2663–2674.\nLi, H., Ellis, J.G., Zhang, L., Chang, S.-F., 2018b. PatternNet: Visual pattern mining with\ndeep neural network. In: Proceedings of the 2018 ACM on International Conference\non Multimedia Retrieval. ACM, pp. 291–299.\nLin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., 2017. Feature\npyramid networks for object detection. In: CVPR, Vol. 1, No. 2. p. 4.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,\nZitnick, C.L., 2014. Microsoft coco: Common objects in context. In: European\nConference on Computer Vision. Springer, pp. 740–755.\nLong, J.L., Zhang, N., Darrell, T., 2014. Do convnets learn correspondence? In: Advances\nin Neural Information Processing Systems. pp. 1601–1609.\nManos, G., Cairns, A., Rickets, I., Sinclair, D., 1994. Segmenting radiographs of the\nhand and wrist. Comput. Methods Programs Biomed. 43 (3–4), 227–237.Martin, D.R., Fowlkes, C.C., Malik, J., 2003. Learning to detect natural image bound-\naries using brightness and texture. In: Advances in Neural Information Processing\nSystems. pp. 1279–1286.\nMittal, A., Paragios, N., 2004. Motion-based background subtraction using adaptive\nkernel density estimation. In: Computer Vision and Pattern Recognition, 2004.\nCVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, Vol.\n2. IEEE, p. II.\nNakamura, T., Sato, M., Kajimoto, H., 2013. Semi-automatic scoring method for\ntorticollis by using kinect: 328. Mov. Disorders 28.\nPemasiri, A., Ahmedt-Aristizabal, D., Nguyen, K., Sridharan, S., Dionisio, S., Fookes, C.,\n2019a. Semantic segmentation of hands in multimodal images: A region new-\nbased CNN approach. In: 2019 IEEE 16th International Symposium on Biomedical\nImaging. ISBI 2019, IEEE, pp. 819–823.\nPemasiri, A., Nguyen, K., Sridharan, S., Fookes, C., 0000. Unified 2D and 3D hand pose\nestimation from a single visible or X-ray image.\nPemasiri, A., Thanh, K.N., Sridharan, S., Fookes, C., 2019b. Semantic correspondence\nin the wild. In: 2019 IEEE Winter Conference on Applications of Computer Vision.\nWACV, IEEE, pp. 1137–1146.\nQiao, Y., Wei, Z., Zhao, Y., 2017. Thermal infrared pedestrian image segmentation\nusing level set method. Sensors 17 (8), 1811.\nRajpurkar, P., Irvin, J., Bagul, A., Ding, D., Duan, T., Mehta, H., Yang, B., Zhu, K.,\nLaird, D., Ball, R.L., et al., 2017. MURA dataset: Towards radiologist-level ab-\nnormality detection in musculoskeletal radiographs. arXiv preprint arXiv:1712.\n06957.\nRen, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. In: Advances in Neural Information\nProcessing Systems. pp. 91–99.\nRowley, H.A., Baluja, S., Kanade, T., 1998. Neural network-based face detection. IEEE\nTrans. Pattern Anal. Mach. Intell. 20 (1), 23–38.\nSalscheider, N.O., 2019. Simultaneous object detection and semantic segmentation.\narXiv preprint arXiv:1905.02285.\nSermanet, P., Kavukcuoglu, K., Chintala, S., LeCun, Y., 2013. Pedestrian detection\nwith unsupervised multi-stage feature learning. In: Computer Vision and Pattern\nRecognition (CVPR), 2013 IEEE Conference on. IEEE, pp. 3626–3633.\nShrivastava, A., Sukthankar, R., Malik, J., Gupta, A., 2016. Beyond skip connections:\nTop-down modulation for object detection. arXiv preprint arXiv:1612.06851.\nSonkusare, S., Ahmedt-Aristizabal, D., Aburn, M.J., Nguyen, V.T., Pang, T., Frydman, S.,\nDenman, S., Fookes, C., Breakspear, M., Guo, C.C., 2019. Detecting changes in facial\ntemperature induced by a sudden auditory stimulus based on deep learning-assisted\nface tracking. Sci. Rep. 9 (1), 4729.\nStolojescu-CriŞan, C., Holban, Ş., 2013. A comparison of X-ray image segmentation\ntechniques. Adv. Electr. Comput. Eng. 13 (3).\nTorabi, A., Bilodeau, G.-A., 2013. A LSS-based registration of stereo thermal–visible\nvideos of multiple people using belief propagation. Comput. Vis. Image Underst.\n117 (12), 1736–1747.\nVaillant, R., Monrocq, C., Le Cun, Y., 1994. Original approach for the localisation of\nobjects in images. IEE Proc.-Vis. Image Signal Process. 141 (4), 245–250.\nWetzler, A., Slossberg, R., Kimmel, R., 2015. Rule of thumb: Deep derotation for im-\nproved fingertip detection. In: Xie, X., Jones, M.W., Tam, G.K.L. (Eds.), Proceedings\nof the British Machine Vision Conference. BMVC, BMVA Press, pp. 33.1–33.12.\nWollmann, T., Gunkel, M., Chung, I., Erfle, H., Rippe, K., Rohr, K., 2019. GRUU-Net:\nIntegrated convolutional and gated recurrent neural network for cell segmentation.\nMed. Image Anal.\nWu, J., Mahfouz, M.R., 2016. Robust x-ray image segmentation by spectral clustering\nand active shape model. J. Med. Imaging 3 (3), 034005.\nXie, S., Girshick, R., Dollár, P., Tu, Z., He, K., 2017. Aggregated residual transformations\nfor deep neural networks. In: Computer Vision and Pattern Recognition (CVPR),\n2017 IEEE Conference on. IEEE, pp. 5987–5995.\nYang, J., Veeraraghavan, H., Armato, III, S.G., Farahani, K., Kirby, J.S., Kalpathy-\nKramer, J., van Elmpt, W., Dekker, A., Han, X., Feng, X., et al., 2018.\nAutosegmentation for thoracic radiation treatment planning: A grand challenge at\nAAPM 2017. Med. Phys. 45 (10), 4568–4581.\nZimmermann, C., Brox, T., 2017. Learning to estimate 3d hand pose from single rgb\nimages. In: Proceedings of the IEEE International Conference on Computer Vision.\npp. 4903–4911.\n11\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nPreprint.\nDigital Object Identiﬁer 10.1109/ACCESS.2017.DOI\nTransMed: Transformers Advance\nMulti-modal Medical Image Classiﬁcation\nYin Dai1,2, Yifan Gao1.\n1College of Medicine and Biological Information Engineering, Northeastern University, China\n2Engineering Center on Medical Imaging and Intelligent Analysis, Ministry Education, Northeastern University, Shenyang 110169, China\nCorresponding author: Yin Dai (e-mail: daiyin@bmie.neu.edu.cn).\nABSTRACT Over the past decade, convolutional neural networks (CNN) have shown very competitive\nperformance in medical image analysis tasks, such as disease classiﬁcation, tumor segmentation, and lesion\ndetection. CNN has great advantages in extracting local features of images. However, due to the locality of\nconvolution operation, it can not deal with long-range relationships well. Recently, transformers have been\napplied to computer vision and achieved remarkable success in large-scale datasets. Compared with natural\nimages, multi-modal medical images have explicit and important long-range dependencies, and effective\nmulti-modal fusion strategies can greatly improve the performance of deep models. This prompts us to\nstudy transformer-based structures and apply them to multi-modal medical images. Existing transformer-\nbased network architectures require large-scale datasets to achieve better performance. However, medical\nimaging datasets are relatively small, which makes it difﬁcult to apply pure transformers to medical\nimage analysis. Therefore, we propose TransMed for multi-modal medical image classiﬁcation. TransMed\ncombines the advantages of CNN and transformer to efﬁciently extract low-level features of images and\nestablish long-range dependencies between modalities. We evaluated our model for the challenging problem\nof preoperative diagnosis of parotid gland tumors, and the experimental results show the advantages of our\nproposed method. We argue that the combination of CNN and transformer has tremendous potential in\na large number of medical image analysis tasks. To our best knowledge, this is the ﬁrst work to apply\ntransformers to medical image classiﬁcation.\nINDEX TERMS Deep Learning, Medical Image Analysis, Transformer, Multi-modal.\nI. INTRODUCTION\nTransformers were ﬁrst applied in the ﬁeld of natural lan-\nguage processing (NLP) [1]. It is a deep neural network\nmainly based on the self-attention mechanism to extract\nintrinsic features. Because of its powerful representation ca-\npabilities, researchers hope to ﬁnd a way to apply transform-\ners to computer vision tasks. Compared with text, images\ninvolve larger size, noise, and redundant modalities, so it is\nconsidered more difﬁcult to use transformers on these tasks.\nRecently, transformers have made a breakthrough in com-\nputer vision. A large number of transformer-based methods\nhave been proposed for computer vision tasks, such as DETR\n[2] for object detection, SETR [3] for semantic segmentation,\nViT [4] and DeiT [5] for image recognition.\nTransformers have achieved success in natural images,\nbut it has received little attention in medical image analy-\nsis, especially in multi-modal medical image fusion. Multi-\nmodal images are widely used in medical image analysisto achieve lesion segmentation or disease classiﬁcation. The\nexisting medical image multi-modal fusion based on deep\nlearning can be divided into three categories: input-level\nfusion, feature-level fusion, and decision-level fusion [6].\nInput-level fusion strategy fuses multi-modal images into the\ndeep network by multi-channel, learns fusion feature repre-\nsentation, and then trains the network. Input-level fusion can\nretain the original image information to the maximum extent\nand learn the image features. Feature-level fusion strategy\ntrains a single deep network by taking the image of each\nmodality as a single input. Each representation is fused in the\nnetwork layer, and the ﬁnal result is fed to the decision layer\nto obtain the ﬁnal result. Feature-level fusion network can\neffectively capture the information of different modalities of\nthe same patient. Decision-level fusion integrates the output\nof each network to obtain the ﬁnal result. Decision-level\nfusion network aims to learn more abundant information\nfrom different modalities independently.\nVOLUME 4, 2016 1arXiv:2103.05940v1  [cs.CV]  10 Mar 2021\n\nY . Dai et al. : TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nHowever, they all have shortcomings in varying degrees.\nThe input-level fusion strategy is difﬁcult to establish the\ninternal relationship between different modalities of the same\npatient, which leads to the degradation of the model per-\nformance. Each modality of the feature-level network corre-\nsponds to a neural network, which brings huge computational\ncosts, especially in the case of a large number of modali-\nties. The output of each modality of decision-level fusion\nis independent of each other, so the model cannot establish\nthe internal relationship between different modalities of the\nsame patient. In addition, like decision-level fusion strategy,\ndecision-level fusion strategy is also computationally inten-\nsive.\nTherefore, there is an urgent need to combine the three\nfusion strategies efﬁciently. A good multi-modal fusion strat-\negy should achieve as much interaction between different\nmodalities as possible with low computational complexity.\nCompared with CNN, transformers can effectively mine\nlong-range relationships between sequences. The existing\ncomputer vision models based on transformer mainly deal\nwith 2D natural images, such as ImageNet [6] and other\nlarge-scale datasets. The method of constructing sequences in\n2D images is to cut the images into a series of patches. This\nkind of sequence construction method implicitly shows long-\nrange dependencies, which is not very intuitive, so it may be\ndifﬁcult to bring signiﬁcant performance improvement.\nOn the contrary, there are more explicit sequences in med-\nical images, which contain important long-range dependency\nand semantic information, as shown in Fig.1. Due to the\nsimilarity of human organs, most visual representations are\norderly in medical images. Destruction of these sequences\nwill signiﬁcantly reduce the performance of the model. It\ncan be considered that compared with natural images, the\nsequence relationship of medical images (such as modality,\nslice, patch) holds more abundant information. In practice,\ndoctors will synthesize the pathological information of each\nmodality to make the diagnosis. However, the existing multi-\nmodal fusion methods are too simple to consider the correla-\ntion of these sequences, and lack of modeling for these long-\nrange dependencies. The transformer is an elegant, efﬁcient,\nand powerful encoder for processing sequence relations,\nwhich is the motivation for us to propose the multi-modal\nmedical image classiﬁcation method based on transformers.\nIn this work, we present the ﬁrst study to explore the\ntremendous potential of transformers in the context of multi-\nmodal medical image classiﬁcation. The proposed method\nis inspired by the property that the transformer is effective\nin extracting the relationship between sequences. However,\ndue to the small scale of medical image datasets and the\nlack of sufﬁcient information to establish the relationship\nbetween low-level semantic features, the performance of pure\ntransformer networks based on ViT and DeiT is not satisfac-\ntory in multi-modal medical image classiﬁcation. Therefore,\nwe propose TransMed, which combines the advantages of\nCNN and transformer to capture low-level features and cross-\nmodality high-level connections. TransMed ﬁrst processes\nFIGURE 1. Compared with natural images, multi-modal medical images have\nmore informative sequences.\nthe multi-modal images as sequences and sends them to\nCNN, then uses transformers to learn the relationship be-\ntween the sequences and make predictions. Since the trans-\nformer effectively models the global features of multi-modal\nimages, TransMed outperforms the existing multi-modal fu-\nsion methods in terms of parameters, operation speed, and\naccuracy. A large number of experiments have proved the\neffectiveness of our method.\nIn summary, we make the following three contributions:\n1) We apply transformers to medical image classiﬁcation\nfor the ﬁrst time, and greatly improve the accuracy and\nefﬁciency of deep models.\n2) We propose a novel multi-modal image fusion strategy\nin this work, which can be leveraged to capture mutual\ninformation from images of different modalities in a\nmore efﬁcient way.\n3) Experimental evaluations demonstrate that the pro-\nposed method achieves the most advanced perfor-\nmance in the classiﬁcation of the parotid gland tumors.\nThe rest of this paper is organized as follows. Section\nII presents some closely related works. The pipeline of our\nproposed method is in Section III. Section IV introduces the\nexperimental results and details. Finally, we summarize our\nwork in Section V .\nII. RELATED WORK\nA. MULTI-MODAL MEDICAL IMAGE ANALYSIS\nMulti-modal medical analysis is one of the most fundamental\nand challenging parts of medical image analysis. It is proved\nthat a reasonable fusion of different modalities has been a\npotential means to enhance Deep networks [6]. Multi-modal\nfusion can capture more abundant pathological information\nand improve the quality of diagnosis.\n[8]–[10] mainly used the input-level fusion, which is\nthe most common fusion method in multi-modal medical\nimage analysis. Some other papers have shown the potential\nof feature-level fusion in medical image processing. Hyper\nDenseNet built dual deep networks for different modalities\n2 VOLUME 4, 2016\n\nY . Dai et al. : TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nFIGURE 2. Overview of TransMed, which is composed of CNN branch and transformer branch.\nof Magnetic resonance imaging (MRI) and linked features\nacross these streams [11]. [12] fused ﬁnal features from\nmodality-speciﬁc paths to make ﬁnal decisions. MMFNet\nused speciﬁc encoders to capture modality-speciﬁc features\nand designs a decoder with a complex structure to fuse these\nfeatures [13]. Different from the ﬁrst two techniques, [14],\n[15] applied decision-level fusion technology to improve\nperformance. [15] set three modality-speciﬁc encoders to\ncapture low-level features and a decoder to fuse low-level and\nhigh-level features, then the results of each branch were fused\nto generate the ﬁnal result. [14] designed a gate network to\ndynamically combine each decision and make a prediction.\nBesides, some studies have evaluated multiple fusion\nmethods at the same time. [16] used feature-level fusion and\ndecision-level fusion in their work. [17] designed three kinds\nof fusion networks, and gets better performance than a single\nmodality. These fusion methods improve the performance of\nthe model to a certain extent, but there are some shortcom-\nings, such as poor scalability, large computational complex-\nity, and difﬁculty in establishing long-range connections.\nB. TRANSFORMERS\nTransformers were ﬁrst proposed for machine translation\nand achieved satisfactory results in a large number of NLP\ntasks. Then, the transformer structures were introduced into\nthe ﬁeld of computer vision, and some modiﬁcations were\nmade according to the speciﬁc tasks. The results show the\npotential of transformers to surpass pure CNN. Some work\nuses the framework of CNN and transformer [2], [18], while\nothers directly use pure transformers to replace CNN [2], [4],[5], [19]. These methods have shown encouraging results in\ncomputer vision tasks, but their direct applications in multi-\nmodal medical images are not effective and require a lot\nof computing resources. As far as we know, TransMed is\nthe ﬁrst multi-modal medical image classiﬁcation framework\nbased on transformers, which provides a novel multi-modal\nimage fusion strategy.\nIII. METHODS\nThe most common method of multi-modal medical image\nclassiﬁcation is to train CNN directly (such as Resnet [20]).\nFirstly, the image is encoded as a high-level feature represen-\ntation, and then its features or decisions are fused. Different\nfrom the existing methods, our method uses transformers\nto introduce the self-attention mechanism into the multi-\nmodal fusion strategy. We will ﬁrst introduce how to directly\napply transformers to aggregate feature representations from\ndecomposed image patches in Section 3.A. Then, the overall\nframework of TransMed will be described in detail in Section\n3.B.\nA. TRANSFORMERS AGGREGATE MULTI-MODAL\nFEATURES\nIn this work, we follow the original DeiT implementation\nas much as possible. The advantage of this intentionally\nsimple setting is to reduce the impact of other tricks on the\nperformance of the model and intuitively show the beneﬁts\nof transformers. In addition, we can use the extensible DeiT\nmodel and its pre-trained weights almost immediately.\nVOLUME 4, 2016 3\n\nY . Dai et al. : TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nFIGURE 3. Overview of self-attention, matmul means matrix product of two\narrays.\nThe important components of the transformer including\nself-attention (SA), multi-head self-attention (MSA), and\nmulti-layer perception (MLP). The input of transformers\nincludes a variety of embeddings and tokens. Slightly dif-\nferent from DeiT, we remove the linear projection layer and\ndistillation token. We will describe each of these components\nin this section.\n1) Self-Attention\nSA is an attention mechanism, which uses other parts of\nthe same sample to predict the rest of the data sample. In\ncomputer vision, it is a little similar to non-local networks\n[21]. SA has many forms, and the common transformer relies\non the form of scaled dot-product shown in Figure 3. In\nthe SA layer, the input vector is ﬁrst transformed into three\ndifferent vectors: query matrix Q, key matrix K, and value\nmatrix V , the output is the weighted sum of the value vectors.\nThe weight assigned to each value is determined by the dot\nproduct of the query and the corresponding key. The attention\nfunction between different input vectors is calculated as\nfollows:\nAttention (Q; K; V ) =Softmax (QKT\npdk)\u0001V (1)\nWhere dkis the dimension of key vector k.pdkprovides\nan appropriate normalization to make the gradient more\nstable.\n2) Multi-head Self-Attention\nMSA is the core component of the transformer. As shown\nin Figure 4, The difference from SA is that the multi-head\nmechanism splits the input into many small parts, then cal-\nculates the scaled dot-product of each input in parallel, and\nsplices all the attention outputs to get the ﬁnal result. The\nformula of MSA can be written as follows:\nhead i=Attention (QWQ\ni; KWK\ni; V WV\ni) (2)\nMSA (Q; K; V ) =Concat (head i; :::; head i)WO(3)\nFIGURE 4. An illustration of our multi-head self-attention component, concat\nmeans concatenate representations.\nWhere the projections WQ\ni,WK\ni,WV\niandWOare train-\nable parameter matrices, his the number of transformer lay-\ners. The advantage of MSA is that it allows the model to learn\nsequence and location information in different representation\nsubspaces.\n3) Multi-Layer Perceptron\nIn this paper, a MLP is added on top of the MSA layer. The\nMLP is composed of linear layers separated by a GeLU [22]\nactivation. Both MSA and MLP have skip-connections like\nresidual networks and with a layer normalization. Therefore,\nit is assumed that the representation of the t\u00001layer is xt\u00001,\nLN represents the linear normalization, and the output of the\ntlayer can be written as follows:\n^xt=MSA (LN(xt\u00001)) +xt\u00001 (4)\nxt=MLP (LN( ^xt)) + ^xt (5)\n4) Embeddings and Tokens\nThe input layer contains ﬁve embeddings and tokens, which\nare patch embedding, position embedding, class embedding,\npatch token, and class token.\nPatch embedding is the representation of each patch’s\noutput from CNN, and class embedding is a trainable vector.\nTo encode the spatial information and location information\nof a patch into patch tokens, we use position embeddings and\npatch embeddings to preserve the information. Class embed-\nding does not have patch embedding that can be added, so\nclass token and class embedding are equivalent. Suppose the\ninput is x, the trainable vector is Wc, the position embedding\nisxpo, patch tokens xptand class token xctcan be expressed\nas follows:\nxpt=Conv (x) +xpo (6)\nxct=Wc(7)\nThe class token is attached to patch tokens before the input\nlayer of transformers, passes through the transformer layer,\n4 VOLUME 4, 2016\n\nY . Dai et al. : TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nand then outputs from the fully connected layer to predict the\nclass.\nB. TRANSMED\nThe structure of TransMed is shown in Figure 2. Instead of\nusing pure transformers as the encoder, TransMed adopts a\nhybrid model including CNN and transformer, in which CNN\nis used as a low-level feature extractor to generate the patch\nembedding.\nGiven a multi-modal image x2RB\u0002M\u0002C\u0002D\u0002H\u0002W,\nwhere spatial resolution is H\u0002W, the depth is D, the\nnumber of channels is C, the number of modalities is M,\nand the batch size is B. Before sending it to the CNN en-\ncoder, it is necessary to construct the sequence. Firstly, three\nadjacent 2D slices of a multi-modal image are superimposed\nto construct three-channel images. Then, according to [4],\neach image will be divided into K\u0002K. The larger Kvalue\nmeans that the size of each patch is smaller. We will evaluate\nthe impact of different Kvalues on the performance of the\nmodel in Section 4.E. Finally, the image is encoded into a\n(1\n3BMCDK2;3;H\nK;W\nK)patch.\nAfter the image sequence is constructed, it is input into\nthe 2D CNN. The last fully connected layer of 2D CNN\nis replaced by a linear projection layer to map the fea-\ntures of the vector patch to the potential embedding space.\n2D CNN extracts low-level features from the image se-\nquence and encodes them preliminarily. The output shape is\n(B;1\n3MCDK2; P), in which the size of Pis set to adapt to\nthe input size of the transformer.\nIV. RESULTS\nA. DATASET AND PREPROCESSING\n1) DATASET\nWe use a dataset collected in cooperative hospitals to evaluate\nthe performance of our proposed method under multi-modal\nimages. This dataset included two modalities of MRI (T1 and\nT2) of 344 patients, and the ground truth labels are obtained\nfrom biopsies.\nThe incidence of malignant tumors in parotid gland tumors\nis about 20 %[23]. Correct preoperative diagnosis of these tu-\nmors is essential for proper surgical planning. Among them,\nimaging examination plays an important role in determining\nthe nature of parotid gland masses. MRI is considered to\nbe the preferred imaging method for preoperative diagnosis\nof parotid tumors [24]. MRI can provide information about\nthe exact location of the lesion, the relationship with the\nsurrounding structure, and can assess the spread of nerves\nand bone invasion. However, it is reported that parotid gland\ntumors show considerable overlap in imaging features (such\nas tumor margins, homogeneity, and signal intensity), so it is\ndifﬁcult for doctors to identify the mass.\nAccording to common clinical classiﬁcations, we divide\nparotid gland tumors into ﬁve categories: Pleomorphic Ade-\nnoma (PA), Warthin Tumor (WT), Malignant Tumor (MT),\nBasal Cell Adenoma (BCA), and Other Benign Lesions\n(OBL) [25].2) PREPROCESSING\nFirst, perform OTSU [26] to extract the foreground area in\nthe original image. Then the images of different modalities\nof the same patient are registered to improve the consistency\nof the foreground area. Then resample each image to (18,\n448, 448). Therefore, 344 images are ﬁnally included, each of\nwhich is a stack of 3D images of MRI T1 and T2, and the size\nis (36, 448, 448). Data augmentation uses random ﬂipping\nand random noise. Random ﬂipping performs ﬂipping of the\nimage with 50 %probability. Random noise adds Gaussian\nnoise with a mean value of 0 and a variance of 0.1 to the\nimage.\nB. EXPERIMENTAL SETTINGS AND EVALUATION\nCRITERIA\nThe patients were randomly divided into the training group (n\n= 275) and independent test group (n = 69) according to the\nratio of 4:1, and then the training group was used to optimize\nthe model parameters. We set SGD as the optimizer with\na learning rate equal to 10\u00003and momentum equal to 0.7.\nThe maximum training round is set to 100. Our experiments\nwere performed on NVIDIA 3080 GPU (with 10GB GPU\nmemory). The code is implemented using Pytorch [27] and\nTorchIO [28]. To eliminate accidental factors, each model\nis subjected to 10 independent experiments, and each ex-\nperiment is randomly divided into the training group and\nthe test group. Besides, other experimental parameters keep\nconsistent during training.\nThe evaluation criteria for each model are the overall\naccuracy rate ACC (i)and the precision rate of each category\nP(i), as deﬁned in the following:\nACC =Tc\nT(8)\nWhere Tis the total number of samples and Tcis the total\nnumber of samples with the correct prediction.\nP(i) =Tic\nTic+Tif(9)\nWhere Ticis the total number of samples correctly pre-\ndicted as class i, and Tifis the total number of samples\nthat are wrongly predicted as class i.P(i)can describe the\nstability and robustness of the model in small datasets.\nC. BASELINE METHODS\nThe input-level fusion strategy can be easily implemented\nusing mainstream 2D CNN and 3D CNN, so the selected\nnetwork includes Resnet34, Resnet152, 3D Resnet34, P3D\n[29], and C3D [30]. In feature-level fusion experiments, we\nused two common feature-level fusion methods [11], [12].\nSince these two papers focus on segmentation tasks, we\nmodify the network structure to adapt to the classiﬁcation\ntasks. The deep network used in the decision-level fusion\nexperiments is the same as the input-level strategy.\nVOLUME 4, 2016 5\n\nY . Dai et al. : TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nTABLE 1. Comparison on the parotid gland tumors dataset (average accuracy %and precision %for each disease. IF , FF and DF represent input-level fusion,\nfeature-level fusion and decision-level fusion, respectively.).\nMethod Dim Backbone Fusion Params TFlops Acc PA WT MT BCA OBL\nP3D 3D P3D IF 67M 0.31 76.1±5.5 59.9±23.1 84.3±5.3 69.7±19.0 71.4±7.3 78.0±14.0\nC3D 3D ConvNet IF 28M 1.36 71.0±4.1 68.3±38.9 81.3±15.4 67.8±10.0 71.4±7.5 84.5±12.4\nResnet34 2D Resnet34 IF 22M 0.02 69.9±4.0 81.0±11.0 77.6±7.4 61.4±10.8 53.8±15.5 68.1±7.9\nResnet152 2D Resnet152 IF 58M 0.05 69.0±3.5 50.5±18.0 74.1±10.1 64.3±20.0 62.9±8.9 75.2±11.4\n3D Resnet34 3D 3D Resnet34 IF 64M 1.08 73.3±5.1 69.2±16.2 86.8±5.2 75.3±18.1 68.7±13.8 65.7±6.4\n[11] 3D ConvNet FF 45M 3.47 74.2±2.9 76.0±24.8 86.2±10.0 72.9±16.3 75.2±24.6 80.0±15.2\n[12] 3D 3D Resnet34 FF 130M 1.35 73.3±2.4 46.2±13.2 78.4±7.9 70.2±15.5 69.8±15.8 79.0±10.7\n3D Resnet34 3D 3D Resnet34 DF 128M 1.28 72.1±3.5 64.7±14.4 81.5±9.3 66.8±8.4 69.6±8.9 72.1±14.9\nP3D 3D P3D DF 136M 0.42 74.8±4.6 50.5±20.0 85.1±4.4 70.5±20.2 69.5±8.7 73.4±14.3\nC3D 3D ConvNet DF 57M 1.45 71.0±3.3 58.3±33.3 70.7±9.3 74.0±8.5 78.9±20.0 73.2±6.7\nResnet34 2D Resnet34 DF 45M 0.03 71.3±4.5 72.7±21.7 75.3±8.1 72.5±10.3 60.9±16.8 70.3±9.7\nResnet152 2D Resnet152 DF 116M 0.06 72.2±5.5 63.5±18.3 75.6±10.4 73.4±18.7 83.2±16.3 69.6±11.5\nTransMed-T 2D Renset18+DeiT-T —— 17M 0.09 87.0±2.6 80.1±13.8 87.3±3.0 90.7±5.1 82.5±15.3 93.6±3.3\nTransMed-S 2D Resnet34+DeiT-S —— 43M 0.19 88.9±3.0 90.1±12.2 89.2±6.8 92.0±4.4 82.9±9.3 88.3±6.1\nTransMed-B 2D Resnet50+DeiT-B —— 110M 0.22 87.4±2.1 86.2±15.2 88.4±3.8 88.2±7.0 84.8±13.8 92.2±8.0\nTransMed-L 2D Resnet152+DeiT-B —— 145M 0.58 86.6±3.4 85.4±12.9 88.7±6.1 90.4±5.3 75.9±8.0 91.8±7.5\nD. RESULTS\nTable. 1 reports the performance of our proposed models, in\nwhich four variants are provided: the tiny version (TransMed-\nT) use ResNet18 and DeiT-Tiny (DeiT-T) as backbones\nfor CNN branch and transformer branch, respectively; the\nsmall version (TransMed-S) use ResNet34 and DeiT-Small\n(DeiT-S) as backbone; the base version (TransMed-B) use\nResNet50 and DeiT-Base (DeiT-B) as backbone; the large\nversion (TransMed-L) use ResNet152 and DeiT-B.\nTransMed consistently outperforms previous multi-modal\nfusion strategies by a large margin. TransFuse-S achieves on\naverage about 12.8 %improvement in terms of the average\naccuracy with respect to the P3D while the larger version\nTransMed-B and TransMed-L slightly suffer from overﬁtting\non the dataset. Table 1 also compares the number of parame-\nters and computational costs between our proposed models\nand previous methods. TransMed achieves state-of-the-art\nperformance with much fewer parameters and computational\ncosts. TransMed is highly efﬁcient as it models the long-\nrange relationship between modalities very well. We expect\nthat our method can inspire further exploration of multi-\nmodal medical image fusion in future work.\nE. ABLATION EXPERIMENTS\nTo demonstrate the effect of transformers in TransMed, we\nconducted ablation experiments. For TransMed, changing\nthe backbone from TransMed-T to TransMed-S results in\n1.9%improvement in average accuracy, at the expense of\na much larger computational cost. Therefore, considering\nthe computation cost, all experimental comparisons in this\npaper are conducted with TransMed-T to demonstrate the\neffectiveness of TransMed.In the experiment, TransMed’s CNN and transformers\nwere removed respectively, and all other conditions remained\nunchanged. The results are shown in Table 2. The results in-\ndicate that the transformer greatly improves the ability of the\ndeep model to explore the relationship between modalities\nwith little increase of parameters and computation. However,\nthe performance of the pure transformer structure is poor due\nto the small dataset.\nWe also explored the impact of different patch sizes on\nperformance in image serialization by changing Kvalues\nrespectively while other conditions remain unchanged. The\nresults are shown in Table 3. The experimental results show\nthat the performance is poor when the Kvalue is large. The\npossible reason is that too small image patches destroy the\nsemantic information of the image.\nV. CONCLUSION\nThe transformer is a powerful deep neural network structure\nfor processing sequences in NLP, but it has received little\nattention in medical image analysis. In this paper, we propose\nTransMed, which is a novel design of multi-modal medical\nimage classiﬁcation based on transformers. TransMed has\nachieved very competitive results in challenging parotid tu-\nmor classiﬁcation. TransMed is easy to implement and has a\nﬂexible structure, which can be extended to multiple medical\nimage modalities with low resource cost.\nThese preliminary results are encouraging, but there are\nstill many challenges. One is to apply TransMed to other\nmedical image analysis tasks, such as tumor segmentation\nand lesion detection. Another challenge is to use the pure\ntransformer structure. Pure transformer structure has been\nsuccessful in large-scale natural image datasets. However,\n6 VOLUME 4, 2016\n\nY . Dai et al. : TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\nTABLE 2. Ablation study on the effectiveness of CNN branch and transformer branch.\nModel Params TFlops Acc PA WT MT BCA OBL\nTransMed-T 17M 0.09 87.0±2.6 80.1±13.8 87.3±3.0 90.7±5.1 82.5±15.3 93.6±3.3\nw/o transformer 12M 0.01 71.3±2.5 71.6±17.8 78.9±3.9 73.9±12.5 66.9±13.1 69.9±13.3\nw/o CNN 5M 0.07 51.3±5.9 20.0±18.7 61.5±13.5 37.0±5.7 41.7±40.1 51.0±16.4\nTABLE 3. Ablation study on different patch sizes.\nModel K Acc PA WT MT BCA OBL\nTransMed-T 1 86.8±2.3 83.1±12.4 90.1±3.9 88.7±10.1 78.3±13.7 95.3±5.9\nTransMed-T 2 87.0±2.6 80.1±13.8 87.3±3.0 90.7±5.1 82.5±15.3 93.6±3.3\nTransMed-T 4 86.4±3.3 86.3±17.2 87.0±6.5 89.5±3.6 75.5±8.3 92.1±7.2\nTransMed-T 8 80.0±5.8 81.2±17.4 81.1±5.3 88.5±4.2 63.9±12.1 88.2±13.6\nTransMed-T 16 65.2±5.8 49.0±32.2 72.1±7.7 70.4±7.9 62.3±23.0 64.1±14.5\nour preliminary experiments show that there is still a big\ngap between the pure transformer and typical CNN in small\nmedical image datasets. We expect future work to further\nimprove TransMed.\nREFERENCES\n[1] Vaswani A et al., “ Attention is All you Need,” in Proc. Conf. Neural Inf.\nProcess. Syst. (NeurIPS) , 2017, pp 1-12.\n[2] Carion et al., “ End-to-end object detection with transformers,” in Proc.\nEur. Conf. Comput. Vis. (ECCV) , 2020, pp. 213-229.\n[3] S. Zheng et al., “ Rethinking Semantic Segmentation from a Sequence-\nto-Sequence Perspective with Transformers,” ArXiv , [Online]. Available:\nhttp://arxiv.org/abs/2012.15840.\n[4] A. Dosovitskiy et al., “ An Image is Worth 16x16 Words: Trans-\nformers for Image Recognition at Scale,” ArXiv , [Online]. Available:\nhttp://arxiv.org/abs/2010.11929.\n[5] H. Touvron et al., “ Training data-efﬁcient image transformers\n& distillation through attention,” ArXiv , [Online]. Available:\nhttp://arxiv.org/abs/2012.12877.\n[6] T. Zhou, S. Ruan, and S. Canu, “A review: Deep learning for medical\nimage segmentation using multi-modality fusion,” in Array. , vol. 3–4, p.\n100004, Sep. 2019, doi: 10.1016/j.array.2019.100004.\n[7] J. Deng et al., “ ImageNet: A large-scale hierarchical image database,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2009, pp.\n248–255, doi: 10.1109/CVPR.2009.5206848.\n[8] A. Myronenko et al., “ 3D MRI Brain Tumor Segmentation Using Autoen-\ncoder Regularization,” in Proc. Int. Conf. Med. Image Comput. Comput.-\nAssist. Intervent. Workshop(MICCAI) , 2019, vol. 11384, pp. 311–320, doi:\n10.1007/978-3-030-11726-9.\n[9] K. Kamnitsas et al., “ Efﬁcient multi-scale 3D CNN with fully connected\nCRF for accurate brain lesion segmentation,” in Med. Image Anal. , vol. 36,\npp. 61–78, Feb. 2017, doi: 10.1016/j.media.2016.10.004.\n[10] F. Isensee, P. Kickingereder, W. Wick, M. Bendszus, and K. H. Maier-\nHein, “Brain Tumor Segmentation and Radiomics Survival Prediction:\nContribution to the BRATS 2017 Challenge,” in Proc. Int. Conf. Med.\nImage Comput. Comput.-Assist. Intervent. Workshop(MICCAI) , 2018, pp.\n287–297, doi: 10.1007/978-3-319-75238-9.\n[11] J. Dolz, K. Gopinath, J. Yuan, H. Lombaert, C. Desrosiers, and I. Ben\nAyed, “HyperDense-Net: A Hyper-Densely Connected CNN for Multi-\nModal Image Segmentation,” in IEEE Trans. Med. Imaging , vol. 38, no.\n5, pp. 1116–1126, May 2019, doi: 10.1109/TMI.2018.2878669\n[12] D. Nie, L. Wang, Y . Gao, and D. Shen, “Fully convolutional networks for\nmulti-modality isointense infant brain image segmentation,” in 2016 IEEE\n13th International Symposium on Biomedical Imaging (ISBI) , Apr. 2016,\npp. 1342–1345, doi: 10.1109/ISBI.2016.7493515.\n[13] H. Chen et al., “ MMFNet: A multi-modality MRI fusion network for\nsegmentation of nasopharyngeal carcinoma,” in Neurocomputing. , 2020,\nvol. 394, pp. 27-40, doi: https://doi.org/10.1016/j.neucom.2020.02.002[14] Y . Shachor, H. Greenspan, and J. Goldberger, “A mixture of views network\nwith applications to multi-view medical imaging,” in Neurocomputing , vol.\n374, pp. 1–9, Jan. 2020, doi: 10.1016/j.neucom.2019.09.027.\n[15] K.-L. Tseng, Y .-L. Lin, W. Hsu, and C.-Y . Huang, “Joint Sequence Learn-\ning and Cross-Modality Convolution for 3D Biomedical Segmentation,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp.\n3739–3746, doi: 10.1109/CVPR.2017.398.\n[16] A. A. A. Setio et al., “ MMFNet: A multi-modality MRI fusion net-\nwork for segmentation of nasopharyngeal carcinoma,” in IEEE Trans.\nMed. Imaging. , vol. 35, no. 5, pp. 1160–1169, May 2016, doi:\n10.1109/TMI.2016.2536809.\n[17] Z. Guo, X. Li, H. Huang, N. Guo, and Q. Li, “Medical image seg-\nmentation based on multi-modal convolutional neural network: Study\non image fusion schemes,” in 2018 IEEE 15th International Sympo-\nsium on Biomedical Imaging (ISBI) , Apr. 2018, pp. 903–907, doi:\n10.1109/ISBI.2018.8363717.\n[18] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable DETR:\nDeformable Transformers for End-to-End Object Detection,” in Arxiv ,\n[Online]. Available: http://arxiv.org/abs/2010.04159.\n[19] Chen M et al., “ Generative pretraining from pixels,” in Int. Conf. on Mach.\nLearn. (ICML) , 2020, pp. 1691-1703.\n[20] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image\nRecognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,\nLas Vegas, NV , USA, Jun. 2016, pp. 770–778.\n[21] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-Local Neural Net-\nworks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , 2018,\npp. 7794-7803.\n[22] D. Hendrycks and K. Gimpel, “Gaussian Error Linear Units (GELUs),” in\nArxiv , [Online]. Available: http://arxiv.org/abs/1606.08415.\n[23] R. A. Lima et al., “ Clinical Prognostic Factors in Malignant Parotid Gland\nTumors,” in Otolaryngol. Neck Surg. , vol. 133, no. 5, pp. 702–708, Nov.\n2005.\n[24] V . Q. Joe and P. L. Westesson., “Tumors of the parotid gland: MR imaging\ncharacteristics of various histologic types,” in Am. J. Roentgenol. , vol. 163,\nno. 2, pp. 433–438, Aug. 1994.\n[25] J.-S. Jiang et al., “ Added value of susceptibility-weighted imaging to\ndiffusion-weighted imaging in the characterization of parotid gland tu-\nmors,” in Eur. Arch. Otorhinolaryngol. , ol. 277, no. 10, pp. 2839–2846,\nOct. 2020.\n[26] Otsu N., “A threshold selection method from gray-level histograms,” in\nIEEE Trans. Systems, Man, and Cybernetics. , vol. 9, no. 1, pp. 62-66,\n1979.\n[27] A. Paszke et al., “ PyTorch: An Imperative Style, High-\nPerformance Deep Learning Library,” ArXiv , [Online]. Available:\nhttp://arxiv.org/abs/1912.01703.\n[28] F. Pérez-García, R. Sparks, and S. Ourselin, “TorchIO: a Python library\nfor efﬁcient loading, preprocessing, augmentation and patch-based sam-\npling of medical images in deep learning,” in Arxiv , [Online]. Available:\nhttp://arxiv.org/abs/2003.04696.\nVOLUME 4, 2016 7\n\nY . Dai et al. : TransMed: Transformers Advance Multi-Modal Medical Image Classiﬁcation\n[29] Z. Qiu, T. Yao, and T. Mei, “Learning Spatio-Temporal Representation\nWith Pseudo-3D Residual Networks,” in Proc. IEEE Inter. Conf. Comput.\nVis. (ICCV) , 2017, pp. 5533-5541.\n[30] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learn-\ning Spatiotemporal Features with 3D Convolutional Networks,” in Proc.\nIEEE Inter. Conf. Comput. Vis. (ICCV) , Santiago, Chile, Dec. 2015, pp.\n4489–4497.\n8 VOLUME 4, 2016\n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\n \nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  \n \n \nA PREPRINT  \n \nPriyanto  Hidayatullaha*, Nurjannah Syakranib, Muhammad Rizqi Sholahuddinc,  \nTrisna Gelard, Refdinal Tubaguse \nabcd Computer Engineering and Informatics Department, Politeknik Negeri Bandung, Kab.  Bandung Barat, Indonesia  \ne Stunning Vision AI , Kota Cimahi , Indonesia  \npriyanto@polban.ac.ida, nurjannahsy@jtk.polban.ac.idb, muhammad.rizqi@polban.ac.idc, \ntrisna.gelar@polban.ac.idd, refdinal@stunningvisionai.come \n \n*Corresponding Author \n \nJanuary  23, 2025 \n \nABSTRACT  \n \nIn the field of deep learning -based computer vision, YOLO is revolutionary. With respect to deep \nlearning models, YOLO is also the one that is evolving the most rapidly.  Unfortunately, not every \nYOLO model possesses scholarly publications. Moreover, there exists a YOLO model that lacks a \npublicly accessible official architectural diagram. Naturally, this engenders challenges, such as \ncomplicating the understanding of how the model operates in practice. Furthermore, the review \narticles that are presently available do not delve into the specifics of each model. The objective of \nthis study is to present a comprehensive and in -depth architecture comparison of the four most \nrecent YOLO models, specifically YOLOv8 through YOLO11, thereby enabling readers to quickly \ngrasp not only how each model functions, but also the distinctions between them.  To analyze each \nYOLO version's architecture, we meticulously examined the relevant academic papers, \ndocumentation, and scrutinized the source code. The analysis reveals that while each version of \nYOLO has improvements in architecture and feature extraction, certain blocks remain unchanged. \nThe lack of scholarly publications and official diagrams presents challenges for understanding the \nmodel's functionality and future enhancement. Future developers are encouraged to provide these \nresources.  \nKeywords  YOLO Architecture  · Computer Vision  · Deep Learning  · Artificial Intelligence  \n \n1 Introduction  \nYOLO (You Only Look Once) is one of the state -of-the-art models in deep learning with the most development \nversions, from YOLOv1 in 2015 to 2024, it has been circulating and the 11th version of YOLO is in use.  \nNotwithstanding the success of YOLO adoption among researchers and developers, challenges persist in \ncomprehending its architecture and components. A significant obstacle is the absence of academic publications and \ncomprehensive architectural diagrams for certain YOLO versions, compelling researchers and practitioners to \ndepend on secondary sources and empirical analysis to understand these models. Moreover, the existing review \npapers frequently lack depth in architectural specifics and provide limited insights. These challenges underscore the \nnecessity for a thorough comparison to close the knowledge gap. A thorough architectural comparison is essential for \nunderstanding the functionality of each model and recognizing their distinct improvements.  \nThis study seeks to rectify the deficiencies by offering a comprehensive and systematic comparison of YOLOv8 to \nYOLO11. This study aims to elucidate the functioning of each model, thereby augmenting comprehension of \nYOLO's progression. This study advances the objective of promoting innovation and development in computer \nvision.   \n \n \n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n2  2 Literatur Review  \nThe YOLO series has evolved considerably since its inception, with each version presenting important enhancements \nand innovations aimed at improving performance and efficiency.  \n• YOLOv8 (2023) introduced an anchor -free detection approach, which simplifies the model architecture and \nenhances performance on small objects. The unified framework for multiple computer vision tasks enhances \nits applicability across various domains [1]. YOLOv8 has tasks that include object detection, instance \nsegmentation, and image classification with the contribution of anchor -free detection. It was first published \nby Ultralytics in 2023. The current YOLOv8 supports a wide variety of visual AI tasks and added new \nfeatures which are object tracking, pose estimation, and oriented bounding boxes [2]. \n• YOLOv9 introduced the PGI (Programmable Gradient Information) framework and GELAN (Generalized \nEfficient Layer Aggregation Network) architecture in 2024. They solve the problem of information \nbottlenecks and improve the accuracy of lightweight models [3]. \n• YOLOv10 (2024) is a big step forward because it uses NMS -free (non -maximum suppression) training, \nwhich makes it easier to deploy from start to finish and reduces the amount of computing needed [4]. Spatial -\nchannel decoupled downsampling and large -kernel convolutions have made performance and efficiency a lot \nbetter [4]. The dual assignments strategy for NMS -free training consistently enhanced accuracy, decreased \nlatency, and supported real -time object detection [4]. This model was developed by Tsinghua University \nresearchers.  \n• YOLO11, the latest YOLO model from Ultralytics, delivers superior performance in various computer vision \ntasks, including object detection, feature extraction, instance segmentation, pose estimation, tracking, and \nclassification. They also represent a major breakthrough in real -time object detection technology [5]. \nAccording to  [6], YOLOv11 adds the C3k2 block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA \n(Convolutional Block with Parallel Spatial Attention) components, which improve the ability to extract \nfeatures and find objects. Improving computer vision tasks like instance segmentation, pose estimation, and \noriented object detection has made it more useful in a wider range of situations [7]. \nNumerous review articles have examined the development and progress of YOLO architectures. [8] provides a \ncomprehensive analysis of the evolution of YOLO, focusing on versions 1 to 8 and the latest YOLO -NAS. They \nprovide a concise historical background, examine the speed -accuracy trade -off, and emphasize the significance of \napplication needs in selecting a YOLO version. Nonetheless, their research is limited to YOLOv8, and they do not \nthoroughly explore the architectural details of each iteration. [6] presents a comprehensive assessment encompassing \nversions 1 to 11, whereas [7] focus on versions 1 to 10, specifically highlighting the agriculture sector. [9] also \naddress versions 1 to 10, providing a succinct summary of the architectural progression. [10] explicitly analyzes \nYOLOv5, YOLOv8, and YOLOv10, whereas [5] concentrate solely on YOLOv11.  \n[5] give a thorough look at the YOLOv1 through YOLOv10 variations, focusing on how they can be used to make \nphotovoltaic (PV) defect detection better. As YOLO architectures get better over time, they carefully look at how \nthese changes affect quality control in the photovoltaic sector. There are programmable gradient information, path \naggregation networks, and generalized efficient layer aggregation topologies that are talked about in the paper as the \nmain things that made each variant work. A major trend in future research, according to the review, is to improve \nYOLO variations to cover a wider range of PV fault scenarios. Current discussions primarily concentrate on \nidentifying micro -cracks, but there is a recognized potential for further expansion. Attention processes in the YOLO \narchitecture will be looked into by researchers, who know that they could greatly improve detection skills, especially \nfor subtle and complicated defects.  \nNonetheless, these reviews often lack a thorough examination of the most recent YOLO versions (v8 to v11). [8] \ngive a thorough look at how YOLO has changed from the first version to YOLOv8. However, they neglect to \nexamine the intricacies of the subsequent architectures, which significantly enhance the design and training of \nnetworks. [7] also do not give a full architectural comparison of YOLOv10 with its predecessors or successors, \nwhich makes it harder to understand what makes it unique and what improvements it makes. [6] give a general \noverview of YOLOv11, but they don't go into detail about its architectural parts and how they affect performance.  \nBecause this study compares and contrasts YOLOv8 and YOLOv11 in a focused and thorough way, it fixes these \nproblems by focusing on the main differences and architectural changes between the two models. By looking at the \narchitectural details of each version and analyzing the source code, this study aims to make recent improvements \neasier to understand and use in real life.  \n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n3  3 Research Methods  \nTo accurately draw the architecture schematic, we read carefully the corresponding literature. However, only reading \nthe literature is not sufficient to understand comprehensively how the model works. Therefore, we deep dive into the \nsource code of every YOLO version. The .yaml file contains the overall architecture blocks whereas the Python \nsource code contains architecture blocks implementation which are in the form of Python modules.  \nWe clone the source code from every YOLO version GitHub repository. We analyzed the YOLO overall architecture \nof every YOLO version from the corresponding .yaml file which is located in the ultralytics/ cfg/models/<vX> folder \nwhere vX is the YOLO version. As for YOLO architecture blocks, we use conv.py, block.py, and head.py  which are \nlocated in the ultralytics/nn/modules  folder, except for YOLOv9. YOLOv9 .yaml file is located in the models/detect \nfolder whereas its architecture blocks are located in models/detect/common.py. Subsequently , we created the YOLO \noverall architecture and architecture blocks diagrams.  \n \n4 Results and Discussions  \nIn this section, we elaborate on every YOLO version’s overall architecture and its blocks based on the corresponding \npaper, source code, and technical documentation.  \n \n4.1 YOLO Overall Architecture  \nThe YOLO architecture generally consists of 3 parts: backbone, neck, and head. An essential part of the YOLO \narchitecture, the backbone is in charge of extracting features from the input image at multiple scales. Convolutional \nlayers and specialized blocks are stacked in this procedure to create feature maps at various resolutions. Features at \ndifferent scales are integrated into the neck and transmitted to the head for prediction. Usually, this procedure entails \nconcatenating and upsampling feature maps from many levels, allowing the model to effectively capture multi -scale \ndata. The head generates the final predictions for object detection and classification. Bounding boxes and class labels \nfor the objects in the image are finally output after processing the feature maps that were provided from the neck [5]. \nThe stem, downsampling layers, stages with fundamental building blocks, and head are the elements that constitute \nYOLO [4]. The first component of the network, known as the stem, is in charge of taking in and processing raw input \ndata, like pictures or videos. To improve computational efficiency and broaden the model's receptive field, the \ndownsampling layer has been assigned to lower the spatial resolution of the features. Each stage of YOLO's \narchitecture typically consists of multiple convolutional blocks known as basic building blocks or structures.  Usually, \neach step produces features at a higher level of abstraction and processes data from a different resolution.  \n \n \n4.1.1 YOLOv8  \nThe diagram is created based on yolov8.yaml which is located at ultralytics/cfg/models/v8 (Glenn Jocher & Paula \nDerrenger, 2023). The YOLOv8 variant is determined by three parameters: depth_multiple, width_multiple, and \nmax_channels. The depth_multiple parameters determine how many bottleneck blocks are in the C2f block. The \nwidth_multiple and max_channels parameters define the output channels. The stem component of YOLOv8 consists \nof two convolution blocks with stride 2 and kernel size 3. These two blocks convert data into initial features and \nreduce input resolution.  \nThe stage component in YOLOv8 uses the C2f block. There are 8 stages, which are blocks no. 2, 4, 6, 8, 12, 15, 18, \nand 21. Stages in the backbone (blocks no. 2, 4, 6, and 8) use shortcuts whereas the neck (blocks 12, 15, 18, and 21) \ndoes not use shortcuts. Using or not using shortcuts is guided by empirical results from trial and error experiments to \nfind the best [13]. YOLOv8's downsampling procedure makes use of a convolution block with stride 2 and kernel size \n3. When using stride 2, the output spatial resolution will be reduced by half.  \nSPPF (Spatial Pyramid Pooling Fast) block at the neck, right after the final block on the backbone, serves to provide a \nmulti -scale representation of the feature map. SPPF enables the model to capture features at various levels of \nabstraction by pooling at different scales [14]. There are some concat and upsample blocks on the neck. The feature \nmap's resolution is increased through upsampling. The nearest neighbor upsampling technique is used by YOLOv8. \nTo fill the newly created pixels in a larger feature map, this technique repeats the values of neighboring pixels. Concat \nis used for combining feature maps. The resolution remains unchanged, but the total number of channels will increase \nwhen feature maps are combined.  \n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n4   \nFigure 1: YOLOv8 Architecture (Adapted from [11], [12] ) \nOn YOLOv8, there are three heads. The first head, which is connected to block No. 15, is used to detect small objects. \nThe second head, which is connected to block No. 18, is used to detect medium objects. The third head, which is \nconnected to block No. 21, is used to detect large objects. As an important note, in every YOLO version, the object \nsize is relative to the image or video frame.  \n \n4.1.2 YOLOv9  \nThe diagram is created based on yolov9 -c.yaml which is located at models/detect/ folder [15]. In YOLOv9, there is \none additional section called the auxiliary. This section is used to generate reliable gradients and update network \nparameters. By providing additional information that connects the input data with the output target, the problem of \ninformation loss when passing through the layers of a deep learning network can be resolved [3]. This auxiliary \nsection is only used in the training process. This section can be eliminated during the inference process to speed up the \nmodel without sacrificing accuracy.  \nYOLOv9 starts with a Silence block. The input is simply returned by this block; no transformation operations are \ncarried out. This block connects the YOLOv9 input to the auxiliary and backbone sections. The stem component in \nYOLOv9 is two convolution blocks with kernel size 3 and stride 2. These two blocks convert data into initial features \nand reduce input resolution.  \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n5   \nFigure 2: YOLOv9 -c Architecture ( Adapted from  [16]) \nThe stage component in YOLOv9 uses the RepNCSPELAN4 block. RepNCSPELAN4 is an implementation of the \nGELAN concept in YOLOv9. GELAN combines CSPNet and ELAN (on YOLOv7) designed with gradient path \nplanning, taking into account lightweight, inference speed, and accuracy. GELAN generalizes ELAN capabilities. \nELAN only uses stacking of convolutional layers whereas GELAN can use various computing blocks such as residual \nblocks, or more complex blocks (C. -Y. Wang et al., 2024). There are 8 stages in YOLOv9 which are blocks 3, 5, 7, 9, \n13, 16, 19, and 22. The downsampling process in YOLOv9 uses the ADown block.  \nAt the neck, precisely after the last block on the backbone, there is a SPPELAN block. This block is the same as SPPF \n(Spatial Pyramid Pooling Fast) in YOLOv8. There are also some upsample and concat blocks. Just like in YOLOv8, \nYOLOv9 uses nearest neighbor upsampling.  \nThere are 3 heads in YOLOv9. The purpose of the first head, which is attached to block No. 16, is to detect small \nobjects relative to the frame or image. Detecting medium objects is done by the second head, which is attached to \nblock No. 19. For detecting large objects, the third head is used, which is connected to block No. 22.  \nThe auxiliary section provides additional information that links the input data to the target output. Therefore, this \nsection takes input from the Silence block and adds several blocks that are identical to those in the backbone which \nare two convolutions, RepNCSPELAN4, and ADown.  \nThere are three CBLinear blocks. These blocks are used to create different pyramidal feature maps and are used to \nobtain higher -level features from the first backbone. The CBFuse block is the main part that contains the reversible \nproperty, which combines the high -level features of the first backbone with the low -level features of the second \nbackbone.  \nBesides the auxiliary section, one novelty in YOLOv9 is additional heads. There are three additional heads. Heads \nconnected to blocks No. 31, No. 34, and No. 37 are used to detect small, medium, and large objects correspondingly.  \n \n \n4.1.3 YOLOv10  \nThe diagram is created based on yolov10l.yaml which is located in ultralytics/cfg/models/v10 folder [17]. YOLOv10 \narchitecture is built based on YOLOv8. Therefore, it has a similar stem component and parameters to YOLOv8 \nparameters. The difference is that depth_multiple parameters in YOLOv10 is used to define how many bottleneck \nblocks are in C2f block and how many CIB blocks are in the C2fCIB block.  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n6  Figure 3 shows the YOLOv10l architecture. The differences between the other variants lie in the use of the C2fCIB \nblock. Details of the C2fCIB block usage for each variant can be seen in the Table 1. \nYOLOv10's stage component uses the C2f or C2fCIB block. There are eight stages, which are blocks 2, 4, 6, 8, 13, \n16, 19, and 22. The concept of rank -guided block design, which aims to optimize the YOLO architecture by taking \ninto account the complexity of the basic blocks at each stage of the model based on intrinsic rank analysis, is present \nin YOLOv10. The intrinsic rank is a numerical value that indicates the number of significant singular values at the end \nof basic blocks. There is typically more redundancy in the larger scale models and the deeper stages of the YOLO \nmodel. CIB (Compact Inverted Block) will be used to replace the bottleneck block in C2f at high redundancy stages, \ntransforming it into C2fCIB (A. Wang et al., 2024). For instance, blocks 8, 13, 19, and 22 of the YOLOv10l variant, \nas shown in the above image, use C2fCIB. The Table 1 presents C2fCIB utilization in each YOLOv10 variant.  \n \nFigure 3: YOLOv10l Architecture (Adapted from [18]) \n \n \n \n \n \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n7  Table 1: C2fCIB block usage  \nModel Variant  C2fCIB  \nn 22 \ns 8, 22  \nm 8, 9, 12 \nb 8, 13, 19, 22  \nl 8, 13, 19, 22  \nx 6, 8, 13, 19, 22  \n \nA Spatial Pyramid Pooling Fast (SPPF) block, analogous to the YOLOv8 architecture, is positioned within the neck \nimmediately after the final module of the backbone. Following this, the Partial Self Attention (PSA) block is \nintroduced. PSA combines global modeling capabilities with a self -attention mechanism to enhance performance [4]. \nTo mitigate the excessive computational burden arising from the intricacies associated with quadratic self -attention, \nPSA is implemented solely after stage 4, utilizing the lowest resolution [4]. \nAs in YOLOv8, there exist several concat and upsample blocks within the neck. The upsample block employs the \nnearest neighbor upsampling as well. There are three heads. Heads connected to blocks No. 16, No. 19, and No. 22 are \nused to detect small, medium, and large objects correspondingly.  \n \n4.1.4 YOLO11  \nThe diagram is created based on yolo11.yaml which is located in ultralytics/cfg/models/11 folder [19]. In YOLO11 \nthere exist 3 variables that dictate the YOLO11 variant. These variables are identical to those in YOLOv8, specifically \ndepth_multiple, width_multiple, and max_channels. The difference is the depth_multiple variable ascertains the \nquantity of bottleneck or C3k blocks present in block C3k2 and PSA blocks within C2PSA. The width_multiple and \nmax_channels variables ascertain the output channels. The stem components of YOLO11 are congruent with those of \nYOLOv8.  \nThe stage component in YOLO11 employs the C3k2 block. C3k2 represents an advancement of C2f in YOLOv8 and \nYOLOv10. C3k2 possesses a reduced number of parameters in comparison to C2f. Within the C3k2 block, there \nexists a c3k parameter which is utilized to ascertain the implementation of the c3k module. If True, then it will employ \nthe C3k module. Conversely, if c3k is False, it will utilize Bottleneck as observed in C2f. However, in the variants m, \nl, and x, the value of c3k is consistently True. This finding is not available in the YOLO11 documentations [2]. \nInstead, we found it after analyzing the source code. Derived from this experience, examining the source code is \nessential to comprehend how the YOLO model works.  \nThere are eight stages, specifically blocks numbered 2, 4, 6, 8, 13, 16, 19, and 22. The downsampling procedure in \nYOLO11 employs a convolutional block with a kernel dimension of 3 and a stride of 2. When utilizing a stride of 2, \nthe resultant spatial resolution will be diminished by 50 percent.  \nOn the neck part, precisely after the terminal segment of the backbone, there exists an SPPF (Spatial Pyramid Pooling \nFast) block analogous to that in YOLOv8. Following this, there is the C2PSA block. Analogous to the PSA in \nYOLOv10, C2PSA employs a self -attention mechanism, which is utilized to enhance efficacy by combining global \nmodeling capabilities. C2PSA is exclusively positioned after stage 4 with minimal resolution. As in YOLOv8 and \nYOLOv10, there exist several concat and upsample blocks within the neck. The upsample block employs nearest -\nneighbor upsampling as well. The head part of YOLO11 is congruent with those of YOLO10, in terms of their \nfunctionality and block number.  \n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n8   \nFigure 4: YOLO11 Architecture ( Adapted from  [20]) \n \n4.2 YOLO Architecture Blocks  \nThe architecture blocks drawing of YOLOv8 is adapted from [11], [12]  whereas YOLOv9 to YOLO11 architecture \nblocks drawings are adapted from [16], [18], [20] . \n \n4.2.1 Input Image Resizing  \nIn contrast to YOLOv4 and its predecessors, the input image in YOLOv8 to YOLO11 will be resized while preserving \nthe image aspect ratio. To preserve the aspect ratio, the image will be padded with gray pixels. If the input image is a \nsquare, the image will be resized without padding. Figure 5 illustrates the example of the image resizing process.  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n9     \n(a) (b) (c) \n \nFigure 5: Image Resizing (a) Square Image (b) Landscape Image (c) Portrait Image  \n4.2.2 Convolutional (Conv) Block  \nIn all YOLO versions in this study, the convolutional block can be drawn in Figure 6. It contains a 2 -dimensional \nconvolutional layer, a 2 -dimensional batch normalization, and a SILU activation function. They are fused into a single \nconvolutional block. However, YOLO11 has two new convolutional blocks. The first one is a convolutional block \nwithout activation function and the second is the depthwise convolutional block (DWConv).  \nThe feed -forward network (FFN) in PSABlock (Position -Sensitive Attention Block) uses Conv block without \nactivation function. In depthwise convolution, only one kernel is used to operate on a single channel, which makes \nDWConv lighter. The matrix value of every kernel varies.  \n \nFigure 6: Standard recent YOLO convolutional block  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n10   \nFigure 7: Two new YOLO11 convolutional blocks  \n4.2.3 Down sampling  Block  \nBoth the YOLOv8 and YOLO11 versions employ typical 3×3 convolution with stride 2 for the downsampling \nprocess. However, this traditional method uses a lot of resources and may be less efficient due to its computing costs \nand relatively large number of parameters.  \nSpatial -Channel Decoupled Downsampling (SCDown), which separates spatial reduction and channel addition \noperations, is used in YOLOv10. First, the number of channels is altered without affecting the spatial resolution using \npointwise convolution (1×1). The spatial resolution is then decreased without altering the number of channels via \ndepthwise convolution (e.g. 3×3). This strategy reduces the computational cost and the number of parameters [4]. \nFigure 8 is the illustration of SCDown.  \n \nFigure 8: SCDown block in YOLOv10  \n \nYOLOv9 employs Adaptive Downsampling (ADown). This block uses a combination of Average Pooling and \nMaxpooling to perform downsampling. In ADown, the parameter count is comparatively lower, as pooling involves \nno parameters. [21]. Figure 9 is the illustration of ADown.  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n11   \nFigure 9: ADown block in YOLOv9  \n4.2.4 Bottleneck  \nThe bottleneck is a block that is similar to the Resnet block. This block is available in YOLOv8, YOLOv10, and \nYOLO11. Bottlenecks enable deeper networks by stacking additional layers with little increase in computing costs. \nSome  bottlenecks utilize shortcuts and those that do not , which are illustrated in Figure 10. \n \nFigure 10: Bottleneck blocks  \n \n4.2.5 C2f Block  \nC2f is a faster Implementation of CSP Bottleneck with 2 convolutions. C2f is utilized for feature extraction at all \nstages. C2f's modules can successfully learn multiscale features and broaden the range of receptive fields by utilizing \nfeature vector switching and multilayer convolution. C2f is available in YOLOv8 and YOLOv10. It is illustrated in \nFigure 11. \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n12   \nFigure 11: C2f Block  \n4.2.6 C2fCIB, CIB, RepVGGDW  \nBesides C2f, YOLOv10 introduced C2fCIB. YOLOv10 employs a rank -guided block design paradigm.  This is a \nstrategy for optimizing the YOLO model architecture by modifying the complexity of the fundamental blocks at each \nmodel stage using intrinsic rank analysis. Intrinsic rank is a numerical measure that counts the number of significant \nsingular values in the final convolution of a basic block.  \nThe YOLO model and other large -scale models tend to have greater redundancy as they progress deeper. At stages \nwith significant redundancy, the bottleneck block in C2f will be replaced with CIB (Compact Inverted Block) to form \nC2fCIB. For example, in the YOLOv10l version, C2fCIB is used in blocks 8, 13, 19, and 22. This strategy improves \nthe model efficiency, i.e., it increases the model training speed compared to YOLOv8 [4]. Figure 12 illustrates \nC2fCIB block.  \n \nFigure 12: C2fCIB block  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n13  CIB uses depthwise convolution. The block structure is depthwise convolution, convolution, depthwise convolution, \nconvolution, and depthwise convolution. Some  CIBs use shortcuts and some  do not. \n \nFigure 13: CIB block  \nThe concept of large -kernel convolution is presented in YOLOv10. By using this method, the input region that affects \nthe convolution layer's neurons' output can be expanded. Large -kernel convolution is used only in deep stages of the \nmodel. In its implementation, the second depthwise convolution in CIB is changed to RepVGGDW. RepVGGDW \ncontains two depthwise convolutional blocks with kernel sizes of three and seven. Because there is no gain when \napplied to larger versions, large -kernel convolution is only utilized on the Yolov10 n and s variants.  \n \nFigure 14: CIB block with RepVGGDW  \n \n4.2.7 C3k2 Block  \nThis block is newly introduced in YOLO11. C3k2 is the replacement for the C2f block in YOLOv8. C3k2 has a \nsimilar structure to C2f. Moreover, the first version C3k2 is congruent with C2f. The second version of C3k2 replaced \nthe bottlenecks with one C3k block as seen in Figure 15. \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n14   \nFigure 15: C3k2 blocks the first and second version  \nC3k itself is a block containing 3 convolution blocks and several bottlenecks. The bottleneck has two versions, one \nwith a shortcut (shortcut=true) and one without a shortcut (shortcut=false). The C3k2 block effectively captures more \ncomplex features, making it appropriate for objects of varying sizes. The illustration for C3k is in Figure 16. \n \nFigure 16: Bottleneck and C3k block  \n4.2.8 SPPF & SPPE LAN  \nSpatial Pyramid Pooling – Fast, or SPPF, constitutes a block that permits feature maps to be represented across \nmultiple scales. By pooling at varying scales, SPPF allows the model to capture features at different levels of \nabstraction. This block was first introduced in YOLOv8 and later adapted for YOLOv10 and YOLO11.  \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n15  SPPF represents a modification of SPP or Spatial Pyramid Pooling which is available in the YOLO model before \nYOLOv8. The Maxpool kernel size in SPP is 3, 5, and 9. In contrast, SPPF employs a kernel size of 5. This \nmodification reduces the quantity of floating -point operations (FLOP) in SPPF relative to SPP. Some of the YOLOv9 \nblocks exhibit analogous structure and function to SPPF. However, in YOLOv9, it is named SPPELAN. Figure 17 \nillustrates SPPF and SPPELAN block.  \n \nFigure 17: SPPF and SPPELAN Block  \n4.2.9 C2PSA  & PSABlock  \nThe attention module is employed to ascertain how the pixels within an image or video frame interrelate. The query, \nkey, and value constitute the three parameters that each pixel within the image retains. The dot product applied to all \nkeys from other pixels will be utilized to execute queries on a particular pixel. The probability that arises from the dot \nproduct will be employed to ascertain which pixels warrant attention. In other terms, the pixels that are observed are \nsimilar to these pixels.  \nAttention block is initially introduced in YOLOv10 and YOLOv11. In YOLO11, the attention process is represented \nas C2PSA. This block has two convolutional blocks positioned at the initial and the terminal. The outcome of the \ninitial convolutional block is divided. The first segment of the results will be processed by multiple PSABlocks to \nascertain which components necessitate attention. The outcomes from PSABlock will be combined, utilizing a concat \nblock, with the second segment of the results.  \nPSABlock contains an Attention block and FFN (Feed Forward Network).  FFN comprises two consecutive \nconvolutional blocks. Nevertheless, the second convolutional block does not incorporate an activation function.  \nYOLOv10 similarly possesses modules that serve to identify regions of the image that necessitate attention, referred \nto as PSA (Partial Self Attention). This block is analogous to C2PSA; however, it features a more straightforward \nstructure. It initiates with a convolutional block that bisects the results, with one half proceeding directly to the \nconcatenation block whereas the other half is processed by the attention block and the convolutional block. At the \nblock’s end, it employs a convolutional block to process the concatenated results.  \nThe PSA in YOLOv10 comprises a singular module featuring one attention layer and one feed -forward layer. C2PSA \nis an advanced module comprising multiple PSA blocks. C2PSA is more complex  than PSA as it iteratively performs \nthe PSA operation, thereby capturing more profound feature relationships. C2PSA efficiently captures deeper  features \nwithout additional time expenditure, as the initial split output convolution splits the input into two pathways: shortcut \nand deep processing.  \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n16   \nFigure 18: C2PSA and Attention Block  \n4.2.10  Detect Block  \nIn YOLOv8 and YOLOv9, the detect block comprises two components, specifically the classification head and the \nregression head. Class probabilities are generated by the classification head, whereas bounding box coordinate \npredictions are generated by the regression head. Each head is constituted of two convolutional blocks, succeeded by a \ntwo-dimensional convolution layer.  \nIn the YOLOv10 detect block, the classification head and regression head exhibit distinct architectures. This \ndifferentiation was necessitated by the disparity in computational costs. The classification head may be twofold larger \nthan the regression head.  \nNonetheless, the regression head plays a more pivotal role in ascertaining performance. Consequently, depthwise \nconvolutions may be employed to optimize the classification head architecture, rendering it more lightweight. With \nthis strategy, the number of model parameters is reduced. Figure 19 illustrates YOLOv8, YOLOv9 and YOLOv10 \nDetect block.  \nYOLOv10 introduces the concept of Consistent Dual Assignment for NMS -free Training. This concept integrates \none-to-many and one -to-one approaches in model training, utilizing consistent matching metrics. This strategy \nemploys two heads during training: one implements one -to-many assignment for robust supervision, whereas the other \nutilizes one -to-one matching. During inference, only the head utilizing one -to-one matching is employed, eliminating \nthe necessity for NMS and improving inference efficiency without sacrificing performance. [4]. \nThe detect block in YOLO11 uses depthwise convolutions in the classification head, which is nearly identical to \nYOLOv10. Nevertheless, YOLO11 incorporates a novel block, specifically DWConv. Figure 20 illustrates YOLO11 \nDetect block.  \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n17   \n \n(a) (b) \n \nFigure 19: (a) YOLOv8 and YOLOv9 Detect Block  (b) YOLOv10 Detect Block.  \n \nFigure 20: YOLO11 Detect Block  \nFrom the above comparison, we can draw a comprehensive comparison between the four YOLO versions as presented \nin Table 2. \nTable 2: YOLO Comprehensive Comparison  \nAspect  YOLOv8  YOLOv9  YOLOv10  YOLO11  \nNMS  NMS  NMS  NMS -free NMS  \nAnchor  Anchor -free Anchor -free Anchor -free Anchor -free \nNovelty  C2f \nSPPF  PGI \nGELAN  \nADown  SCDown  \nPSA \nConv(DepthWise)  C3k2  \nC2PSA  \nDWConv  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n18  4.3 An Illustration of How the Architecture Processes an Image  \nThis illustration utilizes YOLO11l for demonstration purposes. We selected this model because YOLO11 is the latest \nversion of YOLO, lacking an official architectural diagram and published paper. The l variant was selected due to its \nsimpler calculations compared to the others.  \nThe model is employed with a depth_multiple of 1, a width_multiple of 1, and a max_channels of 512 as presented in \nTable 3. The input image measures 640 x 640 pixels and comprises 3 channels. The initial input image is processed \nthrough Block 0, which is a convolutional block featuring a kernel size of 3 and a stride of 2. Applying a stride of 2 \nreduces the spatial resolution of the output to 320 x 320. The output channels are determined by multiplying the lesser \nvalue between the base output channels and max_channels by the width_multiple. In Block no. 0, the base output \nchannels total 64, resulting in output dimensions of 320 x 320 x 64. The process is illustrated in Figure 21. \n \nTable 3: Model Parameters  \nModel Variant  depth_multiple  width_multiple  max_channels  \nYOLO11l  1 1 512 \n \nFigure 21: Input and Block Conv No. 0  \nThe image is subsequently processed through Block 1, which is an additional convolutional block featuring a kernel \nsize of 3 and a stride of 2. The spatial resolution is further reduced to 160 x 160, and the number of channels is \nadjusted to 128. The output from Block No. 1 is transmitted to Block C3k2 (Block No. 2). In this block, deeper feature \nextraction is conducted. The output maintains a spatial resolution of 160 x 160, whereas the number of channels \nincreases to 256. The process is illustrated in Figure 22. \n \nFigure 22: Block Conv No.1 and C3k2 No. 2  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n19  This process continues gradually through subsequent blocks in the Backbone, where the spatial size is progressively \nreduced to 80 x 80 (Block no. 3), 40 x 40 (Block no. 5), and 20 x 20 (Block no. 7), according to the configuration of \nconvolutional layers and stride. Additionally, there are several C3k2 blocks (Block no. 4, 6, and 8) that are connected \nto the neck section. The process is illustrated in Figure 23. \n \nFigure 23: Block No. 3 - 8 \nThe output from the backbone, which is the output of Block C3k2 no. 8, is 20 x 20 x 512. This output is transmitted to \nthe neck, specifically to Block SPPF no. 9. The output remains 20 x 20 x 512. The SPPF enhances the efficiency of \nintegrating spatial information.  The process is illustrated in  Figure 24.  \n \nFigure 24: SPPF Block  \nSubsequently, it proceeds to Block C2PSA no. 9. This block is positioned exclusively at the lowest resolution stage. \nThis is implemented to mitigate excessive computation arising from the complexity of self -attention. The output from \nthis block is consistently 20 x 20 x 512.  The process is illustrated in  Figure 25. \n \nFigure 25: C2PSA Block  \nThe output from the C2PSA block will be upsampled to 40 x 40 x 512, and this feature will subsequently be \nconcatenated with the output from Block C3k2 no. 6 (40 x 40 x 512). In concatenation, the total number of channels is \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n20  aggregated, whereas  the resolution remains constant. The output from Concat (Block no. 12) is 40 x 40 x 1024. This \nprocedure of upsampling and concatenation executes the Feature Pyramid Network (FPN), which seeks to integrate \ninformation from features of varying resolutions and levels of abstraction.  The process is illustrated in Figure 26. \n \nFigure 26: Upsampling and Concat  \nThe process proceeds to Block C3k2 no. 13, yielding an output of 40 x 40 x 512. The output is subsequently \nupsampled to 80 x 80 x 512, and the feature is concatenated with the output from Block C3k2 no. 4 (80 x 80 x 512). \nThe output from Concat number 15 is 80 x 80 x 1024.  The process is illustrated in Figure 27. \n \nFigure 27: Concat Block No. 15  \n \nThe output from Concat no. 15 is transmitted to Block C3k2 no. 16, yielding a feature map of 80 x 80 x 512. The \noutput from this block serves as input for the Detect block, which is designed to identify relatively small objects \nwithin the image. The 80 x 80 dimensions are employed for identifying small objects, as the higher  spatial resolution \nretains more details from the original image.  The process is illustrated in Figure 28. \n \nFigure 28: Detect Block for Small Object  \nThe output from Block C3k2 no. 16 is subsequently transmitted to Block Conv no. 17. The spatial resolution \ndecreases to 40 x 40, whereas  the channel count remains at 256. Larger  objects typically occupy a broader area within \nthe image. Lowering the resolution allows for a more compact representation of the object's features, enhancing the \nmodel's capacity to capture  patterns at the correct scale.  The output from Block Conv no. 17 is concatenated with the \noutput from Block C3k2 no. 14, yielding a feature map of dimensions 40 x 40 x 768.  The process is illustrated in \nFigure 29. \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n21   \nFigure 29: Concat Block No. 18  \nThe output from Concat no. 18 is subsequently transmitted to Block C3k2 no. 19, resulting in a feature map of 40 x 40 \nx 512. The output from this block serves as input for the Detect block, which is specifically designed  to identify \nmedium -sized objects.  The process is illustrated in Figure 30. \n \nFigure 30: Detect Block for Medium Object  \nThe output from Block C3k2 no. 19 is subsequently processed through Block Conv no. 20, resulting in a spatial \nresolution of 20 x 20, while maintaining 512 channels. The output from Block Conv no. 20 is concatenated  with the \noutput from Block C2PSA no. 10, yielding a feature map of dimensions 20 x 20 x 1024.  The output from Concat no. \n21 is subsequently transmitted to Block C3k2 no. 22, resulting in a feature map of dimensions 20 x 20 x 512. The \noutput from this block serves as input for the Detect block, which is specifically designed  to identify large objects.  \nThe process is illustrated in Figure 31. \n \nFigure 31: Detect block for Big Object  \n \n5 Conclusion  \nOur comprehensive in -depth analysis revealed that each version exhibits enhancements and refinements in \narchitecture and feature extraction. However, we identified certain architectural blocks that are identical to the \nprevious version. The absence of scholarly publications and official architecture diagrams presents challenges for \nresearchers and practitioners in understanding the model's functionality. This issue also presents a challenge for future \nmodel enhancement. We also discovered that there is an evolutionary trend in YOLO, incorporating an attention layer \nbeginning with YOLOv10. We urge future YOLO developers to provide an architectural diagram and a scholarly \npublication to accelerate researchers, practitioners, and developers in comprehending the model and enhancing its \nperformance in the future.   \n \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n22  References  \n \n[1] Y. Chen, X. Yuan, R. Wu, J. Wang, Q. Hou, and M. -M. Cheng, “YOLO -MS: Rethinking Multi -Scale \nRepresentation Learning for Real -time Object Detection,” 2023, arXiv . doi: 10.48550/ARXIV.2308.05480.  \n[2] Glenn Jocher, Paula Derrenger, and Muhammad Rizwan Munawar, “Home - Ultralytics YOLO Docs.” Accessed: \nJan. 13, 2025. [Online]. Available: https://docs.ultralytics.com/  \n[3] C.-Y. Wang, I. -H. Yeh, and H. -Y. M. Liao, “YOLOv9: Learning What You Want to Learn Using Programmable \nGradient Information,” 2024, arXiv . doi: 10.48550/ARXIV.2402.13616.  \n[4] A. Wang et al. , “YOLOv10: Real -Time End -to-End Object Detection,” 2024, arXiv . doi: \n10.48550/ARXIV.2405.14458.  \n[5] R. Khanam and M. Hussain, “YOLOv11: An Overview of the Key Architectural Enhancements,” 2024, arXiv . \ndoi: 10.48550/ARXIV.2410.17725.  \n[6] R. Sapkota et al. , “YOLO11 to Its Genesis: A Decadal and Comprehensive Review of The You Only Look Once \n(YOLO) Series,” 2024, arXiv . doi: 10.48550/ARXIV.2406.19407.  \n[7] M. A. R. Alif and M. Hussain, “YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their \napplication in the agricultural domain,” 2024, arXiv . doi: 10.48550/ARXIV.2406.10139.  \n[8] J. Terven, D. -M. Córdova -Esparza, and J. -A. Romero -González, “A Comprehensive Review of YOLO \nArchitectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO -NAS,” MAKE , vol. 5, no. 4, pp. \n1680 –1716, Nov. 2023, doi: 10.3390/make5040083.  \n[9] C.-Y. Wang and H. -Y. M. Liao, “YOLOv1 to YOLOv10: The Fastest and Most Accurate Real -time Object \nDetection Systems,” SIP, vol. 13, no. 1, Art. no. 1, 2024, doi: 10.1561/116.20240058.  \n[10] M. Hussain, “YOLOv1 to v8: Unveiling Each Variant –A Comprehensive Review of YOLO,” IEEE Access , vol. \n12, pp. 42816 –42833, 2024, doi: 10.1109/ACCESS.2024.3378568.  \n[11] Priyanto Hidayatullah and Refdinal Tubagus, YOLOv8 Architecture Detailed Explanation - A Complete \nBreakdown , (Oct. 28, 2023). Accessed: Jan. 20, 2025. [Online Video]. Available: \nhttps://www.youtube.com/watch?v=HQXhDO7COj8  \n[12] RangeKing, “RangeKing,” GitHub. Accessed: Jul. 31, 2023. [Online]. Available: https://github.com/RangeKing  \n[13] Glenn Jocher, “shortcut in backbone and neck · Issue #1200 · ultralytics/ultralytics.” Accessed: Nov. 06, 2024. \n[Online]. Available: https://github.com/ultralytics/ultralytics/issues/1200#issuecomment -1454873251  \n[14] Glenn Jocher, “Understanding SPP and SPPF implementation · Issue #8785 · ultralytics/yolov5,” GitHub. \nAccessed: Nov. 06, 2024. [Online]. Available: https://github.com/ultralytics/yolov5/issues/8785  \n[15] Kin-Yiu, Wong, “yolov9 -c.yaml.” Accessed: Jan. 20, 2025. [Online]. Available: \nhttps://github.com/WongKinYiu/yolov9/blob/main/models/detect/yolov9 -c.yaml  \n[16] Priyanto Hidayatullah and Refdinal Tubagus, YOLOv9 Architecture Explained , (Apr. 19, 2024). Accessed: Jan. \n20, 2025. [Online Video]. Available: https://www.youtube.com/watch?v=oZ6I1VHpil0  \n[17] Wang Ao, “yolov10l.yaml.” Accessed: Jan. 20, 2025. [Online]. Available: https://github.com/THU -\nMIG/yolov10/blob/main/ultralytics/cfg/models/v10/yolov10l.yaml  \n[18] Priyanto Hidayatullah and Refdinal Tubagus, YOLOv10 Architecture Explained - A Complete Breakdown , (Sep. \n03, 2024). Accessed: Jan. 20, 2025. [Online Video]. Available: \nhttps://www.youtube.com/watch?v=A6rHMzRvs98  \n[19] Glenn Jocher and Paula Derrenger, “yolo11.yaml.” Accessed: Jan. 20, 2025. [Online]. Available: \nhttps://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/11/yolo11.yaml  \n[20] Priyanto Hidayatullah and Refdinal Tubagus, YOLO11 Architecture - Detailed Explanation , (Oct. 28, 2024). \nAccessed: Jan. 18, 2025. [Online Video]. Available: https://www.youtube.com/watch?v=L9Va7Y9UT8E  \n[21] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, Dive into Deep Learning . Cambridge University Press, 2023.  \n \n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
