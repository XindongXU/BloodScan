{"custom_id": "request-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息，\n并将其与用户正在开发的双模态YOLO算法进行比较分析。请以学术严谨的态度进行分析，客观指出文献中方法的优点和不足，\n并突出用户方法的创新点和优越性。你的分析将用于用户的开题报告文献综述部分，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\n \nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  \n \n \nA PREPRINT  \n \nPriyanto  Hidayatullaha*, Nurjannah Syakranib, Muhammad Rizqi Sholahuddinc,  \nTrisna Gelard, Refdinal Tubaguse \nabcd Computer Engineering and Informatics Department, Politeknik Negeri Bandung, Kab.  Bandung Barat, Indonesia  \ne Stunning Vision AI , Kota Cimahi , Indonesia  \npriyanto@polban.ac.ida, nurjannahsy@jtk.polban.ac.idb, muhammad.rizqi@polban.ac.idc, \ntrisna.gelar@polban.ac.idd, refdinal@stunningvisionai.come \n \n*Corresponding Author \n \nJanuary  23, 2025 \n \nABSTRACT  \n \nIn the field of deep learning -based computer vision, YOLO is revolutionary. With respect to deep \nlearning models, YOLO is also the one that is evolving the most rapidly.  Unfortunately, not every \nYOLO model possesses scholarly publications. Moreover, there exists a YOLO model that lacks a \npublicly accessible official architectural diagram. Naturally, this engenders challenges, such as \ncomplicating the understanding of how the model operates in practice. Furthermore, the review \narticles that are presently available do not delve into the specifics of each model. The objective of \nthis study is to present a comprehensive and in -depth architecture comparison of the four most \nrecent YOLO models, specifically YOLOv8 through YOLO11, thereby enabling readers to quickly \ngrasp not only how each model functions, but also the distinctions between them.  To analyze each \nYOLO version's architecture, we meticulously examined the relevant academic papers, \ndocumentation, and scrutinized the source code. The analysis reveals that while each version of \nYOLO has improvements in architecture and feature extraction, certain blocks remain unchanged. \nThe lack of scholarly publications and official diagrams presents challenges for understanding the \nmodel's functionality and future enhancement. Future developers are encouraged to provide these \nresources.  \nKeywords  YOLO Architecture  · Computer Vision  · Deep Learning  · Artificial Intelligence  \n \n1 Introduction  \nYOLO (You Only Look Once) is one of the state -of-the-art models in deep learning with the most development \nversions, from YOLOv1 in 2015 to 2024, it has been circulating and the 11th version of YOLO is in use.  \nNotwithstanding the success of YOLO adoption among researchers and developers, challenges persist in \ncomprehending its architecture and components. A significant obstacle is the absence of academic publications and \ncomprehensive architectural diagrams for certain YOLO versions, compelling researchers and practitioners to \ndepend on secondary sources and empirical analysis to understand these models. Moreover, the existing review \npapers frequently lack depth in architectural specifics and provide limited insights. These challenges underscore the \nnecessity for a thorough comparison to close the knowledge gap. A thorough architectural comparison is essential for \nunderstanding the functionality of each model and recognizing their distinct improvements.  \nThis study seeks to rectify the deficiencies by offering a comprehensive and systematic comparison of YOLOv8 to \nYOLO11. This study aims to elucidate the functioning of each model, thereby augmenting comprehension of \nYOLO's progression. This study advances the objective of promoting innovation and development in computer \nvision.   \n \n \n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n2  2 Literatur Review  \nThe YOLO series has evolved considerably since its inception, with each version presenting important enhancements \nand innovations aimed at improving performance and efficiency.  \n• YOLOv8 (2023) introduced an anchor -free detection approach, which simplifies the model architecture and \nenhances performance on small objects. The unified framework for multiple computer vision tasks enhances \nits applicability across various domains [1]. YOLOv8 has tasks that include object detection, instance \nsegmentation, and image classification with the contribution of anchor -free detection. It was first published \nby Ultralytics in 2023. The current YOLOv8 supports a wide variety of visual AI tasks and added new \nfeatures which are object tracking, pose estimation, and oriented bounding boxes [2]. \n• YOLOv9 introduced the PGI (Programmable Gradient Information) framework and GELAN (Generalized \nEfficient Layer Aggregation Network) architecture in 2024. They solve the problem of information \nbottlenecks and improve the accuracy of lightweight models [3]. \n• YOLOv10 (2024) is a big step forward because it uses NMS -free (non -maximum suppression) training, \nwhich makes it easier to deploy from start to finish and reduces the amount of computing needed [4]. Spatial -\nchannel decoupled downsampling and large -kernel convolutions have made performance and efficiency a lot \nbetter [4]. The dual assignments strategy for NMS -free training consistently enhanced accuracy, decreased \nlatency, and supported real -time object detection [4]. This model was developed by Tsinghua University \nresearchers.  \n• YOLO11, the latest YOLO model from Ultralytics, delivers superior performance in various computer vision \ntasks, including object detection, feature extraction, instance segmentation, pose estimation, tracking, and \nclassification. They also represent a major breakthrough in real -time object detection technology [5]. \nAccording to  [6], YOLOv11 adds the C3k2 block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA \n(Convolutional Block with Parallel Spatial Attention) components, which improve the ability to extract \nfeatures and find objects. Improving computer vision tasks like instance segmentation, pose estimation, and \noriented object detection has made it more useful in a wider range of situations [7]. \nNumerous review articles have examined the development and progress of YOLO architectures. [8] provides a \ncomprehensive analysis of the evolution of YOLO, focusing on versions 1 to 8 and the latest YOLO -NAS. They \nprovide a concise historical background, examine the speed -accuracy trade -off, and emphasize the significance of \napplication needs in selecting a YOLO version. Nonetheless, their research is limited to YOLOv8, and they do not \nthoroughly explore the architectural details of each iteration. [6] presents a comprehensive assessment encompassing \nversions 1 to 11, whereas [7] focus on versions 1 to 10, specifically highlighting the agriculture sector. [9] also \naddress versions 1 to 10, providing a succinct summary of the architectural progression. [10] explicitly analyzes \nYOLOv5, YOLOv8, and YOLOv10, whereas [5] concentrate solely on YOLOv11.  \n[5] give a thorough look at the YOLOv1 through YOLOv10 variations, focusing on how they can be used to make \nphotovoltaic (PV) defect detection better. As YOLO architectures get better over time, they carefully look at how \nthese changes affect quality control in the photovoltaic sector. There are programmable gradient information, path \naggregation networks, and generalized efficient layer aggregation topologies that are talked about in the paper as the \nmain things that made each variant work. A major trend in future research, according to the review, is to improve \nYOLO variations to cover a wider range of PV fault scenarios. Current discussions primarily concentrate on \nidentifying micro -cracks, but there is a recognized potential for further expansion. Attention processes in the YOLO \narchitecture will be looked into by researchers, who know that they could greatly improve detection skills, especially \nfor subtle and complicated defects.  \nNonetheless, these reviews often lack a thorough examination of the most recent YOLO versions (v8 to v11). [8] \ngive a thorough look at how YOLO has changed from the first version to YOLOv8. However, they neglect to \nexamine the intricacies of the subsequent architectures, which significantly enhance the design and training of \nnetworks. [7] also do not give a full architectural comparison of YOLOv10 with its predecessors or successors, \nwhich makes it harder to understand what makes it unique and what improvements it makes. [6] give a general \noverview of YOLOv11, but they don't go into detail about its architectural parts and how they affect performance.  \nBecause this study compares and contrasts YOLOv8 and YOLOv11 in a focused and thorough way, it fixes these \nproblems by focusing on the main differences and architectural changes between the two models. By looking at the \narchitectural details of each version and analyzing the source code, this study aims to make recent improvements \neasier to understand and use in real life.  \n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n3  3 Research Methods  \nTo accurately draw the architecture schematic, we read carefully the corresponding literature. However, only reading \nthe literature is not sufficient to understand comprehensively how the model works. Therefore, we deep dive into the \nsource code of every YOLO version. The .yaml file contains the overall architecture blocks whereas the Python \nsource code contains architecture blocks implementation which are in the form of Python modules.  \nWe clone the source code from every YOLO version GitHub repository. We analyzed the YOLO overall architecture \nof every YOLO version from the corresponding .yaml file which is located in the ultralytics/ cfg/models/<vX> folder \nwhere vX is the YOLO version. As for YOLO architecture blocks, we use conv.py, block.py, and head.py  which are \nlocated in the ultralytics/nn/modules  folder, except for YOLOv9. YOLOv9 .yaml file is located in the models/detect \nfolder whereas its architecture blocks are located in models/detect/common.py. Subsequently , we created the YOLO \noverall architecture and architecture blocks diagrams.  \n \n4 Results and Discussions  \nIn this section, we elaborate on every YOLO version’s overall architecture and its blocks based on the corresponding \npaper, source code, and technical documentation.  \n \n4.1 YOLO Overall Architecture  \nThe YOLO architecture generally consists of 3 parts: backbone, neck, and head. An essential part of the YOLO \narchitecture, the backbone is in charge of extracting features from the input image at multiple scales. Convolutional \nlayers and specialized blocks are stacked in this procedure to create feature maps at various resolutions. Features at \ndifferent scales are integrated into the neck and transmitted to the head for prediction. Usually, this procedure entails \nconcatenating and upsampling feature maps from many levels, allowing the model to effectively capture multi -scale \ndata. The head generates the final predictions for object detection and classification. Bounding boxes and class labels \nfor the objects in the image are finally output after processing the feature maps that were provided from the neck [5]. \nThe stem, downsampling layers, stages with fundamental building blocks, and head are the elements that constitute \nYOLO [4]. The first component of the network, known as the stem, is in charge of taking in and processing raw input \ndata, like pictures or videos. To improve computational efficiency and broaden the model's receptive field, the \ndownsampling layer has been assigned to lower the spatial resolution of the features. Each stage of YOLO's \narchitecture typically consists of multiple convolutional blocks known as basic building blocks or structures.  Usually, \neach step produces features at a higher level of abstraction and processes data from a different resolution.  \n \n \n4.1.1 YOLOv8  \nThe diagram is created based on yolov8.yaml which is located at ultralytics/cfg/models/v8 (Glenn Jocher & Paula \nDerrenger, 2023). The YOLOv8 variant is determined by three parameters: depth_multiple, width_multiple, and \nmax_channels. The depth_multiple parameters determine how many bottleneck blocks are in the C2f block. The \nwidth_multiple and max_channels parameters define the output channels. The stem component of YOLOv8 consists \nof two convolution blocks with stride 2 and kernel size 3. These two blocks convert data into initial features and \nreduce input resolution.  \nThe stage component in YOLOv8 uses the C2f block. There are 8 stages, which are blocks no. 2, 4, 6, 8, 12, 15, 18, \nand 21. Stages in the backbone (blocks no. 2, 4, 6, and 8) use shortcuts whereas the neck (blocks 12, 15, 18, and 21) \ndoes not use shortcuts. Using or not using shortcuts is guided by empirical results from trial and error experiments to \nfind the best [13]. YOLOv8's downsampling procedure makes use of a convolution block with stride 2 and kernel size \n3. When using stride 2, the output spatial resolution will be reduced by half.  \nSPPF (Spatial Pyramid Pooling Fast) block at the neck, right after the final block on the backbone, serves to provide a \nmulti -scale representation of the feature map. SPPF enables the model to capture features at various levels of \nabstraction by pooling at different scales [14]. There are some concat and upsample blocks on the neck. The feature \nmap's resolution is increased through upsampling. The nearest neighbor upsampling technique is used by YOLOv8. \nTo fill the newly created pixels in a larger feature map, this technique repeats the values of neighboring pixels. Concat \nis used for combining feature maps. The resolution remains unchanged, but the total number of channels will increase \nwhen feature maps are combined.  \n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n4   \nFigure 1: YOLOv8 Architecture (Adapted from [11], [12] ) \nOn YOLOv8, there are three heads. The first head, which is connected to block No. 15, is used to detect small objects. \nThe second head, which is connected to block No. 18, is used to detect medium objects. The third head, which is \nconnected to block No. 21, is used to detect large objects. As an important note, in every YOLO version, the object \nsize is relative to the image or video frame.  \n \n4.1.2 YOLOv9  \nThe diagram is created based on yolov9 -c.yaml which is located at models/detect/ folder [15]. In YOLOv9, there is \none additional section called the auxiliary. This section is used to generate reliable gradients and update network \nparameters. By providing additional information that connects the input data with the output target, the problem of \ninformation loss when passing through the layers of a deep learning network can be resolved [3]. This auxiliary \nsection is only used in the training process. This section can be eliminated during the inference process to speed up the \nmodel without sacrificing accuracy.  \nYOLOv9 starts with a Silence block. The input is simply returned by this block; no transformation operations are \ncarried out. This block connects the YOLOv9 input to the auxiliary and backbone sections. The stem component in \nYOLOv9 is two convolution blocks with kernel size 3 and stride 2. These two blocks convert data into initial features \nand reduce input resolution.  \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n5   \nFigure 2: YOLOv9 -c Architecture ( Adapted from  [16]) \nThe stage component in YOLOv9 uses the RepNCSPELAN4 block. RepNCSPELAN4 is an implementation of the \nGELAN concept in YOLOv9. GELAN combines CSPNet and ELAN (on YOLOv7) designed with gradient path \nplanning, taking into account lightweight, inference speed, and accuracy. GELAN generalizes ELAN capabilities. \nELAN only uses stacking of convolutional layers whereas GELAN can use various computing blocks such as residual \nblocks, or more complex blocks (C. -Y. Wang et al., 2024). There are 8 stages in YOLOv9 which are blocks 3, 5, 7, 9, \n13, 16, 19, and 22. The downsampling process in YOLOv9 uses the ADown block.  \nAt the neck, precisely after the last block on the backbone, there is a SPPELAN block. This block is the same as SPPF \n(Spatial Pyramid Pooling Fast) in YOLOv8. There are also some upsample and concat blocks. Just like in YOLOv8, \nYOLOv9 uses nearest neighbor upsampling.  \nThere are 3 heads in YOLOv9. The purpose of the first head, which is attached to block No. 16, is to detect small \nobjects relative to the frame or image. Detecting medium objects is done by the second head, which is attached to \nblock No. 19. For detecting large objects, the third head is used, which is connected to block No. 22.  \nThe auxiliary section provides additional information that links the input data to the target output. Therefore, this \nsection takes input from the Silence block and adds several blocks that are identical to those in the backbone which \nare two convolutions, RepNCSPELAN4, and ADown.  \nThere are three CBLinear blocks. These blocks are used to create different pyramidal feature maps and are used to \nobtain higher -level features from the first backbone. The CBFuse block is the main part that contains the reversible \nproperty, which combines the high -level features of the first backbone with the low -level features of the second \nbackbone.  \nBesides the auxiliary section, one novelty in YOLOv9 is additional heads. There are three additional heads. Heads \nconnected to blocks No. 31, No. 34, and No. 37 are used to detect small, medium, and large objects correspondingly.  \n \n \n4.1.3 YOLOv10  \nThe diagram is created based on yolov10l.yaml which is located in ultralytics/cfg/models/v10 folder [17]. YOLOv10 \narchitecture is built based on YOLOv8. Therefore, it has a similar stem component and parameters to YOLOv8 \nparameters. The difference is that depth_multiple parameters in YOLOv10 is used to define how many bottleneck \nblocks are in C2f block and how many CIB blocks are in the C2fCIB block.  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n6  Figure 3 shows the YOLOv10l architecture. The differences between the other variants lie in the use of the C2fCIB \nblock. Details of the C2fCIB block usage for each variant can be seen in the Table 1. \nYOLOv10's stage component uses the C2f or C2fCIB block. There are eight stages, which are blocks 2, 4, 6, 8, 13, \n16, 19, and 22. The concept of rank -guided block design, which aims to optimize the YOLO architecture by taking \ninto account the complexity of the basic blocks at each stage of the model based on intrinsic rank analysis, is present \nin YOLOv10. The intrinsic rank is a numerical value that indicates the number of significant singular values at the end \nof basic blocks. There is typically more redundancy in the larger scale models and the deeper stages of the YOLO \nmodel. CIB (Compact Inverted Block) will be used to replace the bottleneck block in C2f at high redundancy stages, \ntransforming it into C2fCIB (A. Wang et al., 2024). For instance, blocks 8, 13, 19, and 22 of the YOLOv10l variant, \nas shown in the above image, use C2fCIB. The Table 1 presents C2fCIB utilization in each YOLOv10 variant.  \n \nFigure 3: YOLOv10l Architecture (Adapted from [18]) \n \n \n \n \n \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n7  Table 1: C2fCIB block usage  \nModel Variant  C2fCIB  \nn 22 \ns 8, 22  \nm 8, 9, 12 \nb 8, 13, 19, 22  \nl 8, 13, 19, 22  \nx 6, 8, 13, 19, 22  \n \nA Spatial Pyramid Pooling Fast (SPPF) block, analogous to the YOLOv8 architecture, is positioned within the neck \nimmediately after the final module of the backbone. Following this, the Partial Self Attention (PSA) block is \nintroduced. PSA combines global modeling capabilities with a self -attention mechanism to enhance performance [4]. \nTo mitigate the excessive computational burden arising from the intricacies associated with quadratic self -attention, \nPSA is implemented solely after stage 4, utilizing the lowest resolution [4]. \nAs in YOLOv8, there exist several concat and upsample blocks within the neck. The upsample block employs the \nnearest neighbor upsampling as well. There are three heads. Heads connected to blocks No. 16, No. 19, and No. 22 are \nused to detect small, medium, and large objects correspondingly.  \n \n4.1.4 YOLO11  \nThe diagram is created based on yolo11.yaml which is located in ultralytics/cfg/models/11 folder [19]. In YOLO11 \nthere exist 3 variables that dictate the YOLO11 variant. These variables are identical to those in YOLOv8, specifically \ndepth_multiple, width_multiple, and max_channels. The difference is the depth_multiple variable ascertains the \nquantity of bottleneck or C3k blocks present in block C3k2 and PSA blocks within C2PSA. The width_multiple and \nmax_channels variables ascertain the output channels. The stem components of YOLO11 are congruent with those of \nYOLOv8.  \nThe stage component in YOLO11 employs the C3k2 block. C3k2 represents an advancement of C2f in YOLOv8 and \nYOLOv10. C3k2 possesses a reduced number of parameters in comparison to C2f. Within the C3k2 block, there \nexists a c3k parameter which is utilized to ascertain the implementation of the c3k module. If True, then it will employ \nthe C3k module. Conversely, if c3k is False, it will utilize Bottleneck as observed in C2f. However, in the variants m, \nl, and x, the value of c3k is consistently True. This finding is not available in the YOLO11 documentations [2]. \nInstead, we found it after analyzing the source code. Derived from this experience, examining the source code is \nessential to comprehend how the YOLO model works.  \nThere are eight stages, specifically blocks numbered 2, 4, 6, 8, 13, 16, 19, and 22. The downsampling procedure in \nYOLO11 employs a convolutional block with a kernel dimension of 3 and a stride of 2. When utilizing a stride of 2, \nthe resultant spatial resolution will be diminished by 50 percent.  \nOn the neck part, precisely after the terminal segment of the backbone, there exists an SPPF (Spatial Pyramid Pooling \nFast) block analogous to that in YOLOv8. Following this, there is the C2PSA block. Analogous to the PSA in \nYOLOv10, C2PSA employs a self -attention mechanism, which is utilized to enhance efficacy by combining global \nmodeling capabilities. C2PSA is exclusively positioned after stage 4 with minimal resolution. As in YOLOv8 and \nYOLOv10, there exist several concat and upsample blocks within the neck. The upsample block employs nearest -\nneighbor upsampling as well. The head part of YOLO11 is congruent with those of YOLO10, in terms of their \nfunctionality and block number.  \n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n8   \nFigure 4: YOLO11 Architecture ( Adapted from  [20]) \n \n4.2 YOLO Architecture Blocks  \nThe architecture blocks drawing of YOLOv8 is adapted from [11], [12]  whereas YOLOv9 to YOLO11 architecture \nblocks drawings are adapted from [16], [18], [20] . \n \n4.2.1 Input Image Resizing  \nIn contrast to YOLOv4 and its predecessors, the input image in YOLOv8 to YOLO11 will be resized while preserving \nthe image aspect ratio. To preserve the aspect ratio, the image will be padded with gray pixels. If the input image is a \nsquare, the image will be resized without padding. Figure 5 illustrates the example of the image resizing process.  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n9     \n(a) (b) (c) \n \nFigure 5: Image Resizing (a) Square Image (b) Landscape Image (c) Portrait Image  \n4.2.2 Convolutional (Conv) Block  \nIn all YOLO versions in this study, the convolutional block can be drawn in Figure 6. It contains a 2 -dimensional \nconvolutional layer, a 2 -dimensional batch normalization, and a SILU activation function. They are fused into a single \nconvolutional block. However, YOLO11 has two new convolutional blocks. The first one is a convolutional block \nwithout activation function and the second is the depthwise convolutional block (DWConv).  \nThe feed -forward network (FFN) in PSABlock (Position -Sensitive Attention Block) uses Conv block without \nactivation function. In depthwise convolution, only one kernel is used to operate on a single channel, which makes \nDWConv lighter. The matrix value of every kernel varies.  \n \nFigure 6: Standard recent YOLO convolutional block  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n10   \nFigure 7: Two new YOLO11 convolutional blocks  \n4.2.3 Down sampling  Block  \nBoth the YOLOv8 and YOLO11 versions employ typical 3×3 convolution with stride 2 for the downsampling \nprocess. However, this traditional method uses a lot of resources and may be less efficient due to its computing costs \nand relatively large number of parameters.  \nSpatial -Channel Decoupled Downsampling (SCDown), which separates spatial reduction and channel addition \noperations, is used in YOLOv10. First, the number of channels is altered without affecting the spatial resolution using \npointwise convolution (1×1). The spatial resolution is then decreased without altering the number of channels via \ndepthwise convolution (e.g. 3×3). This strategy reduces the computational cost and the number of parameters [4]. \nFigure 8 is the illustration of SCDown.  \n \nFigure 8: SCDown block in YOLOv10  \n \nYOLOv9 employs Adaptive Downsampling (ADown). This block uses a combination of Average Pooling and \nMaxpooling to perform downsampling. In ADown, the parameter count is comparatively lower, as pooling involves \nno parameters. [21]. Figure 9 is the illustration of ADown.  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n11   \nFigure 9: ADown block in YOLOv9  \n4.2.4 Bottleneck  \nThe bottleneck is a block that is similar to the Resnet block. This block is available in YOLOv8, YOLOv10, and \nYOLO11. Bottlenecks enable deeper networks by stacking additional layers with little increase in computing costs. \nSome  bottlenecks utilize shortcuts and those that do not , which are illustrated in Figure 10. \n \nFigure 10: Bottleneck blocks  \n \n4.2.5 C2f Block  \nC2f is a faster Implementation of CSP Bottleneck with 2 convolutions. C2f is utilized for feature extraction at all \nstages. C2f's modules can successfully learn multiscale features and broaden the range of receptive fields by utilizing \nfeature vector switching and multilayer convolution. C2f is available in YOLOv8 and YOLOv10. It is illustrated in \nFigure 11. \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n12   \nFigure 11: C2f Block  \n4.2.6 C2fCIB, CIB, RepVGGDW  \nBesides C2f, YOLOv10 introduced C2fCIB. YOLOv10 employs a rank -guided block design paradigm.  This is a \nstrategy for optimizing the YOLO model architecture by modifying the complexity of the fundamental blocks at each \nmodel stage using intrinsic rank analysis. Intrinsic rank is a numerical measure that counts the number of significant \nsingular values in the final convolution of a basic block.  \nThe YOLO model and other large -scale models tend to have greater redundancy as they progress deeper. At stages \nwith significant redundancy, the bottleneck block in C2f will be replaced with CIB (Compact Inverted Block) to form \nC2fCIB. For example, in the YOLOv10l version, C2fCIB is used in blocks 8, 13, 19, and 22. This strategy improves \nthe model efficiency, i.e., it increases the model training speed compared to YOLOv8 [4]. Figure 12 illustrates \nC2fCIB block.  \n \nFigure 12: C2fCIB block  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n13  CIB uses depthwise convolution. The block structure is depthwise convolution, convolution, depthwise convolution, \nconvolution, and depthwise convolution. Some  CIBs use shortcuts and some  do not. \n \nFigure 13: CIB block  \nThe concept of large -kernel convolution is presented in YOLOv10. By using this method, the input region that affects \nthe convolution layer's neurons' output can be expanded. Large -kernel convolution is used only in deep stages of the \nmodel. In its implementation, the second depthwise convolution in CIB is changed to RepVGGDW. RepVGGDW \ncontains two depthwise convolutional blocks with kernel sizes of three and seven. Because there is no gain when \napplied to larger versions, large -kernel convolution is only utilized on the Yolov10 n and s variants.  \n \nFigure 14: CIB block with RepVGGDW  \n \n4.2.7 C3k2 Block  \nThis block is newly introduced in YOLO11. C3k2 is the replacement for the C2f block in YOLOv8. C3k2 has a \nsimilar structure to C2f. Moreover, the first version C3k2 is congruent with C2f. The second version of C3k2 replaced \nthe bottlenecks with one C3k block as seen in Figure 15. \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n14   \nFigure 15: C3k2 blocks the first and second version  \nC3k itself is a block containing 3 convolution blocks and several bottlenecks. The bottleneck has two versions, one \nwith a shortcut (shortcut=true) and one without a shortcut (shortcut=false). The C3k2 block effectively captures more \ncomplex features, making it appropriate for objects of varying sizes. The illustration for C3k is in Figure 16. \n \nFigure 16: Bottleneck and C3k block  \n4.2.8 SPPF & SPPE LAN  \nSpatial Pyramid Pooling – Fast, or SPPF, constitutes a block that permits feature maps to be represented across \nmultiple scales. By pooling at varying scales, SPPF allows the model to capture features at different levels of \nabstraction. This block was first introduced in YOLOv8 and later adapted for YOLOv10 and YOLO11.  \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n15  SPPF represents a modification of SPP or Spatial Pyramid Pooling which is available in the YOLO model before \nYOLOv8. The Maxpool kernel size in SPP is 3, 5, and 9. In contrast, SPPF employs a kernel size of 5. This \nmodification reduces the quantity of floating -point operations (FLOP) in SPPF relative to SPP. Some of the YOLOv9 \nblocks exhibit analogous structure and function to SPPF. However, in YOLOv9, it is named SPPELAN. Figure 17 \nillustrates SPPF and SPPELAN block.  \n \nFigure 17: SPPF and SPPELAN Block  \n4.2.9 C2PSA  & PSABlock  \nThe attention module is employed to ascertain how the pixels within an image or video frame interrelate. The query, \nkey, and value constitute the three parameters that each pixel within the image retains. The dot product applied to all \nkeys from other pixels will be utilized to execute queries on a particular pixel. The probability that arises from the dot \nproduct will be employed to ascertain which pixels warrant attention. In other terms, the pixels that are observed are \nsimilar to these pixels.  \nAttention block is initially introduced in YOLOv10 and YOLOv11. In YOLO11, the attention process is represented \nas C2PSA. This block has two convolutional blocks positioned at the initial and the terminal. The outcome of the \ninitial convolutional block is divided. The first segment of the results will be processed by multiple PSABlocks to \nascertain which components necessitate attention. The outcomes from PSABlock will be combined, utilizing a concat \nblock, with the second segment of the results.  \nPSABlock contains an Attention block and FFN (Feed Forward Network).  FFN comprises two consecutive \nconvolutional blocks. Nevertheless, the second convolutional block does not incorporate an activation function.  \nYOLOv10 similarly possesses modules that serve to identify regions of the image that necessitate attention, referred \nto as PSA (Partial Self Attention). This block is analogous to C2PSA; however, it features a more straightforward \nstructure. It initiates with a convolutional block that bisects the results, with one half proceeding directly to the \nconcatenation block whereas the other half is processed by the attention block and the convolutional block. At the \nblock’s end, it employs a convolutional block to process the concatenated results.  \nThe PSA in YOLOv10 comprises a singular module featuring one attention layer and one feed -forward layer. C2PSA \nis an advanced module comprising multiple PSA blocks. C2PSA is more complex  than PSA as it iteratively performs \nthe PSA operation, thereby capturing more profound feature relationships. C2PSA efficiently captures deeper  features \nwithout additional time expenditure, as the initial split output convolution splits the input into two pathways: shortcut \nand deep processing.  \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n16   \nFigure 18: C2PSA and Attention Block  \n4.2.10  Detect Block  \nIn YOLOv8 and YOLOv9, the detect block comprises two components, specifically the classification head and the \nregression head. Class probabilities are generated by the classification head, whereas bounding box coordinate \npredictions are generated by the regression head. Each head is constituted of two convolutional blocks, succeeded by a \ntwo-dimensional convolution layer.  \nIn the YOLOv10 detect block, the classification head and regression head exhibit distinct architectures. This \ndifferentiation was necessitated by the disparity in computational costs. The classification head may be twofold larger \nthan the regression head.  \nNonetheless, the regression head plays a more pivotal role in ascertaining performance. Consequently, depthwise \nconvolutions may be employed to optimize the classification head architecture, rendering it more lightweight. With \nthis strategy, the number of model parameters is reduced. Figure 19 illustrates YOLOv8, YOLOv9 and YOLOv10 \nDetect block.  \nYOLOv10 introduces the concept of Consistent Dual Assignment for NMS -free Training. This concept integrates \none-to-many and one -to-one approaches in model training, utilizing consistent matching metrics. This strategy \nemploys two heads during training: one implements one -to-many assignment for robust supervision, whereas the other \nutilizes one -to-one matching. During inference, only the head utilizing one -to-one matching is employed, eliminating \nthe necessity for NMS and improving inference efficiency without sacrificing performance. [4]. \nThe detect block in YOLO11 uses depthwise convolutions in the classification head, which is nearly identical to \nYOLOv10. Nevertheless, YOLO11 incorporates a novel block, specifically DWConv. Figure 20 illustrates YOLO11 \nDetect block.  \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n17   \n \n(a) (b) \n \nFigure 19: (a) YOLOv8 and YOLOv9 Detect Block  (b) YOLOv10 Detect Block.  \n \nFigure 20: YOLO11 Detect Block  \nFrom the above comparison, we can draw a comprehensive comparison between the four YOLO versions as presented \nin Table 2. \nTable 2: YOLO Comprehensive Comparison  \nAspect  YOLOv8  YOLOv9  YOLOv10  YOLO11  \nNMS  NMS  NMS  NMS -free NMS  \nAnchor  Anchor -free Anchor -free Anchor -free Anchor -free \nNovelty  C2f \nSPPF  PGI \nGELAN  \nADown  SCDown  \nPSA \nConv(DepthWise)  C3k2  \nC2PSA  \nDWConv  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n18  4.3 An Illustration of How the Architecture Processes an Image  \nThis illustration utilizes YOLO11l for demonstration purposes. We selected this model because YOLO11 is the latest \nversion of YOLO, lacking an official architectural diagram and published paper. The l variant was selected due to its \nsimpler calculations compared to the others.  \nThe model is employed with a depth_multiple of 1, a width_multiple of 1, and a max_channels of 512 as presented in \nTable 3. The input image measures 640 x 640 pixels and comprises 3 channels. The initial input image is processed \nthrough Block 0, which is a convolutional block featuring a kernel size of 3 and a stride of 2. Applying a stride of 2 \nreduces the spatial resolution of the output to 320 x 320. The output channels are determined by multiplying the lesser \nvalue between the base output channels and max_channels by the width_multiple. In Block no. 0, the base output \nchannels total 64, resulting in output dimensions of 320 x 320 x 64. The process is illustrated in Figure 21. \n \nTable 3: Model Parameters  \nModel Variant  depth_multiple  width_multiple  max_channels  \nYOLO11l  1 1 512 \n \nFigure 21: Input and Block Conv No. 0  \nThe image is subsequently processed through Block 1, which is an additional convolutional block featuring a kernel \nsize of 3 and a stride of 2. The spatial resolution is further reduced to 160 x 160, and the number of channels is \nadjusted to 128. The output from Block No. 1 is transmitted to Block C3k2 (Block No. 2). In this block, deeper feature \nextraction is conducted. The output maintains a spatial resolution of 160 x 160, whereas the number of channels \nincreases to 256. The process is illustrated in Figure 22. \n \nFigure 22: Block Conv No.1 and C3k2 No. 2  \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n19  This process continues gradually through subsequent blocks in the Backbone, where the spatial size is progressively \nreduced to 80 x 80 (Block no. 3), 40 x 40 (Block no. 5), and 20 x 20 (Block no. 7), according to the configuration of \nconvolutional layers and stride. Additionally, there are several C3k2 blocks (Block no. 4, 6, and 8) that are connected \nto the neck section. The process is illustrated in Figure 23. \n \nFigure 23: Block No. 3 - 8 \nThe output from the backbone, which is the output of Block C3k2 no. 8, is 20 x 20 x 512. This output is transmitted to \nthe neck, specifically to Block SPPF no. 9. The output remains 20 x 20 x 512. The SPPF enhances the efficiency of \nintegrating spatial information.  The process is illustrated in  Figure 24.  \n \nFigure 24: SPPF Block  \nSubsequently, it proceeds to Block C2PSA no. 9. This block is positioned exclusively at the lowest resolution stage. \nThis is implemented to mitigate excessive computation arising from the complexity of self -attention. The output from \nthis block is consistently 20 x 20 x 512.  The process is illustrated in  Figure 25. \n \nFigure 25: C2PSA Block  \nThe output from the C2PSA block will be upsampled to 40 x 40 x 512, and this feature will subsequently be \nconcatenated with the output from Block C3k2 no. 6 (40 x 40 x 512). In concatenation, the total number of channels is \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n20  aggregated, whereas  the resolution remains constant. The output from Concat (Block no. 12) is 40 x 40 x 1024. This \nprocedure of upsampling and concatenation executes the Feature Pyramid Network (FPN), which seeks to integrate \ninformation from features of varying resolutions and levels of abstraction.  The process is illustrated in Figure 26. \n \nFigure 26: Upsampling and Concat  \nThe process proceeds to Block C3k2 no. 13, yielding an output of 40 x 40 x 512. The output is subsequently \nupsampled to 80 x 80 x 512, and the feature is concatenated with the output from Block C3k2 no. 4 (80 x 80 x 512). \nThe output from Concat number 15 is 80 x 80 x 1024.  The process is illustrated in Figure 27. \n \nFigure 27: Concat Block No. 15  \n \nThe output from Concat no. 15 is transmitted to Block C3k2 no. 16, yielding a feature map of 80 x 80 x 512. The \noutput from this block serves as input for the Detect block, which is designed to identify relatively small objects \nwithin the image. The 80 x 80 dimensions are employed for identifying small objects, as the higher  spatial resolution \nretains more details from the original image.  The process is illustrated in Figure 28. \n \nFigure 28: Detect Block for Small Object  \nThe output from Block C3k2 no. 16 is subsequently transmitted to Block Conv no. 17. The spatial resolution \ndecreases to 40 x 40, whereas  the channel count remains at 256. Larger  objects typically occupy a broader area within \nthe image. Lowering the resolution allows for a more compact representation of the object's features, enhancing the \nmodel's capacity to capture  patterns at the correct scale.  The output from Block Conv no. 17 is concatenated with the \noutput from Block C3k2 no. 14, yielding a feature map of dimensions 40 x 40 x 768.  The process is illustrated in \nFigure 29. \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n21   \nFigure 29: Concat Block No. 18  \nThe output from Concat no. 18 is subsequently transmitted to Block C3k2 no. 19, resulting in a feature map of 40 x 40 \nx 512. The output from this block serves as input for the Detect block, which is specifically designed  to identify \nmedium -sized objects.  The process is illustrated in Figure 30. \n \nFigure 30: Detect Block for Medium Object  \nThe output from Block C3k2 no. 19 is subsequently processed through Block Conv no. 20, resulting in a spatial \nresolution of 20 x 20, while maintaining 512 channels. The output from Block Conv no. 20 is concatenated  with the \noutput from Block C2PSA no. 10, yielding a feature map of dimensions 20 x 20 x 1024.  The output from Concat no. \n21 is subsequently transmitted to Block C3k2 no. 22, resulting in a feature map of dimensions 20 x 20 x 512. The \noutput from this block serves as input for the Detect block, which is specifically designed  to identify large objects.  \nThe process is illustrated in Figure 31. \n \nFigure 31: Detect block for Big Object  \n \n5 Conclusion  \nOur comprehensive in -depth analysis revealed that each version exhibits enhancements and refinements in \narchitecture and feature extraction. However, we identified certain architectural blocks that are identical to the \nprevious version. The absence of scholarly publications and official architecture diagrams presents challenges for \nresearchers and practitioners in understanding the model's functionality. This issue also presents a challenge for future \nmodel enhancement. We also discovered that there is an evolutionary trend in YOLO, incorporating an attention layer \nbeginning with YOLOv10. We urge future YOLO developers to provide an architectural diagram and a scholarly \npublication to accelerate researchers, practitioners, and developers in comprehending the model and enhancing its \nperformance in the future.   \n \n \n\n\nYOLOv8 to YOLO11: A Comprehensive Architecture In -depth Comparative Review  A PREPRINT   \n22  References  \n \n[1] Y. Chen, X. Yuan, R. Wu, J. Wang, Q. Hou, and M. -M. Cheng, “YOLO -MS: Rethinking Multi -Scale \nRepresentation Learning for Real -time Object Detection,” 2023, arXiv . doi: 10.48550/ARXIV.2308.05480.  \n[2] Glenn Jocher, Paula Derrenger, and Muhammad Rizwan Munawar, “Home - Ultralytics YOLO Docs.” Accessed: \nJan. 13, 2025. [Online]. Available: https://docs.ultralytics.com/  \n[3] C.-Y. Wang, I. -H. Yeh, and H. -Y. M. Liao, “YOLOv9: Learning What You Want to Learn Using Programmable \nGradient Information,” 2024, arXiv . doi: 10.48550/ARXIV.2402.13616.  \n[4] A. Wang et al. , “YOLOv10: Real -Time End -to-End Object Detection,” 2024, arXiv . doi: \n10.48550/ARXIV.2405.14458.  \n[5] R. Khanam and M. Hussain, “YOLOv11: An Overview of the Key Architectural Enhancements,” 2024, arXiv . \ndoi: 10.48550/ARXIV.2410.17725.  \n[6] R. Sapkota et al. , “YOLO11 to Its Genesis: A Decadal and Comprehensive Review of The You Only Look Once \n(YOLO) Series,” 2024, arXiv . doi: 10.48550/ARXIV.2406.19407.  \n[7] M. A. R. Alif and M. Hussain, “YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their \napplication in the agricultural domain,” 2024, arXiv . doi: 10.48550/ARXIV.2406.10139.  \n[8] J. Terven, D. -M. Córdova -Esparza, and J. -A. Romero -González, “A Comprehensive Review of YOLO \nArchitectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO -NAS,” MAKE , vol. 5, no. 4, pp. \n1680 –1716, Nov. 2023, doi: 10.3390/make5040083.  \n[9] C.-Y. Wang and H. -Y. M. Liao, “YOLOv1 to YOLOv10: The Fastest and Most Accurate Real -time Object \nDetection Systems,” SIP, vol. 13, no. 1, Art. no. 1, 2024, doi: 10.1561/116.20240058.  \n[10] M. Hussain, “YOLOv1 to v8: Unveiling Each Variant –A Comprehensive Review of YOLO,” IEEE Access , vol. \n12, pp. 42816 –42833, 2024, doi: 10.1109/ACCESS.2024.3378568.  \n[11] Priyanto Hidayatullah and Refdinal Tubagus, YOLOv8 Architecture Detailed Explanation - A Complete \nBreakdown , (Oct. 28, 2023). Accessed: Jan. 20, 2025. [Online Video]. Available: \nhttps://www.youtube.com/watch?v=HQXhDO7COj8  \n[12] RangeKing, “RangeKing,” GitHub. Accessed: Jul. 31, 2023. [Online]. Available: https://github.com/RangeKing  \n[13] Glenn Jocher, “shortcut in backbone and neck · Issue #1200 · ultralytics/ultralytics.” Accessed: Nov. 06, 2024. \n[Online]. Available: https://github.com/ultralytics/ultralytics/issues/1200#issuecomment -1454873251  \n[14] Glenn Jocher, “Understanding SPP and SPPF implementation · Issue #8785 · ultralytics/yolov5,” GitHub. \nAccessed: Nov. 06, 2024. [Online]. Available: https://github.com/ultralytics/yolov5/issues/8785  \n[15] Kin-Yiu, Wong, “yolov9 -c.yaml.” Accessed: Jan. 20, 2025. [Online]. Available: \nhttps://github.com/WongKinYiu/yolov9/blob/main/models/detect/yolov9 -c.yaml  \n[16] Priyanto Hidayatullah and Refdinal Tubagus, YOLOv9 Architecture Explained , (Apr. 19, 2024). Accessed: Jan. \n20, 2025. [Online Video]. Available: https://www.youtube.com/watch?v=oZ6I1VHpil0  \n[17] Wang Ao, “yolov10l.yaml.” Accessed: Jan. 20, 2025. [Online]. Available: https://github.com/THU -\nMIG/yolov10/blob/main/ultralytics/cfg/models/v10/yolov10l.yaml  \n[18] Priyanto Hidayatullah and Refdinal Tubagus, YOLOv10 Architecture Explained - A Complete Breakdown , (Sep. \n03, 2024). Accessed: Jan. 20, 2025. [Online Video]. Available: \nhttps://www.youtube.com/watch?v=A6rHMzRvs98  \n[19] Glenn Jocher and Paula Derrenger, “yolo11.yaml.” Accessed: Jan. 20, 2025. [Online]. Available: \nhttps://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/11/yolo11.yaml  \n[20] Priyanto Hidayatullah and Refdinal Tubagus, YOLO11 Architecture - Detailed Explanation , (Oct. 28, 2024). \nAccessed: Jan. 18, 2025. [Online Video]. Available: https://www.youtube.com/watch?v=L9Va7Y9UT8E  \n[21] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, Dive into Deep Learning . Cambridge University Press, 2023.  \n \n\n\n</文献>\n\n<我的研究方法>\n本研究项目针对血液分层检测与分割任务，设计了一种创新的双模态YOLO算法，充分利用白光和蓝光图像的互补特性，\n通过变形跨模态Transformer技术实现特征的高效对齐与融合，以提升检测精度和鲁棒性。\n\n白光图像在边界细节捕捉上具有优势，而蓝光图像因其透光性可提供血液内部结构的清晰信息。算法采用双backbone结构\n分别提取两种模态的多尺度特征，通过内容感知的空间对齐机制解决因光线折射和光学特性差异导致的空间不对齐问题。\n\n其核心创新在于变形跨模态Transformer模块，该模块结合空间变形网络与跨模态注意力机制，通过预测像素级偏移场进行\n特征空间重采样，并利用多头注意力实现模态间信息交互与增强。此外，算法提出三种灵活的特征融合策略（加权融合、\n拼接融合、Transformer融合），并在yolo的多尺度特征层（P3/8、P4/16、P5/32）上进行双向特征增强，确保局部细节与\n全局语义的充分整合。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在图像处理、计算机视觉、多模态融合或医学图像分析方面的内容\n2. 评估文献中方法的主要创新点、技术路线和解决的关键问题\n3. 客观指出文献中方法的局限性和不足之处\n4. 详细分析该文献的方法与我的双模态YOLO算法的异同点，特别是在以下几个方面：\n   - 多模态特征提取与融合策略\n   - 空间不对齐问题的处理方法\n   - 特征交互与增强机制\n   - 在医学图像分析特别是血液检测任务中的适用性\n5. 突出我的方法相比该文献的优势和创新点，强调为什么我的方法能够在血液分层检测任务中取得更好的效果\n6. 总结该文献对我的研究的启发和借鉴意义\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
