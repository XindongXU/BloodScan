{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor1 shape: torch.Size([3, 1504, 1504])\n",
      "Tensor2 shape: torch.Size([3, 1504, 1504])\n",
      "Batch shape: torch.Size([2, 3, 1504, 1504])\n"
     ]
    }
   ],
   "source": [
    "img_path1 = '../data/rawdata_cropped/class1/2022-03-28_103204_1_T3_2346.jpg'\n",
    "img_path2 = '../data/rawdata_cropped/class1/2022-03-28_103204_1_T5_2348.jpg'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # [H, W, C] -> [C, H, W], 并归一化到 [0, 1]\n",
    "])\n",
    "\n",
    "# 加载图片并转换\n",
    "image1 = Image.open(img_path1).convert('RGB')\n",
    "image2 = Image.open(img_path2).convert('RGB')\n",
    "\n",
    "tensor1 = transform(image1) # shape: (3, H, W)\n",
    "tensor2 = transform(image2)\n",
    "\n",
    "# 可选：将两个 Tensor 堆叠在一起，生成一个 batch（形状为 [2, 3, H, W]）\n",
    "batch = torch.stack([tensor1, tensor2])\n",
    "\n",
    "# 打印维度验证\n",
    "print(\"Tensor1 shape:\", tensor1.shape)\n",
    "print(\"Tensor2 shape:\", tensor2.shape)\n",
    "print(\"Batch shape:\", batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -1 1 Conv [64, 3, 2]\n",
      "96 3\n",
      "1 -1 1 Conv [128, 3, 2]\n",
      "192 3\n",
      "2 -1 2 C3k2 [256, False, 0.25]\n",
      "384 3\n",
      "3 -1 1 Conv [256, 3, 2]\n",
      "384 3\n",
      "4 -1 2 C3k2 [512, False, 0.25]\n",
      "768 3\n",
      "5 -1 1 Conv [512, 3, 2]\n",
      "768 3\n",
      "6 -1 2 C3k2 [512, True]\n",
      "768 3\n",
      "7 -1 1 Conv [1024, 3, 2]\n",
      "768 3\n",
      "8 -1 2 C3k2 [1024, True]\n",
      "768 3\n",
      "9 -1 1 SPPF [1024, 5]\n",
      "768 3\n",
      "10 -1 2 C2PSA [1024]\n",
      "768 3\n",
      "11 -1 1 Conv [64, 3, 2]\n",
      "768 96\n",
      "12 -1 1 Conv [128, 3, 2]\n",
      "768 192\n",
      "13 -1 2 C3k2 [256, False, 0.25]\n",
      "768 384\n",
      "14 -1 1 Conv [256, 3, 2]\n",
      "768 384\n",
      "15 -1 2 C3k2 [512, False, 0.25]\n",
      "768 768\n",
      "16 -1 1 Conv [512, 3, 2]\n",
      "768 768\n",
      "17 -1 2 C3k2 [512, True]\n",
      "768 768\n",
      "18 -1 1 Conv [1024, 3, 2]\n",
      "768 768\n",
      "19 -1 2 C3k2 [1024, True]\n",
      "768 768\n",
      "20 -1 1 SPPF [1024, 5]\n",
      "768 768\n",
      "21 -1 2 C2PSA [1024]\n",
      "768 768\n",
      "22 4 1 nn.Identity []\n",
      "768 768\n",
      "23 6 1 nn.Identity []\n",
      "768 768\n",
      "24 10 1 nn.Identity []\n",
      "768 768\n",
      "25 -1 1 nn.Upsample ['None', 2, 'nearest']\n",
      "768 768\n",
      "26 [-1, 23] 1 Concat [1]\n",
      "768 1536\n",
      "27 -1 2 C3k2 [512, False]\n",
      "768 768\n",
      "28 -1 1 nn.Upsample ['None', 2, 'nearest']\n",
      "768 768\n",
      "29 [-1, 22] 1 Concat [1]\n",
      "768 1536\n",
      "30 -1 2 C3k2 [256, False]\n",
      "768 384\n",
      "31 -1 1 Conv [256, 3, 2]\n",
      "768 384\n",
      "32 [-1, 27] 1 Concat [1]\n",
      "768 1152\n",
      "33 -1 2 C3k2 [512, False]\n",
      "768 768\n",
      "34 -1 1 Conv [512, 3, 2]\n",
      "768 768\n",
      "35 [-1, 24] 1 Concat [1]\n",
      "768 1536\n",
      "36 -1 2 C3k2 [1024, True]\n",
      "768 768\n",
      "37 [30, 33, 36] 1 Segment ['nc', 32, 256]\n",
      "768 768\n",
      "Transferred 1575/1575 items from pretrained weights\n",
      "YOLO11x-dseg summary: 557 layers, 90,901,049 parameters, 90,901,033 gradients\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolo11x-dseg.yaml').load('weights/dual_yolo_pretrained.pt')\n",
    "model.info(verbose=True)\n",
    "\n",
    "# model.predict(batch) #, visualize = True)\n",
    "# model.predict([[image1, image2]])\n",
    "\n",
    "import torch\n",
    "\n",
    "# 准备张量输入\n",
    "blue_tensor = transform(image1).unsqueeze(0)  # [1, 3, H, W]\n",
    "white_tensor= transform(image2).unsqueeze(0)  # [1, 3, H, W]\n",
    "dual_tensor = torch.cat([blue_tensor, white_tensor], dim=1)  # [1, 6, H, W]\n",
    "\n",
    "# 直接调用模型的forward方法（绕过predict接口）\n",
    "model.model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.model(dual_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 39, 46389])\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs))\n",
    "print(outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67, 188, 188])\n",
      "torch.Size([1, 67, 94, 94])\n",
      "torch.Size([1, 67, 47, 47])\n"
     ]
    }
   ],
   "source": [
    "print(outputs[1][0][0].shape)\n",
    "print(outputs[1][0][1].shape)\n",
    "print(outputs[1][0][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 46389])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 376, 376])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.7710e-04,  7.1227e-05, -2.1646e-04,  ...,  1.2241e-04, -1.8428e-04,  1.4486e-04],\n",
       "          [ 5.9316e-04,  8.9377e-04,  1.0883e-03,  ...,  8.8355e-04,  1.1299e-03,  1.0725e-03],\n",
       "          [ 6.0623e-04,  1.2159e-03,  9.3081e-04,  ...,  1.1847e-03,  9.5595e-04,  1.1655e-03],\n",
       "          ...,\n",
       "          [ 9.6128e-04,  8.7854e-04,  1.5194e-03,  ...,  8.7184e-04,  1.2353e-03,  1.0161e-03],\n",
       "          [ 4.8849e-04,  1.5596e-03,  8.4110e-04,  ...,  1.2496e-03,  8.7963e-04,  1.2062e-03],\n",
       "          [ 3.8277e-04,  1.5843e-03,  1.3197e-03,  ...,  1.4526e-03,  1.5163e-03,  1.4957e-03]],\n",
       "\n",
       "         [[ 1.4279e-03,  2.4088e-03,  2.4572e-03,  ...,  2.2151e-03,  2.5890e-03,  1.6382e-03],\n",
       "          [ 8.7761e-04,  1.7414e-03,  1.3264e-03,  ...,  1.4527e-03,  1.5855e-03,  6.0589e-04],\n",
       "          [ 1.1385e-03,  1.4630e-03,  1.4934e-03,  ...,  1.3499e-03,  1.5933e-03,  5.6698e-04],\n",
       "          ...,\n",
       "          [ 8.9044e-04,  1.4891e-03,  1.6814e-03,  ...,  1.5946e-03,  1.5676e-03,  6.5937e-04],\n",
       "          [ 1.2351e-03,  1.2990e-03,  1.8818e-03,  ...,  1.4249e-03,  1.6363e-03,  7.0924e-04],\n",
       "          [ 4.7368e-05, -1.9669e-05,  5.3636e-04,  ...,  2.1724e-04,  3.9894e-04,  8.9267e-05]],\n",
       "\n",
       "         [[ 6.5386e-04,  4.4669e-04,  1.5261e-04,  ...,  2.1285e-04,  1.8288e-04,  3.0262e-04],\n",
       "          [ 1.9603e-04, -4.7460e-04, -5.4233e-04,  ..., -3.1464e-04, -5.4068e-04, -2.9102e-04],\n",
       "          [ 2.5719e-04, -2.6561e-04, -5.4675e-04,  ..., -4.2738e-04, -4.7225e-04, -5.4029e-04],\n",
       "          ...,\n",
       "          [ 1.3499e-04, -6.8132e-04, -1.6258e-04,  ..., -3.4254e-04, -5.2550e-04, -2.0743e-04],\n",
       "          [-9.8484e-05, -3.8466e-04, -6.7969e-04,  ..., -4.6645e-04, -4.4708e-04, -3.3007e-04],\n",
       "          [ 9.5690e-05,  2.5438e-04,  1.7273e-04,  ...,  1.8172e-04,  2.1800e-04, -1.5823e-04]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.6359e-04, -4.2300e-06,  1.0096e-04,  ...,  2.1188e-04, -1.2354e-04, -5.3657e-04],\n",
       "          [ 3.0039e-04, -3.5268e-04, -4.4021e-04,  ..., -2.8123e-04, -4.7624e-04, -1.4621e-03],\n",
       "          [ 2.2215e-04, -4.6928e-04, -3.3118e-04,  ..., -5.5454e-04, -6.5764e-04, -1.6837e-03],\n",
       "          ...,\n",
       "          [ 2.0055e-04, -4.2550e-04, -6.0392e-04,  ..., -4.9768e-04, -7.7624e-04, -1.5617e-03],\n",
       "          [ 3.8224e-04, -4.9595e-04, -5.2501e-04,  ..., -3.2432e-04, -4.5714e-04, -1.4605e-03],\n",
       "          [-5.5871e-04, -1.1194e-03, -8.3989e-04,  ..., -9.3814e-04, -6.9282e-04, -1.7831e-03]],\n",
       "\n",
       "         [[ 1.8788e-04,  6.8556e-04,  5.2490e-04,  ...,  5.1913e-04,  3.6271e-04,  9.8614e-04],\n",
       "          [-5.0105e-04, -7.1301e-04, -8.3324e-04,  ..., -5.9095e-04, -6.1936e-04,  4.8157e-04],\n",
       "          [-3.0111e-04, -6.1781e-04, -7.1352e-04,  ..., -9.0210e-04, -9.7082e-04,  4.1061e-04],\n",
       "          ...,\n",
       "          [-3.8847e-04, -6.1916e-04, -7.4246e-04,  ..., -5.6008e-04, -7.0630e-04,  5.0213e-04],\n",
       "          [-1.5524e-04, -5.5912e-04, -7.2668e-04,  ..., -5.8353e-04, -6.8920e-04,  3.4721e-04],\n",
       "          [ 7.5296e-04,  8.0500e-04,  6.4402e-04,  ...,  5.7056e-04,  7.0537e-04,  4.6527e-04]],\n",
       "\n",
       "         [[ 9.5981e-04,  3.1382e-04,  6.3831e-05,  ...,  1.7975e-04,  1.3595e-04, -1.6630e-04],\n",
       "          [ 2.3952e-03,  1.8641e-03,  2.0127e-03,  ...,  1.8833e-03,  2.1381e-03,  6.0684e-04],\n",
       "          [ 2.3823e-03,  2.1358e-03,  1.7251e-03,  ...,  2.0943e-03,  2.0696e-03,  6.9446e-04],\n",
       "          ...,\n",
       "          [ 2.5665e-03,  2.2202e-03,  1.8548e-03,  ...,  1.9277e-03,  1.8725e-03,  6.0444e-04],\n",
       "          [ 2.5732e-03,  1.9207e-03,  1.9197e-03,  ...,  1.8856e-03,  1.9663e-03,  5.5949e-04],\n",
       "          [ 2.4538e-03,  2.1249e-03,  2.4359e-03,  ...,  2.1657e-03,  2.4489e-03,  7.8346e-04]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloodscan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
