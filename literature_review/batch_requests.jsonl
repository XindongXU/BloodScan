{"custom_id": "request-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息和文章结构提纲。你的分析将会帮助用户了解文献的具体的工作，并用于用户的论文撰写灵感、论文提纲设计和论文投稿，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nBGF-YOLO: Enhanced YOLOv8 with\nMultiscale Attentional Feature Fusion\nfor Brain Tumor Detection\nMing Kang, Chee-Ming Ting(B), Fung Fung Ting, and Raphaël C.-W. Phan\nSchool of Information Technology, Monash University, Malaysia Campus,\nSubang Jaya, Malaysia\nting.cheeming@monash.edu\nAbstract. You Only Look Once (YOLO)-based object detectors have\nshown remarkable accuracy for automated brain tumor detection. In this\npaper, we develop a novel BGF-YOLO architecture by incorporating Bi-\nlevel Routing Attention (BRA), Generalized feature pyramid networks\n(GFPN), and Fourth detecting head into YOLOv8. BGF-YOLO con-\ntains an attention mechanism to focus more on important features, and\nfeature pyramid networks to enrich feature representation by merging\nhigh-level semantic features with spatial details. Furthermore, we in-\nvestigate the effect of different attention mechanisms and feature fu-\nsions, detection head architectures on brain tumor detection accuracy.\nExperimental results show that BGF-YOLO gives a 4.7% absolute in-\ncrease of mAP 50compared to YOLOv8x, and achieves state-of-the-art\non the brain tumor detection dataset Br35H. The code is available at\nhttps://github.com/mkang315/BGF-YOLO.\nKeywords: Medical image detection ·lesion detection ·YOLO ·feature\nfusion ·attention mechanism\n1 Introduction\nDetecting brain tumors in their early stages can lead to more effective treat-\nments and better prognoses. Therefore, brain tumor detection is a critical aspect\nof medical diagnostics. Magnetic Resonance Imaging (MRI) is the best imaging\ntest to visualize the brain and detect tumors. The You Only Look Once (YOLO)\nseries has been demonstrated to detect brain tumors accurately. Kang et al. [9]\nproposed RCS-YOLO—a novel YOLO architecture with reparameterized con-\nvolution based on channel shuffle—on brain tumor detection and achieved a\nbalance between accuracy and speed.\nThe YOLOv8 architecture [8] is mainly composed of the backbone and head\nparts, in which the neck is included in the head part. The backbone part, which\nis used for feature extraction, contains Conv, C2f (shortcut), and Spatial Pyra-\nmid Pooling Fast (SPPF) modules. The Conv, that is ConvBiSiLU (or CBS),\nand SPPF are the same as those in the YOLOv5 [7] architecture [27], where\nConv is used to perform convolution operation on the input images and assist\n\n2 M. Kang et al.\nC2f (shortcut) in feature extraction and SPPF enables adaptive-sized output.\nThe C2f (shortcut) module is a lightweight convolutional structure compared to\nthe C3 module in YOLOv5. Thus, the gradient flow of the model is enriched by\nconnecting more branches across layers. Therefore, the more vital feature repre-\nsentation ability is enabled. The C2f (shortcut) module enhances the ability to\nexpress features through dense and residual structures, which changes the num-\nber of channels through split and concatenate operations according to scaling\ncoefficients to reduce computational complexity and model capacity. The SPPF\nmoduleattheend ofthebackbonepart increasesthesensitivity andcapturesthe\nfeature information of different levels in the images. The structures of Feature\nPyramid Networks (FPN) [15] and Path Aggregation Network (PANet) [16] are\nused for multiscale feature fusion in the neck part. The FPN-PANet structure\nand C2f (without shortcut) modules fuse feature maps of different scales from\nthe three stages of the backbone, aggregating shallow information to deep fea-\ntures. The head part employs a decoupled-head structure with a classification\nand regression (i.e., localization) prediction end to alleviate the conflict between\nclassification and regression tasks, and an anchor-free mechanism to improve\nthe detection of objects with irregular height and width. For bounding box clas-\nsification, YOLOv8 employs binary cross-entropy loss while varifocal loss [32]\nis an alternative option. It can better handle the category imbalance situation\nand improve detection accuracy. For bounding box regression, YOLOv8 employs\ndistribution focal loss [11,12,13] to overcome the problem of category imbalance\nand background category, allowing the network to quickly focus on the distribu-\ntion of locations close to the object. It also uses the Complete Intersection over\nUnion (CIoU) loss function [36] to alleviate the overlap between predicted and\nground truth boxes. Nevertheless, some improvements potentially boost detec-\ntion performance in the context of object detection in specific domains, which\ncan still pose challenges using vanilla YOLOv8.\nRecent improvements of YOLOv8 focus on attention mechanisms, multiscale\nfeature fusion networks, and regression loss. Multi-Head Self-Attention mech-\nanism was employed in MHSA-YOLOv8 [14]. A lightweight YOLOv8 [30] was\nproposed by combining a dual-path gated attention and feature enhancement\nmodule with the original YOLOv8s. An improved YOLOv8 with the neck struc-\nture of Asymptotic Feature Pyramid Network (AFPN) [31] was proposed in [5].\nUAV-YOLOv8 [23] utilized the BiFormer block [37], focal fasternet blocks, and\nWise-IoU (WIoU) [20] within the YOLOv8. Another improved YOLOv8 [35]\nalso added Biformer in the backbone of YOLOv8 for insulator fault detection.\nDCA-YOLOv8 [29] employed deformable convolution and Coordinate Atten-\ntion (CA) [3] within YOLOv8 for fast cattle detection. CSS-YOLO [17] respec-\ntively introduced the Swin Transformer and Convolution Block Attention Mod-\nule (CBAM) [26] into YOLOv8’s backbone and neck.\nIn this paper, we propose a novel model called BGF-YOLO, which enhances\nthe detection performance of YOLOv8 by incorporating Bi-level Routing At-\ntention (BRA) [37], Generalized-FPN (GFPN) [6], and Fourth detecting head.\nThe contributions of this work are summarized as follows: 1) We reconstruct the\n\nBGF-YOLO: Enhanced YOLOv8 for Brain Tumor Detection 3\noriginal neck part of YOLOv8 with a structured feature fusion network based\non GFPN to facilitate effective feature fusion at different levels. 2) We lever-\nage BRA for both dynamic and sparse attention mechanisms to focus on more\nsalient features and reduce feature redundancy. 3) We add a fourth detecting\nhead to enrich the scales of anchor boxes and optimize regression loss for de-\ntection. 4) To our best knowledge, this is the first use of enhanced YOLOv8\nfor brain tumor detection. The proposed modifications significantly improve tu-\nmor detection compared to YOLOv8. We also assess the effects of using various\nattention mechanisms, feature pyramid networks, and regression losses on the\ndetection performance.\nFig. 1.OverviewofBGF-YOLO.ThearchitectureofBGF-YOLOisbasedonYOLOv8\nand incorporates new modules colored Bi-level Routing Attention (BRA) [37], Cross\nStagePartialDenseNet(CSP)[21],andenhanceddetectinghead.Conv,C2f(shortcut),\nSPPF, Concat, Upsample, and Detect are existing modules in the original YOLOv8\narchitecture [10].\n2 Methods\nFig. 1 illustrates the architecture of the proposed BGF-YOLO. BGF-YOLO is\ncharacterized by a very deep and heavy neck, in contrast to the lightweight neck\n& head part in the YOLOv8. The backbone and head parts of the proposed\n\n4 M. Kang et al.\nnetwork are based on those of YOLOv8. Details of each part of the BGF-YOLO\nnetwork structure are described in this section.\n2.1 Enhanced GFPN for Multilevel Feature Fusion\nFPN was first introduced to address the hierarchical feature fusion issue of con-\nvolutional neural networks (CNNs) and has been proven to effectively enhance\nthe capabilities of deep learning models in handling object detection tasks, espe-\ncially for detecting objects at various scales. PANet has been used to strengthen\nfeaturepropagation andencourage informationreuse,thus improvingthe feature\npyramid’srepresentationalpower.BidirectionalFPN(BiFPN)[19]addsbottom-\nup pathways to FPN with only a top-down pathway, resulting in bidirectional\ncross-scale connections to harness multiscale features efficiently. Generalized-\nFPN (GFPN) [6] employs the structure of dense link and queen fusion to pro-\nduce better-fused features and uses the concat operation instead of the sum to\nperform feature fusion to reduce loss of information. AFPN uses an adaptive spa-\ntial fusion in an asymptotic process, which initially fuses two low-level features,\nthen higher-level features, and finally top-level features, to enhance the signifi-\ncance of pivotal levels and mitigate the impact of contradictory information from\ndifferent objects.\nFPN and PANet were later used for multiscale feature fusion in the necks\nof YOLOv5 and YOLOv8. The difference in the neck parts between YOLOv5\nand YOLOv8 is that the C2f (without shortcut) modules in YOLOv8 replace\nthe C3 modules in YOLOv5 at the upsampling stage. FPN first extracts the\nfeature maps in the CNNs and then uses upper sampling and coarse-grained\nfeature maps to achieve the fusion of the feature maps in a top-down manner. In\ncontrast,PANetfusesfeaturemapsfrombottomtotoptoensurespatialinforma-\ntion is accurately preserved. However, the combination of FPN and PANet can\nonly support top-down and bottom-up feature fusion. The structures of BiFPN,\nAFPN, and GFPN are supposed to quickly integrate features into various levels\nand improve the effect of feature fusion by adding more levels to meet the needs\nof feature fusion at different levels.\nWe modify the structure of the FPN-PANet in the YOLOv8 to achieve mul-\ntilevel feature fusion among different layers by strengthening the multipath fu-\nsion of the networks. Inspired by the GFPN and reparameterized GFPN-based\nDAMO-YOLO [28], we utilize Cross Stage Partial DenseNet (CSP) [21] to add\nskip connections and simultaneously share dense information across various spa-\ntial scales and non-adjacent levels of latent semantics by replacing C2f (without\nshortcut) and combining with Conv. This allows the model to handle both high-\nlevel semantic information and low-level spatial information with equal impor-\ntance in the neck part. Brain tumor images can exhibit variability in terms of\ntumor size, shape, and location. Cross-stage connections allow the network to\nbetter adapt to this variability by incorporating features from different scales,\nimproving the model’s robustness to diverse tumor characteristics.\n\nBGF-YOLO: Enhanced YOLOv8 for Brain Tumor Detection 5\n2.2 BRA-based Attentional Feature Fusion\nThe main idea of multiscale feature fusion networks in the neck part is to fuse\nfeature maps extracted from different network layers to improve object detection\nperformanceatmultiplescales.However,thefeaturefusionlayerinYOLOv8still\nsuffers from the problem of redundant information from different feature maps.\nTo overcome this limitation, we consider incorporating an attention mechanism\nto the feature fusion process in the YOLOv8 model.\nThe attention mechanism was originally proposed to weigh the importance\nof specific features relative to others. In the context of computer vision, there\nare five attention mechanisms that have great potential in improving the per-\nformance of object detection: Squeeze-and-Excitation (SE) [4], CBAM, Efficient\nChannel Attention (ECA) [22], CA, Receptive-Field Attention (RFA) [34], and\nBRA. The differences among them are that the SE and ECA belong to channel\nattention, the RFA and BRA handle spatial attention, and the CBAM and CA\nfacilitate channel and spatial attention. SE is to adaptively recalibrate channel-\nwise feature responses by explicitly modeling the interdependencies between the\nchannels of convolutional features. ECA only captures the local channel interde-\npendencies without relying on global statistics to reduce computational require-\nments. The advantage of RFA is that it provides effective attention weight to\nrealize convolutional kernel parameter sharing. BRA is a dynamic, query-aware\nsparse attention mechanism that enables a small subset of the most relevant\nkey/value tokens for each query in a content-aware manner.\nWe improve the proposed GFPN-based feature fusion structure by adopting\nthe BRA attention module to achieve effective multilevel feature fusion while\navoiding redundant information across feature maps. The dynamic sparse atten-\ntion can reduce redundant feature information and improve the model’s detec-\ntion accuracy, by applying the weight distribution of each channel and spatial\nposition when integrating feature maps of different scales. We place the BRA\nmodules behind the Conv or Upsample module in the feature fusion process to\nmake the model only focus on specific areas after feature extraction. To further\navoid information loss, skip connections in CSP modules enable the knowledge\nof the underlying feature maps to be reused in subsequent layers. BRA aims to\neliminate a majority of non-pertinent key-value pairs input at a broader regional\nlevel, leaving only a select few relevant areas. Taking a feature map as input, the\nBRA first segments it into various areas and derives the query, key, and value\nthrough a linear transformation. The region-level relationship of queries and\nkeys is entered into an adjacency matrix to construct a directed graph and pin-\npoint the association of specific key-value pairs. This essentially identifies which\nareas should be involved with each designated region. Lastly, multi-head self-\nattention is executed between individual tokens by utilizing the region-to-region\nrouting index matrix. Through the bi-level routing optimization for multi-head\nself-attention, more attention is paid to the brain tumor part of the feature map,\ntherebyimprovingthemodel’sabilitytodetectbraintumors.TheBRAattention\nmodule can be integrated into feature fusion to allow the model to focus more\non relevant regions of the MRI scans where tumors are likely to be present. This\n\n6 M. Kang et al.\nselective focus helps in capturing intricate details and subtle features associated\nwith tumors.\nThis proposed method only uses the attention module BRA of BiFormer,\nwhichisdifferentfromtheexistingworks[23,35]thataddBiFormerintoYOLOv8.\n2.3 An Enhanced Detecting Head\nThe original YOLOv8 has three detecting heads with respective dimensions in\nheight and width of 20×20,40×40, and 80×80. In contrast, these heads still\ncannot meet the detection needs of brain tumor detection scenarios, which leads\nto the unsatisfactory detection accuracy of the model for larger objects than the\noriginal scales.\nWe introduce an additional 160×160detecting head in the head part aligned\nwith the new structure of feature fusion networks in the neck part to improve the\ndetection capacity for objects in various scales. The new scale-detecting head is\nadded as the fourth detecting head next to the original 80×80detection scales\nof YOLOv8. It fuses the shallow information of the first C2f (shortcut) module\nfrom the input images, incorporating additional feature fusion networks. The\none more prediction head we add enhances the model to detect objects in richer\nscales.\nThe additional detecting head can classify and localize different brain tumor\nsizeswithinthemodel.Inthecaseofbraintumordetection,thiscanbebeneficial\nwhen there are different sizes of tumors with distinct characteristics. Extensive\nlayers of detection heads can facilitate progressive detection, where the model\nfirstidentifiespotentialregionsofinterestandthenrefinesitspredictionsthrough\nsubsequent processing stages. This step-wise approach can lead to more accurate\nand robust tumor detection.\n3 Experiments and Results\n3.1 Dataset Details\nWeevaluatedtheperformanceoftheproposedBGF-YOLOv8onthepublicbrain\ntumor image dataset Br35H [2] which contains 801 MRI images with annotated\nbrain tumors. The dataset was divided into the train set of 500 images, the\nvalidation set of 201 images, and the test set of 100 images. All the results are\ntested on the test set.\n3.2 Implementation Details\nThe BGF-YOLO was trained and tested with Intel ®Xeon®Platinum 8255C\nCPU @ 2.50GHz and NVIDIA ®GeForce GTX ®1060 6GB GPU. We imple-\nmentedtheproposedmethodsbasedonYOLOv8extralargeversion(YOLOv8x).\nThe hyperparameters used in the training of BGF-YOLO and other comparison\nmethods are the same as YOLOv8x. The training parameter batch size is set to\n\nBGF-YOLO: Enhanced YOLOv8 for Brain Tumor Detection 7\n5, and the epoch is 120 at the training stage. The optimizer uses the stochastic\ngradient descent with the initial and final learning rate of 0.01 and momentum\nof 0.937.\n3.3 Results\nFor a fair comparison, we choose the version with the best performance of the\ncompeting models and use the same evaluation metrics as those used to evaluate\nthem. As shown in Table 1, BGF-YOLO achieved 1.2%, 4.7% and 0.7% absolute\nincrease in precision, mean average precision mAP 50and mAP 50:95respectively,\ncompared to YOLOv8x. It also outperforms YOLOv9-E [24], YOLOv10-X [25],\nRCS-YOLO, and DAMO-YOLO-L⋆,. BGF-YOLO surpasses not only the base-\nline YOLOv8 model but also a GFPN-neck detector DAMO-YOLO and a high-\naccuracy and fast detector RCS-YOLO. As shown in supplementary material\nSection 1, in qualitative comparison, brain tumor tumors are more accurately\ndetected by BGF-YOLO than YOLOv8.\nTable 1. Performance comparison of YOLOv8x, YOLOv9-E, YOLOv10-X, DAMO-\nYOLO-L⋆, RCS-YOLO, and our proposed BGF-YOLO.⋆in DAMO-YOLO-L⋆indi-\ncates distillation was employed. The original code of all DAMO-YOLO versions only\nprints average precision and average recall. The best results are shown in bold.\nModel Precision RecallmAP 50mAP 50:95\nYOLOv8x [8] 0.907 0.8810.9270.646\nYOLOv9-E [24] 0.927 0.8690.9190.630\nYOLOv10-X [25] 0.916 0.8080.8800.603\nRCS-YOLO [9] 0.908 0.8850.8780.580\nDAMO-YOLO-L⋆[28]– –0.9000.610\nBGF-YOLO (Ours) 0.919 0.926 0.974 0.653\n3.4 Ablation Study\nWe conducted a series of ablation studies to assess the advantages of incorpo-\nrating each method in the proposed BGF-YOLO model, and to investigate the\neffect of using different techniques in each method on the detection performance\nthroughthefollowingextensiveexperiments.SeesupplementarymaterialSection\n2: Tables of Ablation Study.\nAblation Study on Overall Architecture We evaluated four incomplete\nBGF-YOLO models by removing each method respectively. Supplementary ma-\nterial Table 1 shows that BRA, GFPN, the fourth head, and GIoU all contribute\nto the accuracy improvement of BGF-YOLO. The w/o GFPN means using the\noriginal neck structure FPN-PANet of YOLOv8. Adding the fourth detection\nhead gives the most impact on the overall accuracy improvement especially for\nmAP 50, followed by GFPN and BRA.\n\n8 M. Kang et al.\nEffect of Different Multiscale Feature Fusion Structures We compared\ntheproposedBGF-YOLOwithBBF-YOLOandBAF-YOLO,whichrespectively\nreplaceGFPNwithBiFPNandAFPNintheneckpartofBGF-YOLOforfeature\nfusion. As shown in Supplementary material Table 2, the precision, mAP 50, and\nmAP 50:95, expect recall of BGF-YOLO with GFPN structure are much higher\nthan the model with BiFPN and AFPN structures.\nEffect of Different Attention Mechanisms We investigated different at-\ntention mechanisms with the proposed BGF-YOLO model. The first letters of\nthe model names listed in supplementary material Table 3 represent the atten-\ntion mechanisms used, which means S, E, C, A, R, and B denote SE, ECA,\nCBAM, CA, RFA, and BRA. BRA gives the largest performance improvement\namong the different attention mechanisms compared to the other five alterna-\ntive ones. Meanwhile, CBAM (i.e., CGF-YOLO) ranks second behind BRA (i.e.,\nBGF-YOLO) in terms of mAP 50and has higher values in precision than BRA.\nAlthough the mAP 50:95of ECA (i.e., EGF-YOLO) and CA (i.e., AGF-YOLO)\nare higher than that of BRA, the mAP 50of ECA and CA are much lower than\nthat of BRA.\nEffect of Different Regression Losses We performed an ablation study\non the influence of regression losses, including Generalized IoU (GIoU) [18]\nwhere the distance between two axis-aligned rectangles is calculated, Distance-\nIoU (DIoU) [36] optimal objectives of which are less than CIoU, Efficient IoU\n(EIoU) [33] which explicitly measures the discrepancies of three geometric fac-\ntors, Scylla-IoU (SIoU) [1] where penalty metrics are redefined, WIoU v3 which\nis a two-layer attention-based with a dynamic nonmonotonic focusing mecha-\nnism regression loss function. These loss functions are represented by C, E, S,\nand W as the fourth letters of model names in supplementary material Table\n4. Compared to other regression losses, The original regression loss CIoU in\nYOLOv8 has better robustness of the bounding box for object detection. The\nmAP 50of DIoU (i.e., BGFD-YOLO) is close to that of CIoU (i.e., BGF-YOLO),\nwhich indicates DIoU is a competitor to CIoU. In terms of mAP 50:95, those of\nGIoU (i.e., BGFG-YOLO) and EIoU (i.e., BGFE-YOLO) are higher than that\nof CIoU. Which regression loss is a better choice depends on the criterion of the\nspecific scenario. In this case, we choose mAP 50as the main metric for brain tu-\nmor detection, and therefore, CIoU is selected as regression loss in the proposed\nBGF-YOLO.\n4 Conclusion\nWe developed a novel BGF-YOLO model building on YOLOv8 for accurate de-\ntection of brain tumors from MRI. We show that the object detection capability\nof YOLOv8 is substantially enhanced by the optimization of the GFPN feature\nfusion structure, BRA attention mechanism, and adding a detecting head in the\n\nBGF-YOLO: Enhanced YOLOv8 for Brain Tumor Detection 9\nBGF-YOLO model. These modifications enable weighted feature fusion at dif-\nferent levels and at richer scales and produce high-quality anchor boxes with\ndynamic focusing mechanisms. Besides, the proposed modules in BGF-YOLO\nare better than the other alternative techniques, as shown in a series of experi-\nmental evaluations on different feature fusion structures, attention mechanisms,\nand regression losses. Our proposed BGF-YOLO becomes the current state-of-\nthe-art model on the brain tumor detector dataset Br35H.\nAcknowledgement. ThisworkwassupportedbytheMonashUniversityMalaysia\nand the Ministry of Higher Education, Malaysia under Fundamental Research\nGrant Scheme FRGS/1/2023/ICT02/MUSM/02/1.\nDisclosure of Interests. The author has no conflict of interest related to the\narticle.\nReferences\n1. Gevorgyan, Z.: SIoU loss: more powerful learning for bounding box regression.\narXiv preprint arXiv:2205.12740 (2022)\n2. Hamada, A.: Br35H :: brain tumor detection 2020. Kaggle (2020). https://www.\nkaggle.com/datasets/ahmedhamada0/brain-tumor-detection\n3. Hou, Q., Zhou, D., Feng, J.: Coordinate attention for efficient mobile network de-\nsign.In:2021IEEE/CVFConferenceonComputerVisionandPatternRecognition\n(CVPR), pp. 13708–13717. IEEE, Piscataway (2021)\n4. He, K., Zhang, X., Ren, S., Sun, J.: Squeeze-and-excitation networks. In: 2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 7132–7141. IEEE, Piscataway (2018)\n5. Huang, Z., Li, L., Krizek, G.C., Sun, L.: Research on traffic sign detection based\non improved YOLOv8. J. Comput. Commun. 11(7), 226–232 (2023)\n6. Jiang, Y., Tan, Z., Wang, J., Sun, X., Lin, M., Li, H.: GiraffeDet: a heavy-neck\nparadigm for object detection. In: 2022 International Conference on Learning Rep-\nresentations (ICLR). (2022)\n7. Jocher, G.: YOLO by ultralytics (version 5.7.0). GitHub (2022).\nhttps://github.com/ultralytics/yolov5\n8. Jocher, G., Chaurasia, A., Qiu, J.: YOLO by ultralytics (version 8.0.190). GitHub\n(2023). https://github.com/ultralytics/ultralytics\n9. Kang, M., Ting, C.-M., Ting, F.F., Phan, R.C.-W.: RCS-YOLO: a fast and\nhigh-accuracy object detector for brain tumor detection. In: Greenspan, H., et\nal. (eds.) MICCAI 2023. LNCS, vol. 14223, 600–610. Springer, Cham (2023).\nhttps://doi.org/10.1007/978-3-031-43901-8_57\n10. King, R.: Brief summary of YOLOv8 model structure. GitHub (2023).\nhttps://github.com/ultralytics/ultralytics/issues/189\n11. Li, X., et al.: Generalized focal loss: Learning qualified and distributed bounding\nboxes for dense object detection. In: Larochelle, H., Ranzato, M., Hadsell, R.,\nBalcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems\n33 (NeurIPS), pp. 21002–21012. Curran Associates, New York (2020)\n\n10 M. Kang et al.\n12. Li, X., Wang, W., Hu, X., Li, J., Tang, J., Yang, J.: Generalized focal loss v2:\nlearning reliable localization quality estimation for dense object detection. In: 2021\nIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pp.\n11632–11641. IEEE, Piscataway (2021)\n13. Li,X.,Lv,C.,Wang,W.,Li,G.,Yang,L.,Yang,J.:Generalizedfocalloss:Towards\nefficient representation learning for dense object detection. IEEE Trans. Pattern\nAnal. Mach. Intell. 45(3), 3139–3153 (2023)\n14. Li, P., Zheng, J., Li, P., Long, H., Li, M., Gao, L.: Tomato maturity detection and\ncounting model based on MHSA-YOLOv8. Sens. 23(15), 6701 (2023)\n15. Lin, T.-Y., Dollar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature\npyramid networks for object detection. In: 2017 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pp. 2117–2125. IEEE, Piscataway\n(2017)\n16. Liu, S., Qi, L., Qin, H., Shi, J., Jia, J.: Path aggregation network for instance\nsegmentation. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 8759–8768. IEEE, Piscataway (2018)\n17. Lu, L.: Improved YOLOv8 detection algorithm in security inspection image. arXiv\npreprint arXiv:2308.06452 (2023)\n18. Rezatofighi, H., Tsoi, J., Gwak, J., Sadeghian, A., Reid, I.: Generalized intersection\nover union: a metric and a loss for bounding box regression. In: 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 658–666.\nIEEE, Piscataway (2019)\n19. Tan, M., Pang, R., Le, Q.V.: EfficientDet: scalable and efficient object detection.\nIn: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 10781–10790. IEEE, Piscataway (2020)\n20. Tong, Z., Chen, Y., Xu, Z., Yu, R.: Wise-IoU: bounding box regression loss with\ndynamic focusing mechanism. arXiv preprint arXiv:2301.10051 (2023)\n21. Wang, C.-Y., Liao, H.-Y.M., Wu, Y.-H., Chen, P.-Y., Hsieh, J.-W., Yeh, I.-H.:\nCSPNet: a new backbone that can enhance learning capability of CNN. In: 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n(CVPRW), pp. 1571–1580. IEEE, Piscataway (2020)\n22. Wang, Q., Wu, B., Zhu, P., Li, P., Zuo, W., Hu, Q.: ECA-Net: efficient channel\nattention for deep convolutional neural networks. In: 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 11534–11542. IEEE,\nPiscataway (2020)\n23. Wang, G., Chen, Y., An, P., Hong, H., Hu, J., Huang, T.: UAV-YOLOv8: a small-\nobject-detection model based on improved YOLOv8 for UAV aerial photography\nscenarios. Sens. 23(16), 7190 (2023)\n24. Wang, C.-Y., Yeh, I.-H., Liao, H.-Y.M.: YOLOv9: learning what you want to learn\nusing programmable gradient information. arXiv preprint arXiv:2402.13616 (2024)\n25. Wang, A., et al.: YOLOv10: real-time end-to-end object detection. arXiv preprint\narXiv:2405.14458 (2024)\n26. Woo, S., Park, J., Lee, J.-Y., Kweon, I.S.: CBAM: convolutional block attention\nmodule. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018.\nLNCS, vol. 11211, pp. 3–19. Springer, Cham (2018). https://doi.org/10.1007/978-\n3-030-01234-2_1\n27. Wu, Z.: YOLOv5 (6.0/6.1) brief summary. GitHub (2022). https://github.com/\nultralytics/yolov5/issues/6998\n28. Xu, X., Chen, Y., Xu, Z., Yu, R.: DAMO-YOLO: a report on real-time object\ndetection design. arXiv preprint arXiv:2211.15444 (2023)\n\nBGF-YOLO: Enhanced YOLOv8 for Brain Tumor Detection 11\n29. Yang, W., et al.: Deformable convolution and coordinate attention for fast cattle\ndetection. Comput. Electron. Agric. 211, 108006 (2023)\n30. Yang, G., Wang, J., Nie, Z., Yang, H., Yu, S.: A lightweight YOLOv8 tomato\ndetection algorithm combining feature enhancement and attention. Agron. 13(7),\n1824 (2023)\n31. Yang,G.,Lei,J.,Zhu,Z.,Cheng,S.,Feng,Z.,Liang,R.:AFPN:asymptoticfeature\npyramid network for object detection. arXiv preprint arXiv:2306.15988 (2023)\n32. Zhang, H., Wang, Y., Dayoub, F., Sünderhauf, N.: VarifocalNet: an IoU-aware\ndense object detector. In: 2021 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 8510–8519. IEEE, Piscataway (2021)\n33. Zhang, Y.-F., Ren, W., Zhang, Z., Jia, Z., Wang, L., Tan, T.: Focal and efficient\nIOU loss for accurate bounding box regression. Neurocomputing 506, 146–157\n(2022)\n34. Zhang, Y., et al.: RFAConv: innovating spatial attention and standard convolu-\ntional operation. arXiv preprint arXiv:2304.03198 (2023)\n35. Zhang, Y., Wu, Z., Wang, X., Fu, W., Ma, J., Wang, G.: Improved YOLOv8 insula-\ntor fault detection algorithm based on BiFormer. In: 2023 IEEE 5th International\nConference on Power, Intelligent Computing and Systems (ICPICS), pp. 962–965.\nIEEE, Piscataway (2023)\n36. Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., Ren, D.: Distance-IoU loss: faster\nand better learning for bounding box regression. Proc. AAAI Conf. Artif. Intell.,\n34(07), 12993–13000 (2020)\n37. Zhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.W.H.: BiFormer: vision transformer\nwith bi-Level routing attention. In: 2023 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pp. 10323–10333. IEEE, Piscataway (2023)\n\n\n</文献>\n\n<我的研究方法>\n在本硕士研究中，我围绕血液试管的自动化识别与分类展开，设计并实现了一个基于计算机视觉的多模态检测系统 —— BloodScan。该系统以深度学习为核心，结合机械臂控制与高分辨率图像采集，实现了对血液样本中不同分层（血清层、白膜层、血浆层）的高精度识别与评估。\n\n研究的核心创新在于提出了跨模态注意力机制（CrossModalAttention），通过蓝光与白光图像的融合，提升了血液分层边界的识别精度。在模型设计上，采用双Backbone结构分别提取不同光源下的特征，并在多个尺度上实现跨模态特征融合，兼顾检测精度与计算效率。相比传统YOLO架构，本研究提出的跨模态注意力策略在准确率、召回率上取得显著提升。\n\n系统模块涵盖了数据采集与增强、YOLO模型训练与评估、注意力可视化、以及机械臂和摄像头的软硬件联动控制。为提升模型训练效率，还构建了6通道融合数据集，并制定了严格的“精确一次检测”医学评估标准。\n\n本项目计划不仅在方法上具有创新性，也希望在工程实现层面达到高完整度，具备良好的可复现性和扩展性，为临床血液分析的自动化提供了有力支撑。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在血液分析检测、图像识别、深度神经网络、图像处理、计算机视觉、多模态融合或医学图像分析等方面的内容\n2. 总结该文献的提纲和关键信息\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息和文章结构提纲。你的分析将会帮助用户了解文献的具体的工作，并用于用户的论文撰写灵感、论文提纲设计和论文投稿，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Original Article\nCMAF-Net: a cross-modal attention fusion-based deep neural \nnetwork for incomplete multi-modal brain tumor segmentation\nKangkang Sun1,2,3,4, Jiangyi Ding2,3,4, Qixuan Li2,3,4, Wei Chen1,2,3,4, Heng Zhang2,3,4, Jiawei Sun2,3,4,  \nZhuqing Jiao1*, Xinye Ni2,3,4*\n1School of Computer Science and Artificial Intelligence, Changzhou University, Changzhou, China; 2Department of Radiotherapy, The Affiliated of \nChangzhou No. 2 People’s Hospital of Nanjing Medical University, Changzhou, China; 3Jiangsu Province Engineering Research Center of Medical \nPhysics, Changzhou, China; 4Center of Medical Physics, Nanjing Medical University, Changzhou, China\nContributions: (I) Conception and design: K Sun, X Ni, Z Jiao; (II) Administrative support: Z Jiao, X Ni; (III) Provision of study materials or patients: \nK Sun, X Ni; (IV) Collection and assembly of data: J Ding, Q Li, W Chen; (V) Data analysis and interpretation: K Sun, H Zhang, J Sun; (VI) \nManuscript writing: All authors; (VII) Final approval of manuscript: All authors.\n*These authors contributed equally to this work.\nCorrespondence to: Xinye Ni, PhD. Department of Radiotherapy, The Affiliated of Changzhou No. 2 People’s Hospital of Nanjing Medical \nUniversity, 68 Gehu Middle Road, Wujin District, Changzhou 213164, China; Jiangsu Province Engineering Research Center of Medical Physics, \nChangzhou, China; Center of Medical Physics, Nanjing Medical University, Changzhou, China. Email: nxy@njmu.edu.cn; Zhuqing Jiao, PhD. \nSchool of Computer Science and Artificial Intelligence, Changzhou University, 21 Gehu Middle Road, Wujin District, Changzhou 213164, China.  \nEmail: jzq@cczu.edu.cn.\nBackground:  The information between multimodal magnetic resonance imaging (MRI) is complementary. \nCombining multiple modalities for brain tumor image segmentation can improve segmentation accuracy, \nwhich has great significance for disease diagnosis and treatment. However, different degrees of missing \nmodality data often occur in clinical practice, which may lead to serious performance degradation or \neven failure of brain tumor segmentation methods relying on full-modality sequences to complete the \nsegmentation task. T o solve the above problems, this study aimed to design a new deep learning network for \nincomplete multimodal brain tumor segmentation.\nMethods: We propose a novel cross-modal attention fusion-based deep neural network (CMAF-Net) \nfor incomplete multimodal brain tumor segmentation, which is based on a three-dimensional (3D) U-Net \narchitecture with encoding and decoding structure, a 3D Swin block, and a cross-modal attention fusion \n(CMAF) block. A convolutional encoder is initially used to extract the specific features from different \nmodalities, and an effective 3D Swin block is constructed to model the long-range dependencies to obtain \nricher information for brain tumor segmentation. Then, a cross-attention based CMAF module is proposed \nthat can deal with different missing modality situations by fusing features between different modalities to \nlearn the shared representations of the tumor regions. Finally, the fused latent representation is decoded to \nobtain the final segmentation result. Additionally, channel attention module (CAM) and spatial attention \nmodule (SAM) are incorporated into the network to further improve the robustness of the model; the \nCAM to help focus on important feature channels, and the SAM to learn the importance of different spatial \nregions.\nResults:  Evaluation experiments on the widely-used BraTS 2018 and BraTS 2020 datasets demonstrated \nthe effectiveness of the proposed CMAF-Net which achieved average Dice scores of 87.9%, 81.8%, and \n64.3%, as well as Hausdorff distances of 4.21, 5.35, and 4.02 for whole tumor, tumor core, and enhancing \ntumor on the BraTS 2020 dataset, respectively, outperforming several state-of-the-art segmentation methods \nin missing modalities situations.\nConclusions: The experimental results show that the proposed CMAF-Net can achieve accurate brain 4604\n\nSun et al. CMAF-Net 4580\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Introduction \nA brain tumor is an abnormal growth of brain cells, \nwhich is considered one of the most prevalent and deadly  \ndiseases (1). Relevant data show that brain tumors account \nfor approximately 1.6% of the incidence and 2.5% of the \nmortality of all tumors, posing a great threat to human \nhealth (2). Accurate segmentation of brain tumors provides \nimportant information to assess disease progression and \ndevelop treatment plans. Magnetic resonance imaging \n(MRI) is a high-performance imaging technology that \ncauses little damage to the human body. It is extensively \nused to examine brain tumors because it can provide high-\nresolution images of the anatomical structure of soft \ntissues (3). Multimodal MRI is an imaging sequence under \ndifferent imaging parameters that preserves the structural \nfeatures of brain diseases from multiple perspectives. In \nthe multimodal MRI of brain tumors, commonly used \nMRI sequences include fluid-attenuated inversion recovery \n(FLAIR), T1-weighted (T1), contrast-enhanced T1-\nweighted (T1ce), and T2-weighted (T2) (4). Different \nmodalities can describe different kinds of pathological features of brain tumor structures, and multimodal image \ninformation can effectively complement one another.  \nFigure 1 shows a typical example of multimodal MRI for \nbrain tumor segmentation in the BraTS 2020 dataset (5). \nFrom left to right, the MRI images of the 4 modalities \nand the corresponding tumor region labels are shown. As \nshown in Figure 1, different modality sequences highlight \ndifferent features, and limitations exist in characterizing \nthese features with only a single-modality MRI. Combining \nmultiple modality images can provide comprehensive \ninformation for analyzing different subregions of brain \ntumors and improving the accuracy of diagnosis and \nsegmentation (6). However, manual segmentation of \ndifferent subregions of brain tumors from multimodal MRI \nusually requires substantial time and effort from expert \nradiologists (7), and the results are usually subjective. \nTherefore, designing an automatic multimodal MRI brain \ntumor segmentation algorithm is needed to improve the \naccuracy and efficiency of clinical diagnosis (8).\nFor the auxiliary diagnosis technology of medical images, \nsome studies have mainly focused on the classification tumor segmentation in the case of missing modalities with promising application potential.\nKeywords: Brain tumor segmentation; magnetic resonance imaging (MRI); missing modalities; multimodal \nfusion; cross-modal attention fusion (CMAF)\nSubmitted Jan 03, 2024. Accepted for publication Apr 19, 2024. Published online Jun 27, 2024.\ndoi: 10.21037/qims-24-9\nView this article at: https://dx.doi.org/10.21037/qims-24-9\nFigure 1 Example of data from the BraTS 2020 dataset. From left to right, the images represent 4 MRI modalities: T1, T1ce, T2, FLAIR, \nand the ground truth labels. In the ground truth image, the green region represents edema; the yellow region represents enhanced tumor; \nand the red region represents necrotic and non-enhancing tumor. T1, T1-weighted; T1ce, contrast-enhanced T1-weighted; T2, T2-\nweighted; FLAIR, fluid-attenuated inversion recovery; GT, ground truth; MRI, magnetic resonance imaging.\nT1 T2 GT T1ce FLAIR\n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4581\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9of brain tumors. For example, Badjie et al. (9) proposed \na new model combining convolutional neural network \n(CNN) and AlexNet to enhance brain tumor detection and \nclassification in MR images. Khan et al. (10) proposed a \nbrain tumor detection and classification framework based \non saliency map and deep learning feature optimization, and \ndesigned an improved dragonfly optimization algorithm for \noptimal feature selection, but their framework is limited by \nincorrect tumor segmentation. Later, Kurdi et al. (11) used \nthe Harris Hawks optimized convolutional neural network \n(HHOCNN) method to classify brain tumors, which further \nimproved the overall tumor recognition accuracy. The \nfocus of these works was still mainly on the task of brain \ntumor classification, but the current image segmentation \nresearch is also becoming more and more important, such \nas the realization of CNN-based joint segmentation and \nclassification to improve the disease detection accuracy \nof benchmark and clinical images (12), or the use of deep \nlearning methods to segment brain tumor images.\nDeep learning is widely used in various computer vision \nproblems because of its strong feature extraction ability \nand high adaptability. CNNs show good performance \nin brain tumor segmentation. Mainstream CNN-based \nnetworks with an encoder–decoder structure such as 3D \nU-Net (13), Attention U-Net (14), and V-net (15) provide \npromising results for 2-dimensional (2D) and 3-dimensional \n(3D) brain tumor segmentation tasks. However, CNN-\nbased methods also have limitations. The localized nature \nof convolution limits the model when dealing with long-\nrange dependencies, resulting in insufficient ability to learn \nglobal features and remote features, which are essential for \naccurately segmenting tumors of different types and sizes. \nT o address this issue, T ransformer (16) has been rapidly \napplied to brain tumor MRI image segmentation tasks with \ngood performance by establishing connections between \ntoken features and employing a self-attention mechanism \nto be able to better capture global contextual information. \nCompared with the traditional CNN, the T ransformer \nmodel solves the local inductive bias and improves the \nability to deal with non-local interactions by introducing \na self-attention mechanism. Existing T ransformer-based \nbrain tumor segmentation methods can compensate for \nthe problem of limited receptive field of traditional CNN, \nbut the computational cost is high (17,18). T o reduce \nthe computational complexity of T ransformer, Swin \nT ransformer (19) utilizes window-based self-attention to \nreduce parameters and computation, as well as a shifted-\nwindow mechanism for global dependency modeling. Inspired by this, Hatamizadeh et al. (20) proposed Swin \nUNETR (UNEt TRansformers), a new architecture for the \nsemantic segmentation of brain tumors by using multimodal \nMRI images. It uses Swin T ransformer as an encoder and a \nCNN-based decoder. Among the techniques using artificial \nneural networks, there are also various preprocessing \ntechniques such as data augmentation, transfer learning, and \ntest time augmentation that can improve the performance \nof classifiers with limited training data. T ransfer learning \ncan accelerate the CNN training process by using pre-\ntrained models. Deep CNN combined with transfer \nlearning techniques can achieve intelligent recognition and \nclassification of brain tumors, which helps to reduce the \nwork intensity of doctors (21). Data augmentation processes \ncan solve problems with limited datasets and improve \nclassification performance, and are often incorporated into \nmodel training (22). The combined use of these techniques \ncan bring more possibilities and development opportunities \nfor research and practice in areas such as medical image \nanalysis. In addition, multimodal joint learning can help \nthe model to mine the relationship between modal data \nand establish complementary links between modalities (23). \nThis strategy is widely used in the field of medical image  \nanalysis (24). Compared with using unimodal MRI, \nsegmentation using multimodal MRI can fully utilize the \ncomplementary information among different modalities and \nsignificantly improve the segmentation performance (25).  \nT o date, numerous multimodal methods for automated \nbrain tumor segmentation have been proposed (26-28). \nAmong them, a common way to use multimodal MRI \nimages is to directly concatenate them in the channel \ndimension as inputs early in the network, but it does not \ntake full advantage of the complementary relationship \namong different modal images (26). T o address this \nproblem, recent works have improved the performance and \nexpressiveness of the model by using high-level features \nextracted from each modality. They use a hierarchical \nfusion strategy in the middle layers of the network (27,28). \nXing et al. (27) proposed a nested Modality Aware Feature \nAggregation module for multimodal fusion, which utilizes \na nested T ransformer to establish intra- and inter-modal \nlong-range correlations for a more efficient feature \nrepresentation. Considering the significant differences \nbetween modalities, T ranSiam (29) uses 2 parallel CNNs \nto extract features specific to each modality, and fuses the \nfeatures of different modalities through a locally-aware \naggregation block to establish correlations between the \nfeatures of different modalities. However, it focuses on \n\nSun et al. CMAF-Net 4582\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9generating shared representations by fusing multi-modal \ndata, without considering modality-specific features. \nF2Net (30) has an obvious advantage, that is, it retains \nmodal-specific features while exploring complementary \ninformation, thereby improving segmentation performance. \nThe methods discussed above have a better segmentation \neffect when the multimodal MRI data are complete. \nHowever, considering factors such as scanning cost and data \nquality, not all patients have complete multimodal MRI \ndata. As a result, the problem of missing modalities arises, \nand segmentation performance is reduced. Some existing \nbrain tumor segmentation methods (31,32) can use only \ndata with complete modalities to complete the segmentation \ntask, which cannot be directly applied to modality missing \nscenarios in clinical practice. Therefore, the development \na robust multimodal brain tumor segmentation method is \nurgently needed to solve the missing modality problem.\nCurrent mainstream segmentation methods for dealing \nwith missing modalities can be roughly divided into 2 \nkinds. One relatively straightforward approach is to train \na dedicated network for each possible combination of \nMRI modalities (8), which is a complex and inefficient \ntraining process despite better segmentation performance. \nAnother more typical solution is to synthesize the missing \nmodalities by generative modeling and then using the \ncomplete modalities for segmentation (33,34). For example, \nZhan  et al. (33) proposed Multi-Scale Gate Mergence \nGenerative Adversarial Network (MGM-GAN), an end-\nto-end multimodal MRI synthesis architecture, based on \nconditional generation adversarial networks. The model \nextracts valuable features from multiple MRI modalities \nand is used to synthesize missing or corrupted images in \nthe clinic. However, this approach requires training the \ngenerative model for each missing modality, which not \nonly introduces additional computational cost and manual \nintermediate steps, but also the generative network training \nfaces an instability problem that may lead to unsatisfactory \nsynthesis results. Ultimately, the segmentation results \nare affected. The third solution to incomplete modality \nis knowledge distillation (35,36). Li et al. (36) proposed \na Deeply-supervised knowledGE tranSfer neT work \n(DIGEST), constructing a knowledge transfer learning \nframework that enables a student model to learn modality-\nsharing semantic information from an instructor model pre-\ntrained with complete multimodal MRI data. It can achieve \naccurate brain tumor segmentation in different modality-\nabsent scenarios. However, the performance and robustness \nof the student model may be affected by the teacher model. Similarly, D2-Net (37) segments brain tumors with missing \nmodalities and identifies the relationship between the state \nand the tumor region by aligning the characteristics of \nthe disentangled binary teacher network with the overall \nstudent network. Meanwhile, Qiu et al. (38) proposed a \nCategory Aware Group Self-Support Learning framework \nfor incomplete multi-modal brain tumor segmentation. \nUnlike previous distillation algorithms, the framework \ndoes not use a larger network as the teacher, but a self-\nsupporting group of students as the teacher, and no new \nmodels or parameters are introduced. The first ‘dedicated’ \nmethod performs well in segmentation effect; however, \nthey are resource-intensive. Since there may be various \nmissing modes, these methods usually need to train \nmultiple networks, which requires substantial time and \ncomputer resources. The synthetic network training in the \nsecond method requires certain computing and storage \nresources, and its performance is affected by the quality of \nthe synthetic image of the missing mode. The third method \nbased on knowledge distillation needs to train a series of \nstudent models for each missing modality, which leads to \nhuge space and time costs. At the same time, this method \nalso has the risk of inaccurate information caused by direct \ntransmission of knowledge between modalities. Although \nknowledge distillation can mine useful knowledge from the \nteacher network to guide the feature learning of the student \nnetwork, it is still challenging to enable the student model \nto obtain key feature representations from all modalities. \nRecent approaches have attempted to create a shared \nfeature space to retrieve missing information and establish \na unified model for all missing modality cases (39-41).  \nSpecifically, Havaei et al. (40) proposed Hetero-Modal \nImage Segmentation (HeMIS). It trains a feature extractor \nfor each modality and computes first-order moments and \nsecond-order moments of the available features to fuse \nmultimodal information and deal with missing modalities. \nAfter the HeMIS network, Dorent et al. (41) proposed a \nHetero-Modal Variational Encoder-Decoder (U-HVED), \nwhich extends the Multi-Modal Variational Auto-Encoder \nand has been shown to be robust to missing modes. \nHowever, these 2 methods may not be able to aggregate \nfeatures efficiently and perform poorly when critical modes \nare missing. Zhang et al. (42) proposed a T ransformer-based \nincomplete multimodal learning approach for brain tumor \nsegmentation, aiming to fuse the features of all modalities \ninto more comprehensive features, but it ignores the use of \nthe correlation between different modalities. Region-aware \nFusion Network for Incomplete Multi-modal Brain T umor \n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4583\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Segmentation (RFNet) (43) achieves area-aware fusion \nthrough a tumor area-aware module, but fails to establish \ncross-modal long-range dependencies. Furthermore, \nKonwer et al. (44) proposed a new training strategy to solve \nthe missing problem in brain tumor segmentation under \nlimited full-modal supervision through metal learning and \nmodal combination as a meta-task. These previous works \nare cleverly designed and robust under missing modality \nconditions, but these methods still need to rely on complete \nmodalities during training. Unlike previous methods, our \napproach is effective and resource efficient because it does \nnot require training specific models for individual missing \nmodality cases. Meanwhile, our framework simulates only \nthe missing modalities by randomly “dropping out” the \nimaging modalities throughout the training process without \nchanging the model structure.\nT o overcome the above limitations, the present study \naimed to adopt a more efficient incomplete multimodal \nlearning strategy to learn modality-invariant shared-feature \nrepresentations by fusing any number of available modalities \nin the latent space. A unified model was then established \nfor all possible missing-modality scenarios. We propose a \ndeep CNN [cross-modal attention fusion-based deep neural \nnetwork (CMAF-Net)] consisting of a 3D U-Net (13) and \na cross-modal attention fusion (CMAF) module for brain \ntumor segmentation with missing modalities. Our network \ncan automatically capture complementary information from \nincomplete multimodal data and learn brain tumor shared \nfeature representations to handle the problem of missing \nmodalities in real-world scenarios. In our design, we used \n4 independent coding branches for each modality, where \nthe convolutional block and 3D Swin block (45) learn local \nfeatures and global dependencies, respectively. Then, we \ndesigned a CMAF module, which can effectively utilize the \ncomplementary information between different modalities to \nlearn modality-shared potential representations. Finally, the \nmodality-invariant shared features obtained after the fusion \nstage were upsampled to obtain the final segmentation \nresults. Extensive experimental results on BraTS 2018 \nand 2020 (5) demonstrated that CMAF-Net exhibited a \nsuperior performance compared to other popular networks \nin brain tumor segmentation under various missing \nmodality settings. The main contributions of this paper are \nsummarized as follows:\n\tWe developed a multi-encoder and single-decoder \nbrain tumor segmentation network called CMAF-\nNet to solve the incomplete multimodal brain tumor \nsegmentation problem. Particularly, we designed a new CMAF module that can adequately model the \ncorrelation between multimodal data and handle \nthe case of missing modalities by fusing multimodal \nfeatures to learn modality-invariant shared \nrepresentations.\n\tT o fully utilize the respective advantages \nof Convolution and T ransformer for better \nsegmentation performance, we proposed a shift-\nwindow based 3D Swin block to capture long-\ndistance contextual interactions while maintaining \nlow computational complexity.\n\tIn order to segment brain tumors more accurately, \nwe introduced a spatial and channel self-attention \nmodule in the decoding stage to gradually aggregate \nmultimodal and multi-level features. With these \n2 modules, the network can selectively emphasize \ninformational features while suppressing less useful \nfeatures.\nThe rest of this paper is organized as follows. The ‘Methods’ \nsection elaborates the overall framework of the proposed \nCMAF-Net and the detailed structure of each module and the \n‘Experiments and results’ section describes the experimental \nsetup and details, as well as provides the experimental results \nand analysis. The ‘Discussion’ section presents the discussion \nof study implications and offers future research directions. In \nthe last section, the conclusion is presented.\nMethods\nThe proposed network architecture is first introduced. Each \ncomponent including the 3D Swin block and the CMAF \nmodule is then described in detail. Finally, details of the loss \nfunction and hyperparameters used in the training phase are \ndiscussed. The study was conducted in accordance with the \nDeclaration of Helsinki (as revised in 2013).\nNetwork architecture\nT o overcome the challenges of missing modalities in clinical \npractice, we proposed a brain tumor segmentation algorithm \nbased on CMAF-Net. Our CMAF-Net is based on a multi-\nencoder 3D U-Net (13) architecture using incomplete \nmultimodal MRI data with { } ,2 ,1 ,1 c e M FLAIR T T T= . It \ncomprises an encoder stage, a fusion stage, and a decoder \nstage, as shown in Figure 2. In the encoder phase, CMAF-\nNet takes multimodal MRI images as input. First, the \navailable MRI modalities are fed into each independent \n\nSun et al. CMAF-Net 4584\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9encoder to extract the features of different modalities. \nThen, the local feature maps of each modality generated by \nthe convolutional encoder are then fed into the constructed \n3D Swin block to model the long-range correlation. In the \nmultimodal fusion stage, we proposed a new multimodal \nfusion module called CMAF module. It efficiently fuses \nfeatures among different modalities to learn a modality-invariant shared representation corresponding with the \ntumor region and cope with the situations of missing \nmodalities. Finally, the decoder progressively aggregates \nmultimodal and multilevel features by using spatial and \nchannel self-attention modules and then upsamples the \nfused features to the original input resolution to make pixel-\nlevel predictions.Figure 2 An overview of our proposed network architecture, including encoder stage, fusion stage, and decoder stage. 3D Swin block is \nused to capture global information. The CMAF block is used to fuse multimodal features. CAM, channel attention module; SAM, spatial \nattention module; CMAF , cross-modal attention fusion; T2, T2-weighted; T1, T1-weighted; T1ce, contrast-enhanced T1-weighted; \nFLAIR, fluid-attenuated inversion recovery; 3D, three-dimensional; F , input feature map; C, channels of the feature map; H, height of \nfeature map; W, width of feature map; D, depth of feature map; CA, channel attention; Fc, feature map weighted by channel attention; Fs, \nfeature map weighted by spatial attention; SA, spatial attention; V , value; K, key; Q, query.\nCAM\nCAM\nCAM\nCAM\nSAM\nFlairLinear proj. Linear proj. Linear proj. Linear proj.\n3D Swin Block3D Swin Block3D Swin Block3D Swin BlockCross-modal attention fusion (CMAF)\nδFlair\nδT2\nQQ\nK,VK,V\nCross \nAttention\nFusion stage Decoder stageCross \nAttention\nδT1\nδT1ceReshape\nEncoder stage\nConcatenation \nSummation \nMultiplication \nBernoulli indicator\nChannel attention module CAM SAM Spatial attention moduleFReshape\nReshape\nReshape\nChannel attention module Spatial attention moduleSoftmaxCxCCA\nReshapeReshapeFc FFs\nCxHxWxDCxHxWxDCxHxWxDCxHxWxDV\nSoftmaxSA\nK\nQDown-sampling \nUp-sampling\nSegmentation head \nSkip connectionT2\nT1\nT1ce\n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4585\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Encoder stage\nIn the encoder stage, the hybrid encoder can efficiently \nbridge the convolutional encoder and the 3D Swin block to \nextract local and global features within a specific modality, \nrespectively. Specifically, given a multimodal 3D MRI \nimage {}1M\nmmxx== , 1 HW Dx×××∈ℜ , where M is the number \nof modalities, D is the number of slices, H and W are the \nheight and width of the inputs, respectively. A convolutional \nencoder is used to extract rich local 3D contextual features, \nand then 3D Swin blocks are constructed to model the \nlong-range dependencies of the extracted feature maps.\nConvolutional encoder\nThe convolution encoder is stacked by 333××  convolution \nblocks and downsampled by using a convolution with a step \nsize of 2. Specifically, we construct a five-stage encoder, \neach stage comprises two convolution blocks through the \nGroupNorm + ReLu + 3D Conv structure. The encoder \ngradually encodes each modal image into a low-resolution \nfeature map with local context 16 16 16DHWCL local\nmF×××−∈ℜ , []1,mM∈ ,  \nwhere ,,16 16 16DHD\n is 1/16 of the input spatial resolution H,  \nW, and depth dimension D; M is the number of modal \nimages. The channel dimension C and the number of stages \nL in the encoder are set to 128 and 5, respectively.\n3D Swin block \nT raditional CNNs can accurately segment brain tumors in \nlocal MRI images, but this method may not perform well \nfor challenges such as artifacts that may exist in brain tumor \nMRI images. For dense prediction tasks such as brain tumor \nsegmentation, it is crucial to consider both local and global \ninformation. CNNs excel in extracting local information, \nbut they have limited capability in extracting global \ninformation. Relying solely on local information may not \neffectively solve some of the complex problems mentioned \nabove. Several studies have confirmed that incorporating \nglobal information can enhance the model’s ability to \ncapture correlations among different parts of the image \nand help improve the segmentation effect (17,46). CNN \nand T ransformers are 2 mainstream architectures. CNN is \nproficient at extracting local features through convolutional \noperations, but its limited receptive field hinders its ability \nto capture global representations. Based on the shift \nwindow mechanism, Swin T ransformer retains the powerful \nglobal information extraction ability of the T ransformer. It offers the potential to address the aforementioned issues by \nefficiently processing information that deserves attention \nthrough the self-attention mechanism. Therefore, to \naddress the challenge posed by the convolutional encoder’s \ninability to capture long-range dependencies within each \nmodality, we proposed a 3D Swin block inspired by the \nmodule design in the Swin T ransformer (19). This block \nincorporates a multi-head self-attention (MSA) mechanism \nto facilitate effective long-range context modeling, thereby \nenhancing the model’s capacity to comprehend and process \nglobal information. The main difference is that we extend \nthe Swin T ransformer block to 3D structures. An important \nfactor limiting the T ransformer (16) structure’s application \nin medical image tasks is the consumption of excessive \ncomputational resources and storage space when an entire \nimage is converted into a sequence for self-attention \ncomputation. Consequently, inefficiency and performance \ndegradation ensue. Conversely, Swin T ransformer (19) \nis a method of calculating self-attention within a local \nwindow based on a shifted window, which greatly reduces \nthe number of parameters while showing better feature-\nlearning ability. The structure of the 3D Swin block is \nshown in Figure 3. It consists of 2 basic units, and each \nsubunit comprises a LayerNorm layer, an attention module, \na LayerNorm layer, and an MLP module. The first subunit \nuses the 3D windowed multi-head self-attention (3D \nW-MSA) module, whereas the second one uses the 3D \nshifted window multi-head self-attention mechanism (3D \nSW-MSA) module.\nThe 3D Swin block processes embeddings in a sequence-\nto-sequence manner based on an attention mechanism. The \nlinear embedding layer is responsible for converting the \nlocal feature map L local\nmF− generated by the convolutional \nencoder into non-overlapping patches. Each patch is \nconsidered a “token”. Subsequently, the patches are mapped \nto C (C is set to 512) dimensional vectors. For efficient \nmodeling, we propose to compute self-attention within a \nlocal window. By arranging the windows, the input feature \nmaps are evenly divided in a non-overlapping manner. \nAssuming that each window contains MMM××  patches, the \nstandard multi-head self-attention computation is performed \nseparately for each window. In the computation of W-MSA, \ninformation exchange is lacking due to the non-overlapping \nwindows being unconnected to one another, which limits its \nmodeling capability. T o solve this problem, cross-window \nconnectivity is introduced to transfer information among \nneighboring windows in SW-MSA while maintaining \n\nSun et al. CMAF-Net 4586\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9effective computation in non-overlapping windows. In SW-\nMSA, the 888××  feature map is uniformly partitioned \ninto 222××  windows of size () 444 4 M×× = . First, MSA \ncalculation is performed on each window. Second, the shift \nwindow mechanism is used to realize the communication \nbetween windows to achieve the effect of global modeling. \nThrough the above operations, information exchange \nbetween windows can be realized. We used W-MSA and \nSW-MSA alternately to achieve cross-window connection. \nUnder the shift structure, the entire calculation process of \n3D Swin Block is as follows:\n()()11 ˆ 3L LLZ DW MSA LN Z Z−−= −+  [1]\n()()ˆ ˆL LLZ MLP LN Z Z= + [2]()()1ˆ 3L LLZ DSW MSA LN Z Z+= −+  [3]\n()()1 11ˆ ˆL LLZ MLP LN Z Z+ ++= +  [4]\nwhere L denotes the block layer; ˆLZ and 1ˆLZ+ denote the \noutput features of the (S)W-MSA module and the MLP \nmodule, respectively. As described above, 3D Swin block \ncan effectively extract global and long-range dependencies \nthrough the self-attention mechanism and generates \nthe feature maps global\nmF  with global context within each \nmodality through 3D Swin blocks. The W-MSA module \nand the SW-MSA module primarily comprise the multi-\nhead self-attention mechanism and the trainable relative \nposition encoding. The specific formula is shown in Eq. [5]:\n() ,,T\nkQKAttention Q K V Softmax B V\nd\n= + [5]\nwhere Q, K, and V are the query, key, and value, respectively; \nkd is the dimension of the key; and B is the learnable \nrelative position encoding.\nFusion stage\nAccording to the clinical knowledge of radiologists \ndiagnosing brain tumors from multiple MRI modalities, \na strong structural correlation is known to exist between \nimaging modalities (47). The T1 and T1ce modalities are \nusually paired to detect the tumor core region, and the \nFLAIR and T2 modalities are often interpreted jointly. \nTherefore, a relatively strong correlation may exist between \nthese 2 pairs of modalities. In the fusion stage, we consider \nT1, T1ce as 1 modality (T1-T1ce) and FLAIR, T2 as the \nother modality (FLAIR-T2) because the difference between \nthem is very small (48). We cascade the embeddings from \nmodality-specific encoders into input multimodal tokens by \ncombining them 2-by-2 based on the correlation between \nthe modalities, defined respectively as follows:\n22 -2 ,token global global\nFlair Flair T T FLAIR T F Fδδ= [6]\n1 1 11 1- 1 ,token global global\nT ce T ce T T T T ce F F δδ= [7]\nwhere (),⋅⋅ indicates the concatenation operation. During \ntraining, we randomly set mδ to 0 to simulate the missing \nmodality. In case of missing modalities, the token for the \nmissing modalities is replaced by a zero vector.Figure 3 The structure of 3D Swin block. MLP , multi-layer \nperceptron; 3D W-MSA, 3D windowed multi-head self-attention; \n3D SW-MSA, 3D shifted window multi-head self-attention \nmechanism; 3D, three-dimensional.\nMLP\nLayer \nNorm\n3D W- \nMSA\nLayer \nNormLayer \nNorm3D SW- \nMSAMLP\nLayer \nNormL1Z+\nLˆZL+1ˆZ\nL1Z− LZ\n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4587\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Figure 4 The structure of CMAF Module. It consists of fusion block1 and fusion block2. CMAF , cross-modal attention fusion; V , value; K, \nkey; Q, query; Conv Proj, convolutional projection; FLAIR, fluid-attenuated inversion recovery; T2, T2-weighted; T1, T1-weighted; T1ce, \ncontrast-enhanced T1-weighted; FFN, feed forward network; MHA, multi-head attention.\nCMAF Module\nFusion \nBlock1\nCross \nattentionCross \nattentionFFN\nLayerNorm\nMHA\nLayerNorm\nT1 & T1ce Flair & T2 Flair & T2 T1 & T1ceConv \nProjConv \nProjv v\nv k qk k q q\nConv \nProjConv \nProjFusion \nBlock2\nMultimodal data, which offer a more comprehensive \nunderstanding of information than any single modality alone, \noften encounter the challenge of incomplete modalities. \nTherefore, the effective integration of these multimodal \ndata has become a crucial concern, particularly in scenarios \nwhere modalities are incomplete. Multimodal data fusion \ncan make full use of the complementary information among \nmultiple modal data to improve the segmentation accuracy. \nAccordingly, we propose a new cross-attention-based fusion \nmechanism, called CMAF module, which can fully model \nthe correlation between multimodal data and handle the case \nof missing modalities by fusing multimodal features to learn \nmodality-invariant shared representations. The motivation \nbehind the CMAF module is to mine the complementary \ninformation between multimodal data to stably establish \nlong-range dependency across modalities while learning \nfrom incomplete modalities more flexibly. The CMAF \nmodule is described in detail below.\nCMAF module\nThe CMAF module first receives the features of 2 different \nmodalities and subsequently fuses the features of the 2 \nmodalities by cross-attention. As shown in Figure 4, the \nCMAF module comprises 2 fusion blocks. Fusion block 1 fuses the features of T1-T1ce into FLAIR-T2. Fusion block \n2 fuses FLAIR-T2 to T1-T1ce. In the interactive fusion \nof available modality information, we use the principle of \ncross-attention (49) to realize the information interaction \namong different modalities. We have used fusion block \n1 as an example for illustration. We initially use linear \ntransformation to map the features of T1-T1ce to Q (query) \nvectors followed by the features of FLAIR and T2 to K (key) \nand V (value) vectors. The input triple ( ) Query, Key, Value  of \ncross-attention is calculated as follows:\n1- 1 ,\n-2 ,\n-2token\nq\ntoken\nk\ntoken\nvQ T T ce W\nK FLAIR T W\nV FLAIR T W=\n=\n= [8]\nwhere qW, kW, and vW are the weight matrix. For Q from \n1- 1T T ce , it fuses cross-modal information by attention \nweighting with K and V from -2 FLAIR T  while retaining \n1- 1T T ce  modality-specific information through residual \nconnections, and vice versa. In other words, cross-attention \nhighlights similar features between the 2 modalities by \nestablishing correlations between them. Considering that \nthese features can be extracted from both modality data, \nthey should have high confidence (29) and are important for \n\nSun et al. CMAF-Net 4588\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9representing correlation and shared information between \nmodalities. Given a token sequence from 2 different \nmodality images, the cross-attention (CA) is denoted as \nfollows:\n( ) 1- 1 , - 2T\ntoken token QKCA T T ce FLAIR T Softmax V\nd=  [9]\n( ) 1 1- 1 , - 2 1- 1token token tokeny CA T T ce FLAIR T T T ce= +  [10]\nwhere -2tokenFLAIR T  denotes the features of FLAIR  \nand 2T, 1- 1tokenT T ce  denotes the features of 1T and \n1T ce , 1y denotes the output of fusion block 1, and \n( ) 1- 1 , - 2token tokenCA T T ce FLAIR T  denotes cross-attention.\nDecoder stage\nIn the decoder stage, we first cascade the outputs 1y and 2y \nfrom the two fusion blocks of the fusion stage. This process \ncan be expressed as follows:\n()12,globalF Concat y y=  [11]\nwhere 1y and 2y denote the fused deep features of CMAF \nmodule output, respectively, and () Concat⋅ represents the \nconcatenation of channel dimensions.\nT o enhance the accuracy and robustness of the proposed \nmodel, we developed 2 independent self-attention \nmodules during the decoding stage: the channel attention \nmodule (CAM) and the spatial attention module (SAM). \nThese modules were designed to specifically address \nchannel characteristics and spatial location dependencies, \nrespectively. CAM and SAM can strengthen inter-channel \nrelationships and obtain a more comprehensive contextual \nrepresentation. For example, the literature (50) introduced \nan innovative attention mechanism that seamlessly \nintegrates features from 2 different perspectives: modal \nand spatial, for the purpose of multimodal fusion. This \ninnovative fusion approach has been shown to be highly \neffective across a range of tasks.\nThe decoder adopts a structure symmetric to the \nencoder, and the function of the convolutional decoder is \nto perform an upsampling operation on the fused feature \nrepresentations to recover the spatial resolution of the \nfeature map for final pixel-level classification. The output \nsequence globalF  of the fusion stage is reshaped into a high-\nlevel 4D feature map corresponding with the size before \nflattening. T o obtain spatially detailed features that enrich \nthe available modalities, we progressively cascade the feature maps obtained at each level of the encoder for different \nmodalities and use skip connection to fuse the encoder and \ndecoder features for finer segmentation. T o obtain a more \ncomprehensive representation of shared features and to \nfully utilize the information between multiple modalities, \na CAM is used to integrate low-level multimodal features. \nEach channel map is regarded as a modality-specific feature \nmap, with features from different modalities interacting and \ncorrelating with each other, which means that the network \ncan better understand the correlations between different \nmodalities, thus improving the representation of shared \nfeature maps for tumor regions. T o model the interaction of \nmultilevel features, low-level shared features from the CAM \nare downsampled with higher-level multimodal features \ncascaded to the decoder for brain tumor segmentation. \nMeanwhile, the multimodal fusion features globalF  from the \nfusion stage are cascaded with the previous level of shared \nfeatures learned by the channel self-attention module. \nThe SAM is then used to model the long-range spatial \nrelationships of the higher-level features.\nLoss function\nThe performance and robustness of the model can be \nimproved by using a joint loss function. Cross-entropy \nloss function is a basic loss function for medical image \nsegmentation. However, the cross-entropy loss function \ndoes not easily deal with category imbalance. Dice loss is \nused to evaluate the overlap between the predictions and \nthe ground truth, which can reduce the effect of category \nimbalance. Therefore, for end-to-end training of the entire \nnetwork, we use a loss that is the sum of the cross-entropy \ncrossL and the dice loss function diceL. This loss can be \nformulated as follows:\ndice cross LL L= +  [12]\n()11\n1112CN\nij ij ij\ndice CN\nij ij ijpg\nL\nPgε\nε= =\n= =+\n= −\n++∑∑\n∑∑ [13]\n()()() logcross l l xL gx px=−∑ [14]\nwhere N is the number of samples, C is the number of \nsegmentation classes, ijP is the predicted probability that \npixel i belongs to tumor class j, ijg is the actual probability \nthat pixel i belongs to tumor class j, ε is a small constant \nto avoid being divided by 0, ()lpx  is the estimated \nprobability that pixel x belongs to class l, and ()lgx  is the \n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4589\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9truth label of pixel x.\nHyperparameters selection\nThis subsection describes the hyperparameter settings \nchosen to produce efficient results. We selected the \nhyperparameters for model training after extensive \nexperimentation. T able 1 lists the hyperparameters used for \nthe training of the CMAF-Net model, where the learning \nrate is tuned smaller to fit the other hyperparameters. \nAdamW (27) can help the model converge faster. A batch \nsize of 1 reduces the demand for computational resources.\nWe chose AdamW (27) as the optimizer to achieve the best \nloss reduction during the training process. This optimization \ntechnique has an adaptive learning rate and weight decay \nmechanism, which can improve the model performance by \nautomatically adjusting the learning rate and controlling the \nmodel complexity according to the different conditions of \nthe parameters during the training process. As opposed to \nother optimization techniques (e.g., Sgdm or RMSprop), we \nchoose AdamW because of its easy implementation, efficient \nmemory utilization and faster learning rate. Recently, \nAdamW has shown excellent performance in deep learning \napplications such as medical image analysis (51).\nResults\nWe briefly describe herein the dataset and evaluation \nmetrics used for the experiments and then present a \ndescription of the implementation details. We demonstrate \nthe effectiveness of our proposed CMAF-Net method by \ncomparing it with other state-of-the-art methods on the \nBrain T umor Segmentation Challenge (BraTS) datasets. \nAblation experiments are performed to evaluate the \neffectiveness of each component of our CMAF-Net.Datasets and evaluation metric\nDatasets\nWe evaluated our CMAF-Net on the publicly available \nbrain tumor segmentation datasets BraTS 2018 and BraTS \n2020. BraTS 2018 and 2020 contain 285 and 369 annotated \nbrain tumor samples, respectively. Each case contains 4 \naligned modalities including FLAIR, T1, T1ce, and T2, \nas well as expert manually segmented labeled images. \nThe ground-truth labels provided by BraTS include \nenhanced tumor (ET), edema (ED), necrosis, and non-\nenhancing tumor core (NCR/NET) regions. Each modality \nis 240×240×155 in size and has been skull stripped, co-\nregistered, and resampled to a resolution of 1 mm3.  To  \nprovide a fair comparison, we divided the 285 case samples \nin the BraTS 2018 dataset into 190 cases as the training set \nand the other 95 cases as the validation set using the same \nsplit list as in (41), and we conducted experiments using  \n3-fold cross-validation. Regarding the BRATS 2020 dataset \ncontaining 369 training samples, we divided the dataset \naccording to the ratio 220:74:75 (training/validation/testing) \nin (36). We compared our method with other methods on 2 \ndatasets. In addition, as BraTS 2020 has a larger dataset size \ncompared with BraTS 2018, our ablation experiments were \nmainly conducted on this dataset in this study. \nEvaluation metrics\nIn our experiments, we used Dice similarity coefficient (DSC) \nand Hausdorff distance, which are commonly used in brain \ntumor segmentation research, to evaluate the segmentation \nresults. The DSC showed the similarity between the \nprediction results and the ground truth. A higher value of \nDSC corresponded with a better segmentation results. The \ncalculation formula of DSC is as follows:\n2PTDSCPT∩=+ [15]\nwhere P is the prediction result of the model, and T is the \nground-truth label.\nThe Hausdorff distance metric is a measure of boundary \nsimilarity to evaluate the distance between model prediction \nregion and ground truth region. The lower the value \nof Hausdorff distance, the better the predicted mask. \nHausdorff distance is defined as:\n() () { } min min max sup , ,sup ,pP gG d gp d pg Hausdorff distance∈∂ ∈∂ =  [16]\nwhere P∂ and G∂ denote the tumor boundary point sets of \nthe network prediction and the ground truth, respectively, Table 1 Hyperparameters of the proposed CMAF-Net model\nHyperparameters Values\nOptimizer AdamW\nEpochs 800\nBatchsize 1\nInitial learning rate 1e−4\nWeight decay 1e−5\nCMAF-Net, cross-modal attention fusion based deep neural \nnetwork.\n\nSun et al. CMAF-Net 4590\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9and ()min , d gp  indicates the minimum Euclidean distances \nbetween voxel g and voxels in a set P.\nImplementation details\nOur CMAF-Net was implemented on the PyT orch \nframework by using an Nvidia RTX 3090 (Nvidia, Santa \nClara, CA, USA). We used 3D U-Net (13) as the backbone \nnetwork for CMAF-Net. During model training, the \nAdamW optimizer (27) was used for optimization with an \ninitial learning rate of 0.0001. The input image size was \n128×128×128 voxels, the batch size was set to 1, and the \nnumber of training times was 800 epochs. A simple data \nenhancement strategy including random rotation, cropping, \nand random flipping was used. Following (42), we utilized \na modality missing mask to discard modality and simulate \nvarious missing modalities scenarios. During the training \nprocess, we randomly set some modalities as zero matrices \nto simulate the case of missing modalities (41). In the \ntesting phase, for the case of missing modalities, we set the \nmissing modalities as zero vectors.\nComparison with state-of-the-art methods on incomplete \nMRI data\nWe performed a series of comparative experiments on \nthe BraTS 2018 and BraTS 2020 datasets to demonstrate \nthe effectiveness of our proposed method. In Section \n(Comparison on BRATS 2020), we compare our method \nwith other state-of-the-art brain tumor segmentation \nmethods with missing modalities. In Section (Comparison \non BRATS 2018), we compare our method with state-of-\nthe-art full-modality methods.\nComparison on BRATS 2020\nT o evaluate the effectiveness of our proposed approach, we \ncompared our proposed CMAF-Net with several SOTA \nmodels. These models include 2 representative models \nU-HeMIS (40) and U-HVED (41) that learn shared \nfeature representations in the latent space, DIGEST (36), \nD2-Net (37), and GSS (38) that are based on teacher-\nstudent distillation framework, RFNet (43) based on CNN, \nand mmFormer (42) based on T ransformer. For a fair \ncomparison, we used the same data split (36) and reference \nthe results directly. At the same time, we reproduced the \nresults of mmFormer (42) on our data split by running the \nauthor’s code. T able 2 shows the quantitative results of all \nmethods on the BraTS 2020 dataset for 15 different missing MRI modalities. T able 2 shows that the average results of our \nmethod in most missing modality cases outperform those \nof the previous methods. Our method achieves the highest \nvalues for the average DSC metrics in the 3 nested tumor \nregions, namely, 87.9%, 81.8%, and 64.3%, respectively. \nNotably, in the difficult-to-segment tumor core region, our \nmethod outperformed other methods on most subsets of \nmissing modalities. This further supported the effectiveness \nof CMAF-Net for richer cross-modal information learning, \nwhich is more conducive to dealing with cases with missing \nmodalities. Our CMAF-Net performed better in almost all \ncases where 2 or 3 modalities were missing. Moreover, on \ntumor segmentation with only 1 available modality, Dice was \nimproved by 12.1% on average. This finding suggests that \nthe proposed CMAF-Net can successfully learn potential \nshared feature representations for brain tumor segmentation, \nshowing the effectiveness and robustness of the proposed \nmodel. Figure 5 provides a visual representation in the form \nof a bar graph, comparing the segmentation results predicted \nby our network and other methods on the BraTS 2020 \ndataset. It is illustrated that our method achieved outstanding \nsegmentation performance on this dataset. For qualitative \nanalysis, Figure 6 compares the segmentation results of our \nmethod with existing ones (40), (41), and (42) for a different \nnumber of missing modalities from the same patient. We can \nsee that our method produced more accurate segmentation \nresults in most of the cases and especially performs more \nclearly with stronger generalization ability when only 1 \nmodality was available.\nT o evaluate the robustness of our model to missing \nmodalities, we randomly selected several cases from the \nBraTS 2020 dataset. The segmentation results in the \ncase of missing modalities are shown in Figure 7. With \nincreased number of missing modalities, the segmentation \nresults progressively deteriorated; however, the decline \nwas not sudden and sharp, so we can still obtain a good \nsegmentation result. Furthermore, even with only 1 FLAIR \nmodality available, our CMAF-Net was able to segment the \nbrain tumor appropriately.\nComparison on BRATS 2018\nT o further evaluate the proposed method, we also compared \nthe segmentation performance with the state-of-the-\nart methods using 3-fold cross-validation on the BraTS \n2018 dataset. The comparison results are shown in T able \n3. From T able 3, we can see that our proposed model \nobtained the best average DSC in all 3 tumor regions \ncompared to other methods. Our method outperformed \n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4591\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Table 2 Comparison with different methods such as U-HeMIS (40), U-HVED (41), DIGEST (36), D2-Net (37), mmFormer (42), RFNet (43), and GSS (38) on the \nBraTS 2020 dataset\nMethodsType: FLAIR/T1/T1c/T2\nAVG\n○/○/○/●○/○/●/○○/●/○/○●/○/○/○○/○/●/●○/●/●/○●/●/○/○○/●/○/●●/○/○/●●/○/●/○●/●/●/○●/●/○/●●/○/●/●○/●/●/●●/●/●/●\nWT\nU-HeMIS 76.7 65.2 58.8 79.6 81.1 69.8 83.2 79.2 85.4 85.2 86.4 86.9 87.9 83.1 88.5 79.8\nU-HVED 80.9 65.4 83.5* 83.5 83.5 69.7 84.3 81.7 87.6 86.5 86.7 88.2 89.2 84.0 89.2 82.9\nDIGEST 78.6 74.4 68.6 84.2 85.5 77.6 85.8 84.3 87.8 88.6 88.5 88.8 89.7 86.0 90.2 83.9\nD2-Net 77.8 65.5 46.6 81.3 82.0 76.5 83.8 80.4 86.2 85.0 88.1 89.0 89.2 82.5 89.4 80.2\nmmFormer 85.8 78.1 79.2 85.8 87.5 80.7 88.1 87.4 89.9 89.3 89.7 89.8 90.2 88.0 90.1 86.6\nRFNet 86.1 78.5 79.7 86.3 87.9 81.4 89.0 88.2 89.7 89.9 90.0 90.0 90.4 88.2 90.3 87.0\nGSS 86.6 78.8 80.3 86.5 88.1 82.0 89.5 87.9 90.0 89.7 90.1 90.2 90.0 88.5 90.4 87.2\nOurs 86.9* 81.0* 81.4 88.3* 88.4* 83.1* 89.9* 88.3* 90.2* 90.1* 90.4* 90.4* 90.8* 88.8* 90.9* 87.9*\nTC\nU-HeMIS 50.7 68.3 38.9 51.6 74.6 70.8 55.9 52.7 59.6 73.4 74.6 60.7 76.8 76.1 77.1 64.1\nU-HVED 57.9 70.7 53.8 53.8 79.5 73.2 55.7 59.2 62.7 76.7 77.0 63.4 80.1 79.7 80.2 68.2\nDIGEST 60.1 83.3 54.5 60.4 87.1 84.1 65.0 63.8 67.8 86.1 85.6 67.5 87.2 86.1 87.0* 75.0\nD2-Net 58.8 71.4 43.2 55.5 82.6 79.3 58.6 62.7 68.5 79.7 79.2 65.0 82.2 83.1 81.6 70.1\nmmFormer 74.2 86.2 69.6 68.0 87.5 86.3 73.4 75.8 75.9 86.3 86.4 76.2 87.1 87.0 86.5 80.4\nRFNet 74.0 86.5 69.8 68.3 87.6* 86.2 73.5 75.6 76.4 86.7 86.6 76.4 87.4* 87.2 86.9 80.6\nGSS 74.8 85.8 71.6 71.4 86.5 86.1 75.0 75.4 76.6 86.4 86.8 77.3 87.0 86.7 85.7 80.9\nOurs 76.0* 86.6* 72.1* 75.4* 87.4 87.0* 76.3* 76.2* 77.1* 87.5* 87.2* 77.6* 86.7 87.6* 86.8 81.8*\nET\nU-HeMIS 21.2 66.2 13.1 26.5 71.8 68.2 28.4 25.3 20.8 71.6 74.1 33.0 72.9 72.5 73.4 49.3\nU-HVED 31.0 66.3 22.5 22.5 72.6 68.1 20.4 30.5 34.6 72.6 72.7 32.7 73.7 72.5 73.6 51.1\nDIGEST 39.1 78.3* 35.1 39.8 81.0 79.6 42.9 42.4 44.6 78.7 77.7 45.5 80.3* 82.3* 81.2* 61.9\nD2-Net 20.8 72.5 19.4 21.2 76.7 72.2 25.6 27.5 29.1 70.2 73.3 31.7 70.8 71.6 72.8 50.4\nmmFormer 44.9 75.9 40.4 42.2 77.2 77.2 45.3 47.2 49.4 77.0 77.0 49.9 76.6 78.5 76.5 62.3\nRFNet 45.0 76.3 41.2 42.4 78.0 77.5 46.1 47.8 48.7 77.6 77.2 50.0 76.8 78.7 76.9 62.7\nGSS 45.7 77.0 42.3 42.8 79.3 77.6 47.3 48.9 48.5 77.8 77.4 50.2 77.1 78.5 78.3 63.2\nOurs 46.9* 77.6 44.0* 44.9* 81.2* 79.8* 48.1* 49.8* 50.8* 78.8* 77.8* 51.3* 77.6 78.1 77.8 64.3*\nThe results reported in the table are the DSC for the different missing sceneries. The higher the DSC value, the better the results. ○ and ● indicate the \nmissing and available modalities, respectively. *, the best results. AVG denotes the average results on one target region across all the situations. FLAIR, fluid \nattenuated inversion recovery; T1, T1-weighted; T1ce, contrast-enhanced T1-weighted; T2, T2-weighted; WT, whole tumor; TC, tumor core; ET, enhancing \ntumor; DSC, dice similarity coefficient.\nthe current state-of-the-art method, GSS (38), with average \nDSC increases of 1.6%, 0.5%, and 0.6% in the WT, TC, \nand ET regions, respectively. Moreover, our proposed \nmethod outperformed the compared methods in most of \nthe modal combinations on BRATS 2018, indicating the \neffectiveness of our proposed method. Since our goal was to also help with segmentation in the absence of modalities, \nwe also compared the results of different numbers of \nmodalities as input with state-of-the-art methods, namely \nU-HeMIS (40), U-HVED (41), DIGEST (36), D2-Net (37),  \nmmFormer (42), RFNet (43), and GSS (38). The results are \nshown in T able 4. It can be seen that the proposed method \n\nSun et al. CMAF-Net 4592\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9is robust and superior to up-to-date approaches GSS (38),  \nregardless of using 1, 2, 3, or 4 modalities as input. \nWith the increase of the number of missing modalities, \nthe improvement of CMAF-Net was more impressive. \nThese results demonstrate the effectiveness of CMAF-\nNet in incomplete brain tumor segmentation. In Figure 8,   \nour method is compared with other methods on the BraTS \n2018 dataset using boxplots of DSC obtained from the \nsegmentation results of 15 missing modality cases and \nhistograms generated from average results. The figure \nillustrates the characterization of the distribution of the \nsegmentation results of the different methods for various \nmodality missing cases.\nIn addition, we also compare the Hausdorff Distance of \nour proposed method with other methods on both datasets, \nas shown in T able 5. Compared to other methods, CMAF-\nNet achieves optimal performance on Hausdorff Distance. \nThe results of the Hausdorff Distance evaluation indicate \nthat each region’s boundary is segmented more accurately.\nComparison with the state-of-the-art methods on full \nmodalities\nT o explore the performance of our model on full modalities, \nwe also compared it with the recent state-of-the-art \n3D U-Net (13), Attention U-Net (14), T ransBTS (17),  \nF2Net (30), and Swin-UNETR (20) on multimodal \nbrain tumor segmentation with full MRI modalities. The \nhyperparameters of all experiments are almost identical. We reproduced the results by using the official codebase. The \nresults of the different methods are compared in T able 6.  \nOur method achieved the best results in terms of average \nDice scores and average Hausdorff distance for all tumor \nregions segmented. These results show that our multimodal \nrepresentation of CMAF-Net learning is robust to missing \nmodalities and effective for full modalities.\nAblation study\nIn this section, by adding different components individually \nto the baseline model, we conducted a comprehensive \nablation experiment to evaluate the effectiveness of each \ncomponent in CMAF-Net. The baseline was the proposed \nmethod without using the 3D Swin module, the CMAF \nmodule, and the hybrid attention module. We present in \nT able 7 the quantitative results of the average DSC of the \n3 variants with CMAF-Net over all possible subsets of the \ninput modalities. When the 3D Swin module was applied \nto the baseline, the DSCs of ET, WT, and TC increased \nby 1.8%, 0.7%, and 0.7%, respectively. The above results \nshow that the proposed 3D Swin block can significantly \nimprove the segmentation performance, which is attributed \nto the fact that the 3D Swin block can model long-range \ndependencies and learn global and detailed features. On the \nbasis of the above architecture, the CMAF block was further \nintroduced into the network. The results showed that the \nDSC is improved in all 3 tumor regions. This finding can \nbe explained by the fact that CMAF further improves the \nfeature learning ability of the model by fully integrating the \ncomplementary information among different modalities \nand establishing long-range dependencies among them. \nAdditionally, a complete segmentation network model was \nconstructed by adding hybrid attention modules (SAM and \nCAM). The segmentation accuracy was found to further \nimprove, resulting in a 1.1% increase in the average DSC. \nWe effectively demonstrated that SAM can learn the \nimportance of different spatial regions, thereby enhancing \nthe feature representation. CAM can increase the network’s \nattention to key features to further improve the results. \nAs a result, the proposed CMAF-Net shows significant \nimprovement in DSC for all 3 types of tumor regions \ncompared with the baseline, showing that each module \ncontributes to the performance improvement. In order to \nverify the contribution of each term in our proposed joint \nloss function, we compared the performance differences \nof CMAF-Net using different loss functions. It was found \nthat the loss function of CMAF-Net removed any item Figure 5 Bar graph of the DSC of comparison experiments on the \nBraTS 2020 dataset. Error bars show standard error. DSC, dice \nsimilarity coefficient; ET, enhancing tumor; TC, tumor core; WT, \nwhole tumor. \n ET TC WTU-Hemis \nU-HVED \nDIGEST \nD2-NetmmFormer \nRFNet\nGSS \nOurs100\n80\n60\n40Dice, %\n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4593\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Figure 6 Comparison of segmentation results on four cases of missing modalities: complete modalities; FLAIR, T1ce, T2; FLAIR, T1ce; \nT1ce. From the left to right are 4 MRI modalities: T1, T2, FLAIR, and T1ce; the fifth column presents the ground truth of 2 patients, the \nsixth to eighth columns show the results of the state-of-the-art methods, and the rightmost column shows our segmentation results. T2, T2-\nweighted; FLAIR, fluid-attenuated inversion recovery; T1ce, contrast-enhanced T1-weighted; T1, T1-weighted; GT, ground truth; MRI, \nmagnetic resonance imaging.\n T1 T2 FLAIR T1ce GT (40) (41) (42) OursT1 \nMissingT1 \nMissingT1 \nMissingT1 \nMissingT1 \nMissingT1 \nMissingComplete T2 + FLAIR + T1ce FLAIR + T1ce T1ce\nT2 \nMissingT2 \nMissingT2 \nMissingT2 \nMissing\nFLAIR \nMissingFLAIR \nMissing\n\nSun et al. CMAF-Net 4594\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Figure 7 Examples of the segmentation results of CMAF-Net with various available modalities. FLAIR, fluid-attenuated inversion recovery; \nT1ce, contrast-enhanced T1-weighted; T2, T2-weighted; T1, T1-weighted; CMAF-Net, cross-modal attention fusion based deep neural \nnetwork.\nFLAIR modality FLAIR + T1ce + T2 + T1 FLAIR + T1ce Ground truth FLAIR + T1ce + T2 FLAIR\nwill reduce the segmentation performance of the model. \nThe quantitative evaluation shown in T able 8 verifies the \neffectiveness of the joint loss function proposed in this \npaper.\nIn order to demonstrate the effectiveness of each \nmodule in the proposed model on the segmentation results,  \nFigure 9 shows the visualization of the segmentation results \nwe obtained using different models under the four available \nmodal combinations. Compared with the baseline model, \nit is clearly shown that the segmentation performance can \nbe improved by adding the proposed modules, making \nthe segmentation results of our CMAF-Net closer to the \nground truth. T o show the distribution characteristics of the \nsegmentation results of the 4 methods, Figure 10 shows the \nablation experimental results of different modules in 75 test \nsamples, and the DSC are represented by boxplots and a \nbar graph. It can be seen from the figure that the proposed \nCMAF-Net presents fewer outliers, indicating that it has high stability and consistency. \nDiscussion\nBrain tumor segmentation is an important step in the \nclinical diagnosis and development of treatment plans. It \ncan provide a reference for assisted diagnosis and surgical \nplanning. The brain tumor segmentation of multimodal \nMRI images using deep learning methods has great \npotential and excellent performance. However, in the \nactual clinical data acquisition, due to various reasons such \nas equipment failure, data corruption, and human error, \nthe final collected data often have different degrees of \nmissing modality. Consequently, performance degradation \nof existing multimodal MRI brain tumor segmentation \nmethods occurs. T o solve the above problems, we propose in \nthis work a brain tumor segmentation network CMAF-Net \nbased on CMAF by using incomplete multimodal MRI data. \n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4595\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Table 3 Comparison with different methods such as U-HeMIS (40), U-HVED (41), DIGEST (36), D2-Net (37), mmFormer (42), RFNet (43) and GSS (38) on the \nBraTS 2018 dataset\nMethodsType: FLAIR/T1/T1c/T2\nAVG\n○/○/○/●○/○/●/○○/●/○/○●/○/○/○○/○/●/●○/●/●/○●/●/○/○○/●/○/●●/○/○/●●/○/●/○●/●/●/○●/●/○/●●/○/●/●○/●/●/●●/●/●/●\nWT\nU-HeMIS 80.9 61.5 57.6 52.4 82.4 68.4 64.6 82.4 82.9 68.9 72.3 83.4 83.8 83.9 84.7 74.0\nU-HVED 79.8 53.6 49.5 84.3 81.3 64.2 85.7 81.5 87.5 85.9 86.7 88.0 88.0 82.3 88.4 79.1\nDIGEST 80.4 77.2 61.6 84.5 83.6 70.8 86.9 83.3 88.8 86.4 87.0 88.7 89.4 83.4 89.9 82.8\nD2-Net 76.3 42.8 15.5 84.2 84.1 62.1 87.3 80.1 87.9 87.5 87.7 88.4 88.8 80.9 88.8 76.2\nmmFormer 84.5 74.1 72.0 86.2 86.3 77.0 87.1 85.6 89.0 87.9 88.5 89.3 89.8 86.7 90.0 84.9\nRFNet 85.1 73.6 74.8 85.8 85.6 77.5 87.4 86.7 89.2 88.7 88.9 89.4 90.0 86.9 90.1 85.3\nGSS 85.7 75.8 75.4 86.7 86.9 78.5 88.1 87.0 89.3 89.2 89.4 89.7 90.2 87.7 90.2 86.0\nOurs 86.9* 81.8* 80.9* 87.9* 88.2* 83.3* 89.2* 87.9* 89.6* 89.6* 89.8* 89.8* 90.4* 88.5* 90.4* 87.6*\nTC\nU-HeMIS 57.2 65.2 37.3 26.0 76.6 72.4 41.1 60.9 57.6 71.4 76.0 60.3 77.5 78.9 79.4 62.5\nU-HVED 54.6 59.5 33.9 57.9 73.9 67.5 61.1 56.2 62.7 75.0 77.0 63.1 76.7 75.2 77.7 64.8\nDIGEST 61.3 78.7 60.6 62.5 81.4 75.8 69.6 64.2 71.1 79.4 78.3 65.5 83.5 82.6 81.8 73.1\nD2-Net 56.7 65.1 16.8 47.3 80.3 78.2 61.6 63.2 62.6 80.8 80.9 63.7 80.7 79.0 80.1 66.5\nmmFormer 70.5 82.1 68.7 68.3 83.0 82.1 72.4 73.4 72.5 83.1 82.9 73.8 83.0 82.8 82.6 77.4\nRFNet 70.7 82.4 69.2 68.9 83.2 82.7 73.1 73.8 73.7 82.7 82.7 74.6 83.2 83.3 83.1 77.8\nGSS 71.2 82.8 70.4 73.6* 82.8 83.0 74.0 74.4 74.1 83.0 82.9 75.7 83.6 83.5 83.4 78.6\nOurs 73.5* 83.1* 71.5* 72.5 83.5* 83.2* 75.2* 75.3* 75.2* 83.2* 83.1* 76.2* 83.9* 83.7* 83.6* 79.1*\nET\nU-HeMIS 25.6 62.0 10.1 11.7 67.8 66.2 10.7 32.3 30.2 66.1 68.5 31.0 68.7 69.9* 70.2* 46.0\nU-HVED 22.8 57.6 8.6 23.8 67.8 61.1 27.9 24.2 32.3 68.3 68.6 32.3 68.9 67.7 69.0 46.8\nDIGEST 32.3 62.2 33.7 32.0 68.4 68.0 38.6 40.4 39.7 67.9 68.0 37.5 68.1 69.5 67.8 52.9\nD2-Net 16.0 66.3 8.1 8.1 68.5 68.0 9.5 16.5 17.4 64.8 65.7 19.4 66.4 68.3 68.4 42.1\nmmFormer 36.0 65.8 35.2 32.9 67.1 67.0 37.5 41.3 39.8 66.8 67.0 40.8 67.9 67.2 68.0 53.4\nRFNet 36.5 66.4 35.9 33.5 67.6 67.4 38.8 40.7 40.3 67.1 67.6 41.5 68.4 67.6 67.8 53.8\nGSS 36.8 67.0 36.4 37.9* 68.0 67.7 40.9* 41.1 40.6 67.4 68.0 44.3* 68.7 68.2 68.1 54.7\nOurs 37.6* 67.8* 37.2* 37.5 68.7* 68.4* 40.5 41.5* 41.8* 68.5* 68.8* 44.0 69.1* 69.0 68.5 55.3*\nThe results reported in the table are the DSC for the different missing sceneries. The higher the DSC value, the better the results. ○ and ● indicate the \nmissing and available modalities, respectively. *, the best results. AVG denotes the average results on one target region across all the situations. FLAIR, fluid \nattenuated inversion recovery; T1, T1-weighted; T1ce, contrast-enhanced T1-weighted; T2, T2-weighted; WT, whole tumor; TC, tumor core; ET, enhancing \ntumor; DSC, dice similarity coefficient.\nFirst, we proposed a 3D Swin block based on a shift window \nto capture long-range context information. Long-range \ncontext information is very important in medical image \nsegmentation. When segmenting tumor tissues that are \nfar away from the tumor core region and have no obvious \ntumor features, CNN-based models can lead to insufficient modeling of global information due to the limitation of \nreceptive fields (52). Thus, the segmentation results are \naffected. Given that the convolutional operation in CNN \ncan capture only local information, the T ransformer model \ncan better capture long-range dependencies through the \nself-attention mechanism. Compared with the traditional \n\nSun et al. CMAF-Net 4596\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9T ransformer, the 3D Swin block introduces a shift-window \nmechanism to achieve window-to-window communication. \nIt reduces the amount of computation and introduces a \nlocality prior, so it can be better applied to the segmentation \ntask. Brain tumors are usually highly localized (53) and \nspatially variable, requiring the modeling of features at different scales and different locations. Therefore, applying \n3D Swin block based on shift window to the brain tumor \nsegmentation task is reasonable. As shown in T able 7,  \nthe results of ablation experiments demonstrated that \nour proposed 3D Swin block can effectively improve the \nsegmentation performance and prove its effectiveness. \nSecond, we designed a cross-attention based CMAF \nmodule to deal with the missing modality cases by fusing \nfeatures from different modalities to learn shared potential \nrepresentations. It is able to adaptively learn the correlations \nbetween modalities and thus better capture complementary \ninformation. Specifically, the CMAF module can effectively \nutilize the complementarities between different modalities \nduring training. Even if some modalities are missing, \nthe network can still synthesize the information of other \navailable modalities to infer the features of the missing \nmodalities, maintaining accuracy and stability. Finally, the \nchannel and spatial self-attention modules are used in the \ndecoding stage to gradually aggregate multimodal and \nmultilevel features and better focus on the key information \nin the feature map, thereby further improving the accuracy \nof segmentation. For the task of brain tumor image \nsegmentation, the use of the attention mechanism extracts \nricher semantic information and pays more attention to the \ninformation of small-area brain tumors. Consequently, the \nsegmentation effect of brain tumors is improved (54).\nExisting methods for incomplete multimodal brain \ntumor segmentation mainly differ in their fusion strategies. \nA common approach is to calculate the mean and variance \nof each available modality and fuse the corresponding \nfeatures with equal importance. However, this method \nmay not efficiently aggregate the features of the missing \nmodalities, potentially compromising the effectiveness \nof fusion. Another approach involves directly integrating \nmodal features through convolution. However, the fusion \nstrategy based on convolution may not be adequate for \nintegrating global information due to the localized nature \nof convolution operations and the inherent bias of weight \nsharing. This limitation inevitably hinders the modeling \nof remote dependencies. In contrast, the CMAF module \nutilizes a cross-attention mechanism that is able to capture \ncorrelations between different modalities and interactively \npropagate information in a dynamic and learnable manner. \nIn this way, global information and remote dependencies \ncan be captured effectively. Therefore, introducing the \nCMAF module to supplement the global information is a \nsimple solution. In the field of computer vision, multimodal \nfusion is important for joint modeling and understanding Table 4 Improvements of CMAF-Net upon U-HeMIS (40), \nU-HVED (41), DIGEST (36), D2-Net (37), mmFormer (42), \nRFNet (43), and GSS (38) with different numbers of input \nmodalities evaluated by DSC (%)\nMethodsNumbers of input modalities\n1 2 3 4\nWT\nU-HeMIS +85.1 +78.2 +35.1 +5.7\nU-HVED +70.3 +41.7 +13.5 +2.0\nDIGEST +34.7 +28.0 +10.0 +0.5\nD2-Net +118.7 +38.8 +12.7 +1.6\nmmFormer +20.7 +14.9 +4.2 +0.4\nRFNet +18.2 +12.7 +3.3 +0.3\nGSS +13.9 +8.8 +1.5 +0.2\nTC\nU-HeMIS +114.9 +95.6 +34.2 +4.2\nU-HVED +94.7 +79.2 +34.9 +5.9\nDIGEST +37.5 +34.1 +17.0 +1.8\nD2-Net +114.7 +48.9 +22.6 +3.5\nmmFormer +11.0 +9.1 +4.4 +1.0\nRFNet +9.4 +6.8 +3.1 +0.5\nGSS +2.6 +4.3 +1.2 +0.2\nET\nU-HeMIS +70.7 +56.1 +12.8 -1.7\nU-HVED +67.3 +47.8 +11.2 -0.5\nDIGEST +19.9 +6.4 +7.8 +0.7\nD2-Net +81.6 +84.7 +31.1 +0.1\nmmFormer +10.2 +9.9 +8.0 +0.5\nRFNet +7.8 +7.5 +5.8 +0.7\nGSS +2.0 +3.7 +1.7 +0.4\nCMAF-Net, cross-modal attention fusion-based deep neural \nnetwork; 3D, three-dimensional; DSC, dice similarity coefficient; \nWT, whole tumor; TC, tumor core; ET, enhancing tumor.\n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4597\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Figure 8 Boxplots and bar graph of the DSC of the comparison experiment results. (A) The results for 15 cases of missing modalities. (B) \nThe average results of different methods and ours. Error bars show standard error. ET, enhancing tumor; TC, tumor core; WT, whole \ntumor; DSC, dice similarity coefficient.\n ET TC WT ET TC WTU-Hemis \nU-HVED \nDIGEST \nD2-Net \nmmFormer \nRFNet \nGSS \nOursU-Hemis \nU-HVED \nDIGEST \nD2-NetmmFormer \nRFNet\nGSS \nOurs100\n80\n60\n40\n20\n0100\n80\n60\n40Dice, %\nDice, %A B\nTable 5 Comparison of different methods in terms of average \nHausdorff distance on BraTS 2018 and BraTS 2020 datasets\nDataset MethodAverage Hausdorff distance\nET TC WT\nBraTS 2018 U-HeMIS 15.38 17.62 16.22\nU-HVED 15.11 16.63 14.87\nDIGEST 8.93 10.11 10.06\nD2-Net 11.74 13.67 13.54\nmmFormer 7.32 7.54 6.82\nRFNet 7.09 7.30 6.63\nGSS 6.54 6.87 5.91\nOurs 5.95* 6.59* 4.38*\nBraTS 2020 U-HeMIS 15.40 18.37 20.32\nU-HVED 13.86 16.65 21.00\nDIGEST 8.72 12.70 13.05\nD2-Net 11.48 8.64 5.67\nmmFormer 8.09 5.94 5.01\nRFNet 7.62 6.15 4.82\nGSS 6.73 5.79 4.65\nOurs 4.02* 5.35* 4.21*\n*, the best results. Average Hausdorff distance denotes the \naverage results on 1 target region across all the situations. ET, \nenhancing tumor; TC, tumor core; WT, whole tumor. between images and texts. By integrating image and text \ndata from multiple modalities, better image description \ngeneration, visual question and answer, and other tasks can \nbe realized (55). Our CMAF-Net is flexible and scalable \nin processing multimodal data and solving segmentation \ntasks, where the multimodal fusion module can be easily \nadapted to other multimodal network architectures and \nresearch areas. This makes our approach promising for a \nwide range of applications in various fields. In addition, the \ncomplete training and inference process of our CMAF-Net \nis conducted in an end-to-end manner, and the method is \ngeneralized to handle any number of modality data, which \ncan help segmentation even if some modalities are missing.\nExtensive segmentation results were compared with \nother methods on incomplete and complete MRI data \nfrom the BraTS 2020 dataset. The results showed that our \nmethod solves the problem of missing modalities in brain \ntumor segmentation and obtains the feature information \nin multimodal data, thereby performing effective feature \nfusion to segment brain tumors more accurately. This \nfinding fully demonstrates the effectiveness of CMAF-Net \nand the advantages of utilizing multimodal data fusion. \nCurrently, the use of this method can help to segment brain \ntumors when 1 or more modalities are missing and obtain \nbetter segmentation results, which has great value in clinical \npractice. In actual clinical practice, doctors need to use \nimage data for tumor localization and evaluation to develop \n\nSun et al. CMAF-Net 4598\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9treatment plans (56). If the tumor segmentation model can \nprovide reliable results despite missing modalities, it will \nbecome an important auxiliary tool for doctors and help \nthem understand the patient’s condition more accurately to support clinical decision making. In addition, a tumor \nsegmentation model that handles missing modalities can \nstreamline clinical workflows and reduce the extra time and \neffort consumed by physicians dealing with missing data. \nBy integrating the model into medical imaging devices \nor diagnostic software, physicians can obtain accurate \ntumor segmentation results more quickly, thereby saving \ndiagnostic time and improving diagnostic accuracy. This \nintegration can increase productivity by allowing physicians \nto focus more on diagnosis and treatment planning \nrather than spending a significant amount of time in the \nprocess of segmenting tumors. With the model’s accurate \ntumor segmentation results, doctors are able to develop \npersonalized treatment plans with a clearer understanding \nof the tumor’s size, shape, location, and relationship to \nsurrounding tissues. This helps to optimize surgical and \nradiotherapy protocols, monitor tumor growth, assess \ntreatment efficacy, and adjust treatment strategies for Table 6 Comparison of full modality performance of different methods\nMethodDSC (%) (mean ± SD) Hausdorff distance\nWT TC ET Average WT TC ET Average\n3D U-Net (13) 87.3±0.321 80.7±0.351 73.8±0.439 80.6 5.68 5.19 7.24 6.04\nAttention U-Net (14) 87.8±0.265 83.4±0.323 74.8±0.404 82.0 5.37 5.49 7.07 5.98\nTransBTS (17) 88.3±0.200 85.8±0.252 77.5±0.321 83.9 5.60 8.13 6.21 6.65\nF2Net (30) 91.2±0.252 85.4±0.264 77.0±0.350 84.5 3.73 6.02 4.93 4.89\nSwin-UNETR (20) 91.4*±0.208 86.2±0.305 76.9±0.360 84.8 3.61* 4.48 8.39 5.49\nOur 90.9±0.200 86.8*±0.231 77.8*±0.265 85.2* 3.65 4.45* 6.16* 4.75*\n*, the best results. Average indicates the average results of the 3 target regions. Higher DSC indicate better results, while lower Hausdorff \ndistance indicate better results. DSC, dice similarity coefficient; SD, standard deviation; WT, whole tumor; TC, tumor core; ET, enhancing \ntumor; 3D, three-dimensional.\nTable 7 Ablation study of critical components of CMAF-Net \nMethodAverage DSC (%)\n WT TC ET Average\nBaseline 85.8 77.9 60.3 74.7\nBaseline + 3D Swin block 86.5 78.6 62.1 75.7\nBaseline + 3D Swin block + CMAF 87.1 80.4 63.3  76.9\nBaseline + 3D Swin block + CMAF + CAM 87.5 81.1 64.0  77.5\nCMAF-Net 87.9* 81.8* 64.3*  78.0*\n*, the best results. CMAF-Net, cross-modal attention fusion-based deep neural network; DSC, dice similarity coefficient; WT, whole tumor; \nTC, tumor core; ET, enhancing tumor; 3D, three-dimensional; CMAF , cross-modal attention fusion; CAM, channel attention module.\nTable 8 Performance comparison of different loss functions in \nCMAF-Net\nLoss functionAverage DSC (%)\nWT TC ET Average\ncrossL 86.8 81.4 63.8 77.3\ndiceL 87.4 81.1 64.0 77.5\nL 87.9* 81.8* 64.3* 78.0*\n*, the best results. CMAF-Net, cross-modal attention fusion-\nbased deep neural network; DSC, dice similarity coefficient; \nWT, whole tumor; TC, tumor core; ET, enhancing tumor; L, loss \nfunction; cross, the cross-entropy.  \n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4599\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9better clinical outcomes. Advances in artificial intelligence, \nespecially deep learning, have enabled the development \nof automated brain tumor segmentation techniques that \novercome manual segmentation methods reliant on heavy \nlabor and subjective operator judgment. Beyond its impact \non brain tumor segmentation, AI has broader implications \nin medical imaging. For example, AI has the potential to \nhelp radiologists to identify and prioritize patients with the \nmost severe or complex cases (57). Additionally, compared \nto the traditional imaging workflow that heavily relies on \nhuman labor, AI enables more safe, accurate, and efficient \nimaging solutions (58).\nHowever, the limitation of this method is the inability to \nrecover missing modalities, and accurate clinical diagnosis \nand treatment rely on complete and accurate data. In \nfuture work, we will try to explore multi-task learning to \ncomplement the missing modalities and thus provide richer \nimage information for brain tumor diagnosis. We also \nplan to extend our proposed CMAF-Net to the problem \nof segmentation of completely different modalities [e.g., \ncomputed tomography (CT) and MRI] and explore more \nways to further improve the proposed method’s performance. \nAt the same time, we also realize that due to the complexity \nof the model, CMAF-Net has specific limitations of large memory and time resource usage during training. \nAlthough the introduction of new modules can bring higher \ncharacterization capabilities to the model, it also bears the \nlimitations of more parameters, difficult optimization, and \nthe need for more data volume. Therefore, we will further \noptimize the model in the future while reducing memory \nconsumption and making it more lightweight.\nConclusions\nWe have proposed an effective CMAF-based 3D \nsegmentation model CMAF-Net for incomplete multimodal \nbrain tumor segmentation. It fully combines the advantages \nof CNN and T ransformer structures to improve the ability \nof extracting local features and global context information. \nWe also designed a cross attention-based multimodal \nfeature fusion module that models long-range correlations \nbetween different modalities and learns modality-invariant \nshared feature representations. Ablation experiments \nvalidated the effectiveness of key components in CMAF-\nNet. The experimental results on the BraTS 2018 and \nBraTS 2020 datasets showed that the proposed method \nachieves excellent performance and robustness in dealing \nwith multimodal brain tumor segmentation with missing Figure 9 Visual comparison of the effects of different components of CMAF-Net in the ablation study. T1ce, contrast-enhanced T1-\nweighted; T1, T1-weighted; FLAIR, fluid attenuated inversion recovery; T2, T2-weighted; 3D, three-dimensional; CMAF , cross-modal \nattention fusion; CAM, channel attention module; CMAF-Net, cross-modal attention fusion-based deep neural network; GT, ground truth.\nT1ce \nT1 \nFLAIR \nT2 Baseline Baseline + 3D \nSwin blockBaseline + 3D Swin \nblock + CMAFBaseline + 3D Swin \nblock + CMAF + CAMCMAF-Net GTFLAIR + T1ce \n+ T2 + T1FLAIR + T1ce + \nT2 FLAIR + T1ce FLAIR\n\nSun et al. CMAF-Net 4600\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Figure 10 Boxplots and bar graph of the DSC of ablation experiments for different modules. (A-C) The DSC of WT, TC, and ET, \nrespectively. (D) The bar chart comparison result of different components. 3D, three-dimensional; CMAF , cross-modal attention fusion; \nCAM, channel attention module; CMAF-Net, cross-modal attention fusion based deep neural network; FLAIR, fluid attenuated inversion \nrecovery; T1ce, contrast-enhanced T1-weighted; T2, T2-weighted; WT, whole tumor; TC, tumor core; ET, enhancing tumor; DSC, dice \nsimilarity coefficient.\n Flair Flair + T1ce Flair + T1ce + T2 Complete\nWT\n Flair Flair + T1ce Flair + T1ce + T2 Complete\nFT Flair Flair + T1ce Flair + T1ce + T2 Complete\nTCBaseline \nBaseline + 3D Swin block \nBaseline + 3D Swin block + CMAF \nBaseline + 3D Swin block + CMAF + CAM \nCMAF-Net100\n80\n60\n40\n20\n0\n100\n80\n60\n40\n20\n0100\n80\n60\n40100\n80\n60\n40\n20\n0DSC, % DSC, %\nDSC, %Baseline\nBaseline + 3D \nSwin blockBaseline + 3D Swin \nblock + CMAF + CAM\nCMAF-Net\nBaseline + 3D Swin block \n+ CMAF\n ET TC WTDSC, %A\nCB\nD\nmodalities. In the future, we will continue to explore how to \nimprove our multimodal model so that it can be applied to \nreal-world clinical scenarios and improve the performance \nof brain tumor segmentation.\nAcknowledgments \nFunding:  This work was supported by the National \nNatural Science Foundation of China (No. 62371243, to \nX.N.), the General Program of Natural Foundation of \nJiangsu Province (No. BK20231190, to X.N.), the General \nProgram of Jiangsu Provincial Health Commission (No. \nM2020006, to X.N.), the Jiangsu Provincial Medical Key \nDiscipline Cultivation Unit of Oncology Therapeutics \n(Radiotherapy) (No. JSDW202237, to X.N.), the Jiangsu \nProvincial Key Research and Development Program Social Development Project (No. BE2022720, to X.N.), \nand the Changzhou Social Development Project (No. \nCE20235063, to X.N.).\nFootnote\nConflicts of Interest: All authors have completed the ICMJE \nuniform disclosure form (available at https://qims.\namegroups.com/article/view/10.21037/qims-24-9/coif). \nThe authors have no conflicts of interest to declare. \nEthical Statement: The authors are accountable for all \naspects of the work in ensuring that questions related \nto the accuracy or integrity of any part of the work are \nappropriately investigated and resolved. The study was \nconducted in accordance with the Declaration of Helsinki (as \n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4601\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9revised in 2013).\nOpen Access Statement: This is an Open Access article \ndistributed in accordance with the Creative Commons \nAttribution-NonCommercial-NoDerivs 4.0 International \nLicense (CC BY-NC-ND 4.0), which permits the non-\ncommercial replication and distribution of the article with \nthe strict proviso that no changes or edits are made and the \noriginal work is properly cited (including links to both the \nformal publication through the relevant DOI and the license). \nSee: https://creativecommons.org/licenses/by-nc-nd/4.0/.\nReferences\n1. Krishna PR, Prasad V , Battula TK. Optimization \nempowered hierarchical residual VGGNet19 network for \nmulti-class brain tumour classification. Multimed T ools \nAppl 2023;82:16691-716.\n2. Sung H, Ferlay J, Siegel RL, Laversanne M, \nSoerjomataram I, Jemal A, Bray F . Global Cancer Statistics \n2020: GLOBOCAN Estimates of Incidence and Mortality \nWorldwide for 36 Cancers in 185 Countries. CA Cancer J \nClin 2021;71:209-49.\n3. Zhu Z, He X, Qi G, Li Y, Cong B, Liu Y. Brain tumor \nsegmentation based on the fusion of deep semantics \nand edge information in multimodal MRI. Inf Fusion \n2023;91:376-87.\n4. Rehman MU, Ryu J, Nizami IF , Chong KT. RAAGR2-\nNet: A brain tumor segmentation network using parallel \nprocessing of multiple spatial frames. Comput Biol Med \n2023;152:106426.\n5. Menze BH, Jakab A, Bauer S, Kalpathy-Cramer J, \nFarahani K, Kirby J, et al. The Multimodal Brain T umor \nImage Segmentation Benchmark (BRATS). IEEE T rans \nMed Imaging 2015;34:1993-2024.\n6. Diao Y, Li F , Li Z. Joint learning-based feature \nreconstruction and enhanced network for incomplete \nmulti-modal brain tumor segmentation. Comput Biol Med \n2023;163:107234.\n7. Peng Y, Sun J. The multimodal MRI brain tumor \nsegmentation based on AD-Net. Biomed Signal Process \nControl 2023;80:104336.\n8. Wang Y, Zhang Y, Liu Y, Lin Z, Tian J, Zhong C, Shi Z, \nFan J, He Z. ACN: Adversarial co-training network for \nbrain tumor segmentation with missing modalities. In: \nde Bruijne M, Cattin PC, Cotin S, Padoy N, Speidel S, \nZheng Y, Essert C. editors. Medical Image Computing \nand Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September \n27–October 1, 2021, Proceedings, Part VII 24. Springer, \n2021:410-20.\n9. Badjie B, Ülker ED. A Deep T ransfer Learning Based \nArchitecture for Brain T umor Classification Using MR \nImages. Inf T echnol Control 2022;51:332-44.\n10. Khan MA, Khan A, Alhaisoni M, Alqahtani A, Alsubai S, \nAlharbi M, Malik NA, Damaševi čius R. Multimodal brain \ntumor detection and classification using deep saliency map \nand improved dragonfly optimization algorithm. Int J \nImaging Syst T echnol 2023;33:572-87.\n11. Kurdi SZ, Ali MH, Jaber MM, Saba T, Rehman A, \nDamaševi čius R. Brain T umor Classification Using Meta-\nHeuristic Optimized Convolutional Neural Networks. J \nPers Med 2023;13:181.\n12. Rajinikanth V , Kadry S, Nam Y. Convolutional-Neural-\nNetwork Assisted Segmentation and SVM Classification of \nBrain T umor in Clinical MRI Slices. Inf T echnol Control \n2021;50:342-56.\n13. Çiçek Ö, Abdulkadir A, Lienkamp SS, Brox T, \nRonneberger O. 3D U-Net: learning dense volumetric \nsegmentation from sparse annotation. In: Ourselin S, \nJoskowicz L, Sabuncu M, Unal G, Wells W. editors. \nMedical Image Computing and Computer-Assisted \nIntervention–MICCAI 2016: 19th International \nConference, Athens, Greece, October 17-21, 2016, \nProceedings, Part II 19. Springer, 2016:424-32.\n14. Oktay O, Schlemper J, Folgoc LL, Lee M, Heinrich M, \nMisawa K, Mori K, McDonagh S, Hammerla NY, Kainz B, \nGlocker B, Rueckert D. Attention u-net: Learning where \nto look for the pancreas. arXiv:1804.03999, 2018.\n15. Milletari F , Navab N, Ahmadi SA. V-net: Fully \nconvolutional neural networks for volumetric medical \nimage segmentation. 2016 fourth international conference \non 3D vision (3DV) 2016:565-71.\n16. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, \nGomez AN, Kaiser L, Polosukhin I. Attention is all you \nneed. Advances in Neural Information Processing Systems \n2017;30:6000-10.\n17. Wang W, Chen C, Ding M, Li J, Yu H, Zha S. T ransbts: \nMultimodal brain tumor segmentation using transformer. \nIn: de Bruijne M, Cattin PC, Cotin S, Padoy N, Speidel \nS, Zheng Y, Essert C. editors. Medical Image Computing \nand Computer Assisted Intervention – MICCAI 2021. \nMICCAI 2021. Lecture Notes in Computer Science, \nSpringer, 2021:109-19.\n18. Chen J, Lu Y, Yu Q, Luo X, Adeli E, Wang Y, Lu L, \nYuille AL, Zhou Y. T ransunet: T ransformers make \n\nSun et al. CMAF-Net 4602\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9strong encoders for medical image segmentation. \narXiv:2102.04306, 2021.\n19. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo \nB. Swin transformer: Hierarchical vision transformer \nusing shifted windows. Proceedings of the IEEE/\nCVF International Conference on Computer Vision; \n2021:10012-22.\n20. Hatamizadeh A, Nath V , T ang Y, Yang D, Roth H, Xu D. \nSwin unetr: Swin transformers for semantic segmentation \nof brain tumors in MRI images. In: Crimi A, Bakas S. \neditors. Brainlesion: Glioma, Multiple Sclerosis, Stroke \nand T raumatic Brain Injuries. BrainLes 2021. Lecture \nNotes in Computer Science, Springer, 2021:272-84.\n21. Lamrani D, Cherradi B, El Gannour OE, Bouqentar MA, \nBahatti L. Brain tumor detection using mri images and \nconvolutional neural network. Int J Adv Comput Sci Appl \n2022;13:452-60.\n22. Asif S, Zhao M, T ang F , Zhu Y. An enhanced deep \nlearning method for multi-class brain tumor classification \nusing deep transfer learning. Multimed T ools Appl \n2023;82:31709-36.\n23. Acar E, Levin-Schwartz Y, Calhoun VD, Adali T. \nACMTF for fusion of multi-modal neuroimaging data \nand identification of biomarkers. 2017 25th European \nSignal Processing Conference (EUSIPCO), Kos, Greece, \n2017:643-7.\n24. Rahaman MA, Chen J, Fu Z, Lewis N, Iraji A, Calhoun \nVD. Multi-modal deep learning of functional and \nstructural neuroimaging and genomic data to predict \nmental illness. Annu Int Conf IEEE Eng Med Biol Soc \n2021;2021:3267-72.\n25. Nie D, Wang L, Gao Y, Shen D. Fully convolutional \nnetworks for multi-modality isointense infant brain image \nsegmentation. Proc IEEE Int Symp Biomed Imaging \n2016;2016:1342-5.\n26. Peiris H, Hayat M, Chen Z, Egan G, Harandi M. A \nrobust volumetric transformer for accurate 3D tumor \nsegmentation. In: Wang L, Dou Q, Fletcher PT, Speidel \nS, Li S. editors. Medical Image Computing and Computer \nAssisted Intervention – MICCAI 2022. Lecture Notes in \nComputer Science, Springer, 2022:162-72.\n27. Xing Z, Yu L, Wan L, Han T, Zhu L. NestedFormer: \nNested modality-aware transformer for brain tumor \nsegmentation. In: Wang L, Dou Q, Fletcher PT, \nSpeidel S, Li S. editors. Medical Image Computing \nand Computer Assisted Intervention – MICCAI 2022. \nLecture Notes in Computer Science, vol 13435. \nSpringer, 2022:140-50.28. Zhang Y, Yang J, Tian J, Shi Z, Zhong C, Zhang Y, He Z. \nModality-aware mutual learning for multi-modal medical \nimage segmentation. In: de Bruijne M, Cattin PC, Cotin S, \nPadoy N, Speidel S, Zheng Y, Essert C. editors. Medical \nImage Computing and Computer Assisted Intervention – \nMICCAI 2021: 24th International Conference, Strasbourg, \nFrance, September 27–October 1, 2021, Proceedings, Part \nI 24. Springer, 2021:589-99.\n29. Li, X, Ma, S, T ang, J, Guo, F . T ranSiam: Aggregating \nmulti-modal visual features with locality for medical image \nsegmentation. Expert Syst Appl 2024;237:121574.\n30. Yang H, Zhou T, Zhou Y, Zhang Y, Fu H. Flexible \nFusion Network for Multi-Modal Brain T umor \nSegmentation. IEEE J Biomed Health Inform \n2023;27:3349-59.\n31. Zhou C, Ding C, Wang X, Lu Z, T ao D. One-pass Multi-\ntask Networks with Cross-task Guided Attention for Brain \nT umor Segmentation. IEEE T rans Image Process 2020. \n[Epub ahead of print]. doi: 10.1109/TIP .2020.2973510.\n32. Myronenko A. 3D MRI brain tumor segmentation using \nautoencoder regularization. In: Crimi A, Bakas S, Kuijf H, \nKeyvan F , Reyes M, van Walsum T. editors. Brainlesion: \nGlioma, Multiple Sclerosis, Stroke and T raumatic Brain \nInjuries. BrainLes 2018. Lecture Notes in Computer \nScience, Springer, 2019:311-20.\n33. Zhan B, Li D, Wu X, Zhou J, Wang Y. Multi-Modal \nMRI Image Synthesis via GAN With Multi-Scale Gate \nMergence. IEEE J Biomed Health Inform 2022;26:17-26.\n34. Lee D, Moon WJ, Ye JC. Assessing the importance of \nmagnetic resonance contrasts using collaborative generative \nadversarial networks. Nat Mach Intell 2020;2:34-42.\n35. Hu M, Maillard M, Zhang Y, Ciceri T, Barbera GL, \nBloch I, Gori P . Knowledge distillation from multi-\nmodal to mono-modal segmentation networks. In: Martel \nAL, Abolmaesumi P , Stoyanov D, Mateus D, Maria \nA. Zuluaga MA, Zhou SK, Racoceanu D, Joskowicz \nL. editors. Medical Image Computing and Computer \nAssisted Intervention – MICCAI 2020: 23rd International \nConference, Lima, Peru, October 4–8, 2020, Proceedings, \nPart I 23. Springer, 2020:772-81. \n36. Li H, Li C, Huang W, Zheng X, Xi Y, Wang S. DIGEST: \nDeeply supervIsed knowledGE tranSfer neT work \nlearning for brain tumor segmentation with incomplete \nmulti-modal MRI scans. 2023 IEEE 20th International \nSymposium on Biomedical Imaging (ISBI), Cartagena, \nColombia, 2023:1-4.\n37. Yang Q, Guo X, Chen Z, Woo PYM, Yuan Y. D(2)-\nNet: Dual Disentanglement Network for Brain T umor \n\nQuantitative Imaging in Medicine and Surgery, Vol 14, No 7 July 2024 4603\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9Segmentation With Missing Modalities. IEEE T rans Med \nImaging 2022;41:2953-64.\n38. Qiu Y, Chen D, Yao H, Xu Y, Wang Z. Scratch Each \nOther’s Back: Incomplete Multi-Modal Brain T umor \nSegmentation via Category Aware Group Self-Support \nLearning. Proceedings of the IEEE/CVF International \nConference on Computer Vision, 2023:21317-26.\n39. Zhou T, Canu S, Vera P , Ruan S. Latent Correlation \nRepresentation Learning for Brain T umor Segmentation \nWith Missing MRI Modalities. IEEE T rans Image Process \n2021;30:4263-74.\n40. Havaei M, Guizard N, Chapados N, Bengio Y. Hemis: \nHetero-modal image segmentation. In: Ourselin S, \nJoskowicz L, Sabuncu M, Unal G, Wells W. editors. \nMedical Image Computing and Computer-Assisted \nIntervention – MICCAI 2016. Lecture Notes in Computer \nScience, Springer, 2016:469-77.\n41. Dorent R, Joutard S, Modat M, Ourselin S, Vercauteren \nT. Hetero-modal variational encoder-decoder for joint \nmodality completion and segmentation. In: Shen D, Liu \nT, Peters TM, Staib LH, Essert C, Zhou S, T ap PT, Khan \nA. Medical Image Computing and Computer Assisted \nIntervention – MICCAI 2019. Lecture Notes in Computer \nScience, Springer, 2019:74-82.\n42. Zhang Y, He N, Yang J, Li Y, Wei D, Huang Y, Zhang \nY, He Z, Zheng Y. mmformer: Multimodal medical \ntransformer for incomplete multimodal learning \nof brain tumor segmentation. In: Wang L, Dou Q, \nFletcher PT, Speidel S, Li S. editors. Medical Image \nComputing and Computer Assisted Intervention – \nMICCAI 2022. Lecture Notes in Computer Science, \nSpringer, 2022:107-17.\n43. Ding Y, Yu X, Yang Y. RFNet: Region-aware fusion \nnetwork for incomplete multi-modal brain tumor \nsegmentation. 2021 IEEE/CVF International Conference \non Computer Vision (ICCV), Montreal, QC, Canada, \n2021:3975-84.\n44. Konwer A, Hu X, Bae J, Xu X, Chen C, Prasanna P . \nEnhancing modality-agnostic representations via meta-\nlearning for brain tumor segmentation. Proceedings of \nthe IEEE/CVF International Conference on Computer \nVision; 2023:21415-25.\n45. Jiang Y, Zhang Y, Lin X, Dong J, Cheng T, Liang J. \nSwinBTS: A Method for 3D Multimodal Brain T umor \nSegmentation Using Swin T ransformer. Brain Sci \n2022;12:797.  \n46. Ma J, He J, Yang X. Learning Geodesic Active \nContours for Embedding Object Global Information in Segmentation CNNs. IEEE T rans Med Imaging \n2021;40:93-104.\n47. Lin J, Lin J, Lu C, Chen H, Lin H, Zhao B, Shi Z, Qiu \nB, Pan X, Xu Z, Huang B, Liang C, Han G, Liu Z, Han \nC. CKD-T ransBTS: Clinical Knowledge-Driven Hybrid \nT ransformer With Modality-Correlated Cross-Attention \nfor Brain T umor Segmentation. IEEE T rans Med Imaging \n2023;42:2451-61.\n48. Zhou T. Modality-level cross-connection and attentional \nfeature fusion based deep neural network for multi-modal \nbrain tumor segmentation. Biomed Signal Process Control \n2023;81:104524.\n49. Lin H, Cheng X, Wu X, Yang F , Shen D, Wang Z, Song Q, \nYuan W. Cat: Cross attention in vision transformer. 2022 \nIEEE International Conference on Multimedia and Expo \n(ICME), 2022:1-6.\n50. Rahaman MA, Garg Y, Iraji A, Fu Z, Chen J, Calhoun \nV . T wo-dimensional attentive fusion for multi-modal \nlearning of neuroimaging and genomics data. 2022 \nIEEE 32nd International Workshop on Machine \nLearning for Signal Processing (MLSP), Xi'an, China, \n2022:1-6.\n51. T ang Y, Yang D, Li W, Roth HR, Landman B, Xu D, Nath \nV , Hatamizadeh A. Self-supervised pre-training of swin \ntransformers for 3d medical image analysis. Proceedings \nof the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, 2022:20730-40.\n52. Xu Y, Yang Y, Zhang L. DeMT: Deformable mixer \ntransformer for multi-task learning of dense prediction. \nProceedings of the AAAI Conference on Artificial \nIntelligence, 2023:3072-80.\n53. Liang J, Yang C, Zeng L. 3D PSwinBTS: An efficient \ntransformer-based Unet using 3D parallel shifted windows \nfor brain tumor segmentation. Digit Signal Process \n2022;131:103784.\n54. Tian W, Li D, Lv M, Huang P . Axial Attention \nConvolutional Neural Network for Brain T umor \nSegmentation with Multi-Modality MRI Scans. Brain Sci \n2022;13:12.\n55. T an Y, Li C, Qin J, Xue Y, Xiang X. Medical \nImage Description Based on Multimodal Auxiliary \nSignals and T ransformer. Int J Intell Syst 2024. doi: \n10.1155/2024/6680546.\n56. Ottom MA, Rahman HA, Dinov ID. Znet: Deep Learning \nApproach for 2D MRI Brain T umor Segmentation. IEEE \nJ T ransl Eng Health Med 2022;10:1800508.\n57. Alexander A, Jiang A, Ferreira C, Zurkiya D. An \nIntelligent Future for Medical Imaging: A Market Outlook \n\nSun et al. CMAF-Net 4604\n© Quantitative Imaging in Medicine and Surgery. All rights reserved.   Quant Imaging Med Surg 2024;14(7):4579-4604 | https://dx.doi.org/10.21037/qims-24-9on Artificial Intelligence for Medical Imaging. J Am Coll \nRadiol 2020;17:165-70.\n58. Shi F , Wang J, Shi J, Wu Z, Wang Q, T ang Z, He \nK, Shi Y, Shen D. Review of Artificial Intelligence T echniques in Imaging Data Acquisition, Segmentation, \nand Diagnosis for COVID-19. IEEE Rev Biomed Eng \n2021;14:4-15.\nCite this article as: Sun K, Ding J, Li Q, Chen W, Zhang H,  \nSun J, Jiao Z, Ni X. CMAF-Net: a cross-modal attention \nfusion-based deep neural network for incomplete multi-\nmodal brain tumor segmentation. Quant Imaging Med Surg \n2024;14(7):4579-4604. doi: 10.21037/qims-24-9\n\n\n</文献>\n\n<我的研究方法>\n在本硕士研究中，我围绕血液试管的自动化识别与分类展开，设计并实现了一个基于计算机视觉的多模态检测系统 —— BloodScan。该系统以深度学习为核心，结合机械臂控制与高分辨率图像采集，实现了对血液样本中不同分层（血清层、白膜层、血浆层）的高精度识别与评估。\n\n研究的核心创新在于提出了跨模态注意力机制（CrossModalAttention），通过蓝光与白光图像的融合，提升了血液分层边界的识别精度。在模型设计上，采用双Backbone结构分别提取不同光源下的特征，并在多个尺度上实现跨模态特征融合，兼顾检测精度与计算效率。相比传统YOLO架构，本研究提出的跨模态注意力策略在准确率、召回率上取得显著提升。\n\n系统模块涵盖了数据采集与增强、YOLO模型训练与评估、注意力可视化、以及机械臂和摄像头的软硬件联动控制。为提升模型训练效率，还构建了6通道融合数据集，并制定了严格的“精确一次检测”医学评估标准。\n\n本项目计划不仅在方法上具有创新性，也希望在工程实现层面达到高完整度，具备良好的可复现性和扩展性，为临床血液分析的自动化提供了有力支撑。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在血液分析检测、图像识别、深度神经网络、图像处理、计算机视觉、多模态融合或医学图像分析等方面的内容\n2. 总结该文献的提纲和关键信息\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息和文章结构提纲。你的分析将会帮助用户了解文献的具体的工作，并用于用户的论文撰写灵感、论文提纲设计和论文投稿，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nRCS-YOLO: A Fast and High-Accuracy\nObject Detector for Brain Tumor\nDetection\nMing Kang , Chee-Ming Ting(B), Fung Fung Ting ,\nand Raphaël C.-W. Phan\nSchool of Information Technology, Monash University, Malaysia Campus,\nSubang Jaya, Malaysia\nting.cheeming@monash.edu\nAbstract. With an excellent balance between speed and accuracy,\ncutting-edge YOLO frameworks have become one of the most eﬃcientalgorithms for object detection. However, the performance of using\nYOLO networks is scarcely investigated in brain tumor detection. We\npropose a novel YOLO architecture with Reparameterized Convolutionbased on channel Shuﬄe (RCS-YOLO). We present RCS and a One-\nShot Aggregation of RCS (RCS-OSA), which link feature cascade and\ncomputation eﬃciency to extract richer information and reduce time con-sumption. Experimental results on the brain tumor dataset Br35H show\nthat the proposed model surpasses YOLOv6, YOLOv7, and YOLOv8 in\nspeed and accuracy. Notably, compared with YOLOv7, the precision ofRCS-YOLO improves by 1%, and the inference speed by 60% at 114.8\nimages detected per second (FPS). Our proposed RCS-YOLO achieves\nstate-of-the-art performance on the brain tumor detection task. The codeis available at https://github.com/mkang315/RCS-YOLO .\nKeywords: Medical image detection\n·YOLO ·Reparameterized\nconvolution ·Channel shuﬄe ·Computation eﬃciency\n1 Introduction\nAutomatic detection of brain tumors from Magnetic Resonance Imaging (MRI)\nis complex, tedious, and time-consuming because there are a lot of missed, misin-\nterpreted, and misleading tumor-like lesions in the images of the brain tumors [ 8].\nMost of the current work focuses on brain tumor classiﬁcation and segmentation\nfrom MRI and detection tasks are less explored [ 1,13,22]. While existing stud-\nies showed that various Convolutional Neural Networks (CNNs) are eﬃcient forbrain tumor detection, the performance of using You Only Look Once (YOLO)\nnetworks is scarcely investigated [ 12,20,23–25,27].\nWith the rapid development of CNNs, the accuracies of diﬀerent visual tasks\nare constantly improved. However, the increasingly complex network architecture\nSupplementary Information The online version contains supplementary material\navailable at https://doi.org/10.1007/978-3-031-43901-8_57 .\nc/circlecopyrtThe Author(s), under exclusive license to Springer Nature Switzerland AG 2023\nH. Greenspan et al. (Eds.): MICCAI 2023, LNCS 14223, pp. 600–610, 2023.https://doi.org/10.1007/978-3-031-43901-8\n_57\n\nRCS-YOLO: A Fast and High-Accuracy Object Detector 601\nin CNN-based models, such as ResNet [ 6], DenseNet [ 9], Inception [ 28], etc.\nrenders the inference speed slower. Though many advanced CNNs deliver higheraccuracy, the complicated multi-branch designs (e.g., residual-addition in ResNet\nand branch-concatenation in Inception) make the models diﬃcult to implement\nand customize, slowing down the inference and reducing memory utilization. Thedepth-wise separable convolutions used in MobileNets [ 7] also reduce the upper\nlimit of the GPU inference speed. In addition, 3×3regular convolution is highly\noptimized by some modern computing libraries. Consequently, VGG [ 26] is still\nheavily used for real-world applications in both research and industries.\nRepVGG [ 2] is an extension of VGG via reparametrization to accelerate\ninference time. RepVGG uses a multi-branch topological architecture duringthe training phase, which is then reparameterized to a simpliﬁed single-branch\narchitecture during the inference phase. In terms of the optimization strat-\negy of network training, reparameterization was introduced in YOLOv6 [ 16],\nYOLOv7 [ 31], and YOLOv6 v3.0 [ 17]. YOLOv6 and YOLOv6 v3.0 employ repa-\nrameterization from RepVGG. RepConv, a RepVGG without an identity connec-tion, is converted from RepVGG during inference time in YOLOv6, YOLOv6\nv3.0, and YOLOv7 (named RepConvN in YOLOv7). Due to the removal of\nidentity connections in RepConv, direct access to ResNet or the concatenationin DenseNet can provide more diversity of gradients for diﬀerent feature maps.\nGrouped convolutions, which use a group of convolutions with multiple kernels\nper layer, like RepVGG, can also signiﬁcantly reduce the computational com-plexity of the model, but there is no information communication between groups,\nwhich limits the ability of feature extraction of the convolution operator. In order\nto overcome the disadvantage of grouped convolutions, ShuﬄeNet V1 [ 34]a n d\nV2 [21] introduced the channel shuﬄe operation to facilitate information ﬂows\nacross diﬀerent feature channels. In addition, when comparing Spatial Pyra-\nmid Pooling & Cross Stage Partial Network plus ConvBNSiLU (SPPCSPC)in YOLOv7 with Spatial Pyramid Pooling Fast (SPPF) in YOLOv5 [ 10]a n d\nYOLOv8 [ 11], it is found that more convolution layers in SPPCSPC architec-\nture slow down the computation of the network. Nevertheless, SPP [ 4,5] module\nachieves the fusion of local features and global features by max-pooling diﬀerent\nconvolution kernels’ sizes in the neck networks.\nAiming at a faster and high accuracy object detector for medical images,\nwe propose a new YOLO architecture called RCS-YOLO by leveraging on the\nRepVGG/RepConv. The contributions of this work are summarized as follows:\n1) We ﬁrst develop a RepVGG/RepConv ShuﬄeNet (RCS) by combining the\nRepVGG/RepConv with a ShuﬄeNet which beneﬁts from reparameteriza-\ntion to provide more feature information in the training stage and reduce\ninference time. Then, we build an RCS-based One-Shot Aggregation (RCS-OSA) module which allows not only low-cost memory consumption but also\nsemantic information extraction.\n2) We design new backbone and neck networks of YOLO architecture by incor-\nporating the developed RCS-OSA and RepVGG/RepConv with path aggre-\ngation to shorten the information path between feature prediction layers.\n\n602 M. Kang et al.\nThis leads to fast propagation of accurate localization information to feature\nhierarchy in both the backbone and neck networks.\n3) We apply the proposed RCS-YOLO model for a challenging task of brain\ntumor detection. To our best knowledge, this is the ﬁrst work to leverage onYOLO-based model for fast brain tumor detection. Evaluation on a publicly\navailable brain tumor detection annotated dataset shows superior detection\naccuracy and speed compared to other state-of-the-art YOLO architectures.\nFig. 1. Overview of RCS-YOLO. The architecture of RCS-YOLO is mainly comprised\nof the RCS-OSA (blue) and RepVGG (orange) modules. nrepresents the number\nof stacked RCS modules. nclsrepresents the number of classes in detected objects.\nIDetect [ 29] from YOLOv7 denotes detection layers using 2D convolutional neural\nnetworks. (Color ﬁgure online)\n2 Methods\nThe architecture of the proposed RCS-YOLO network is shown in Fig. 1.I t\nincorporates a new module-RCS-OSA in the backbone and neck of the YOLO-based object detector.\n2.1 RepVGG/RepConv ShuﬄeNet\nInspired by ShuﬄeNet, we design a structural reparameterized convolution based\non channel shuﬄe. Figure 2shows the structural schematic diagram of RCS.\n\nRCS-YOLO: A Fast and High-Accuracy Object Detector 603\nGiven that the feature dimensions of an input tensor are C×H×W,a f t e rt h e\nchannel split operator, it is divided into two diﬀerent channel-wise tensors withequal dimensions of C×H×W. For one of the tensors, we use the identity branch,\n1×1convolution, and 3×3convolution to construct the training-time RCS. At\nthe inference stage, the identity branch, 1×1convolution, and 3×3convolu-\ntion are transformed to 3×3RepConv by using structural reparameterization.\nThe multi-branch topology architecture can learn abundant information about\nfeatures during the training time, simpliﬁed single-branch architecture can savememory consumption during the inference time to achieve fast inference. After\nthe multi-branch training of one of the tensors, it is concatenated to the other\ntensor in a channel-wise manner. The channel shuﬄe operator is also appliedto enhance information fusion between two tensors so that the depth measure-\nment between diﬀerent channel features of the input can be realized with low\ncomputational complexity.\nFig. 2. The structure of RCS. (a) RepVGG at the training stage. (b) RepConv during\nmodel inference (or deployment). A rectangle with a black outer border represents the\nspeciﬁc modular operation of the tensor; a rectangle with a gradient color represents\nthe speciﬁc feature of the tensor, and the width of the rectangle represents the channelof the tensor.\nWhen there is no channel shuﬄe, the output feature of each group only relates\nto the input feature within a group of grouped convolutions, and outputs from\n\n604 M. Kang et al.\na certain group only relate to the input within the group. This blocks informa-\ntion ﬂow between channel groups and weakens the ability of feature extraction.When channel shuﬄe is used, input and output features are fully related where\none convolution group takes data from other groups, enabling more eﬃcient fea-\nture information communication between diﬀerent groups. The channel shuﬄeoperates on stacked grouped convolutions and allows more informative feature\nrepresentation. Moreover, assuming that the number of groups is g, for the same\ninput feature, the computational complexity of channel shuﬄe is\n1\ngtimes that\nof a generic convolution.\nCompared with the popular 3×3convolution, during the inference stage,\nRCS uses the operators including channel split and channel shuﬄe to reduce thecomputational complexity by a factor of 2, while keeping the inter-channel infor-\nmation exchange. Moreover, using structural reparameterization enables deep\nrepresentation learning from input features during the training stage, and reduc-\ntion of inference-time memory consumption to achieve fast inference.\n2.2 RCS-Based One-Shot Aggregation\nThe One-Shot Aggregation (OSA) module has been proposed to overcome the\nineﬃciency of dense connections in DenseNet, by representing diversiﬁed fea-\ntures with multi-receptive ﬁelds and aggregating all features only once in thelast feature maps. VoVNet V1 [ 14] and V2 [ 15] used the OSA module within\nits architecture to construct both lightweight and large-scale object detectors,\nwhich outperform the widely-used ResNet backbone with faster speed and betterenergy eﬃciency.\nWe develop an RCS-OSA module by incorporating RCS developed in Sect. 2.1\nfor OSA, as shown in Fig. 3. The RCS modules are stacked repeatedly to ensure\nthe reuse of features and to enhance the information ﬂow among diﬀerent chan-\nnels between features of adjacent layers. At diﬀerent locations of the network, we\nset a diﬀerent number of stacked modules. To reduce the level of network frag-\nmentation, only three feature cascades are maintained on the one-shot aggregate\npath, which can mitigate the amount of network calculation burden and reducethe memory footprint. In terms of multi-scale feature fusion, inspired by the idea\nof Path Aggregation Network (PANet) [ 19], RCS-OSA + Upsampling and RCS-\nOSA + RepVGG/RepConv undersampling carry out the alignment of featuremaps of diﬀerent sizes to allow information exchange between the two predic-\ntion feature layers. This enables high-accuracy fast inference in object detection.\nMoreover, RCS-OSA maintains the same number of input channels and mini-mum output channels, thus reducing the memory access cost (MAC). For net-\nwork building, we perpetuate max-pooling undersampling 32 times of YOLOv7\nto construct a backbone network and adopt RepVGG/RepConv with a step of2 to achieve undersampling. Due to the diversiﬁed feature representation of the\nRCS-OSA module and low-cost memory consumption, we use a diﬀerent number\nof stacked RCS in RCS-OSA modules to achieve semantic information extractionduring diﬀerent stages of both backbone and neck networks.\n\nRCS-YOLO: A Fast and High-Accuracy Object Detector 605\nFig. 3. The structure of RCS-OSA. nrepresents the number of stacked RCS modules.\nThe common evaluation metric of computation eﬃciency (or time complex-\nity) is ﬂoating-point operations (FLOPs). FLOPs are only the indirect indicatorto measure the speed of inference. However, the object detector with a DenseNet\nbackbone shows rather slow speed and low energy eﬃciency because the linearly\nincreasing number of channels by dense connection leads to heavy MAC, whichcauses considerable computation overhead. Given input features of dimension\nM×M, the convolution kernel of size K×K, number of input channels C\n1,a n d\nthe number of output channels C2, FLOPs and MAC can be calculated as:\nFL O Ps =M2K2C1C2 (1)\nMA C =M2(C1+C2)+ K2C1C2 (2)\nAssuming nto be 4, FLOPs of the proposed RCS-OSA and Eﬃcient Layer\nAggregation Networks (ELAN) [ 31,33]a r e 20.25C2M2and 40C2M2respec-\ntively. Compared with ELAN, FLOPs of RCS-OSA are reduced by nearly 50%.The MAC of RCS-OSA (i.e., 6CM\n2+2 0 .25C2) is also reduced compared to that\nof ELAN (i.e., 17CM2+4 0 C2).\n2.3 Detection Head\nTo further reduce inference time, we decrease the number of detection heads com-\nprised of RepVGG and IDetect from 3 to 2. The YOLOv5, YOLOv6, YOLOv7,\nand YOLOv8 have three detection heads. However, we use only two feature lay-ers for prediction, reducing the number of original nine anchors with diﬀerent\nscales to four and using the K-means unsupervised clustering method to regen-\nerate anchors with diﬀerent scales. The corresponding scales are (87, 90), (127,\n\n606 M. Kang et al.\n139), (154, 171), (191,240). This not only reduces the number of convolution\nlayers and computational complexity of RCS-YOLO but also reduces the overallcomputational requirements of the network during the inference stage and the\ncomputational time of postprocessing non-maximum suppression.\n3 Experiments and Results\n3.1 Dataset Details\nTo evaluate the proposed RCS-YOLO model, we used the brain tumor detection\n2020 dataset (Br35H) [ 3], with a total of 701 images in the ‘train’ and ‘val’ two\nfolders, 500 images of which are the ‘train’ folder were selected as the trainingset, while the other 201 images in the ‘val’ folder as the testing set. For the input\nsize of 640×640image, the actual corresponding size is 44×32. The small object\nis deﬁned as the object whose pixel size is less than 32×32deﬁned by the MS\nCOCO dataset [ 18], so there are no small objects in the brain tumor medical\nimage data sets, and the scale change of the target boxes is smooth, almostsquare. The label boxes of the brain images were normalized (See supplementary\nmaterial Sect.1).\n3.2 Implementation Details\nFor model training and inference, we used Ubuntu 18.04 LTS, Intel\nR/circlecopyrtXeon R/circlecopyrt\nGold 5218 CPU processor, CUDA 12.0, and cuDNN 8.2. GPU is GeForce RTX\n3090 with 24G memory size. The networking development framework is Pytorch1.9.1. The Integrated Development Environment (IDE) is PyCharm. We uni-\nformly set epoch 150, the batch size as 8, image size as 640×640. Stochastic\nGradient Descent (SGD) optimizer was used with an initial learning rate of 0.01and weight decay of 0.0005.\n3.3 Evaluation Metrics\nIn this paper, we choose precision, recall, AP\n50,AP 50:95,F L O P s ,a n dF r a m e s\nPer Second (FPS) as comparative metrics of detection eﬀect to determine the\nadvantages and disadvantages of the model. Taking IoU = 0.5 as the standard,\nprecision, and recall can be calculated by the following equations:\nP recision =TP\nTP +FP(3)\nRecall =TP\n(TP +PN )(4)\nwhere TPrepresents the number of positive samples correctly identiﬁed as posi-\ntive samples, FPrepresents the number of negative samples incorrectly identiﬁed\nas positive samples and FNrepresents the number of positive samples incorrectly\nidentiﬁed as negative samples. AP 50is the area under the precision-recall (PR)\ncurve formed by precision and recall. For AP 50:95, divide 10 IoU threshold of\n0.5:0.05:0.95 to acquire the area under the PR curve, then average the results.\nFPS represents the number of images detected by the model per second.\n\nRCS-YOLO: A Fast and High-Accuracy Object Detector 607\n3.4 Results\nTo highlight the accuracy and rapidity of the proposed model for the detection of\nbrain tumor medical image data set, Table 1shows the performance comparison\nbetween our proposed detector and other state-of-the-art object detectors. The\ntime duration of FPS includes data preprocessing, forward model inference, and\npost-processing. The long border of the input images is set as 640 pixels. The\nshort border adaptively scales without distortion, whilst keeping the grey ﬁlling\nwith 32 times the pixels of the short border.\nIt can be seen that RCS-YOLO with the advantages of incorporating the\nRCS-OSA module performs well. Compared with YOLOv7, the FLOPs of the\nobject detectors of this paper decrease by 8.8G, and the inference speed improvesby 43.4 FPS. In terms of detection rate, precision improves by 0.024; AP\n50\nincreases by 0.01; AP 50:95by 0.006. Also, RCS-YOLO is faster and more accu-\nrate than YOLOv6-L v3.0 and YOLOv8l. Although the AP 50:95of RCS-YOLO\nequals that of YOLOv8l, it doesn’t obscure the essential advantage of RCS-\nYOLO. The results clearly show the superior performance and eﬃciency of our\nmethod, compared to the state-of-the-art for brain tumor detection. As shownin supplementary material Fig.2, brain tumor regions are accurately detected\nfrom MRI by using the proposed method.\nTable 1. Quantitative results of diﬀerent methods. The best results are shown in bold.\nModel Params Precision Recall AP 50 AP 50:95 GFLOPs FPS\nYOLOv6-L [ 17] 59.6M 0.907 0.920 0.929 0.709 150.5 64.0\nYOLOv7 [ 31] 36.9M 0.912 0.925 0.936 0.723 103.3 71.4\nYOLOv8l [ 11] 43.9M 0.934 0.920 0.944 0.729 164.8 76.2\nRCS-YOLO (Ours) 45.7M 0.936 0.945 0.946 0.729 94.5 114.8\nTable 2. Ablation study on proposed RCS-OSA module. The best results are shown\nin bold.\nModel Params Precision Recall AP 50 AP 50:95 GFLOPs FPS\nRepVGG-CSP (w/o RCS-OSA) 22.6M 0.926 0.930 0.933 0.689 43.3 6.1\nRCS-YOLO (w/ RCS-OSA) 45.7M 0.936 0.945 0.946 0.729 94.5 114.8\n3.5 Ablation Study\nWe demonstrate the eﬀectiveness of the proposed RCS-OSA module in YOLO-\nbased object detectors. The results of RepVGG-CSP in Table 2, where RCS-OSA\n\n608 M. Kang et al.\nin the RCS-YOLO is replaced with the Cross Stage Partial Network (CSP-\nNet) [ 32] used in existing YOLOv4-CSP [ 30] architecture, are decreased than\nRCS-YOLO except GFLOPs. Because the parameters of RepVGG-CSP (22.2M)\nare less than half those of RCS-YOLO (45.7M), the computation amount (i.e.,\nGFLOPs) of RepVGG-CSP is accordingly smaller than RCS-YOLO. Neverthe-less, RCS-YOLO still performs better in actual inference speed measured by\nFPS.\n4 Conclusion\nWe developed an RCS-YOLO network for fast and accurate medical object\ndetection, by leveraging the reparameterized convolution operator RCS basedon channel shuﬄe in the YOLO architecture. We designed an eﬃcient one-shot\naggregation module RCS-OSA based on RCS, which serves as a computational\nunit in the backbone and neck of a new YOLO network. Evaluation of the brainMRI dataset shows superior performance for brain tumor detection in terms of\nboth speed and precision, as compared to YOLOv6, YOLOv7, and YOLOv8\nmodels.\nReferences\n1. Amin, J., Muhammad, S., Haldorai, A., Yasmin, M., Nayak, R.S.: Brain tumor\ndetection and classiﬁcation using machine learning: a comprehensive survey. Com-plex Intell. Syst. 8, 3161–3183 (2022)\n2. Ding, X., Zhang, X., Ma, N., Han, J., Ding, G., Sun, J.: RepVGG: making VGG-\nstyle ConvNets great again. In: 2021 IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pp. 13733–13742. IEEE, Piscataway (2021)\n3. Hamada, A.: Br35H : : brain tumor detection 2020. Kaggle (2020). https://www.\nkaggle.com/datasets/ahmedhamada0/brain-tumor-detection\n4. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional\nnetworks for visual recognition. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.\n(eds.) ECCV 2014. LNCS, vol. 8691, pp. 346–361. Springer, Cham (2014). https://\ndoi.org/10.1007/978-3-319-10578-9_23\n5. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional\nnetworks for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell. 37(9),\n1904–1916 (2015)\n6. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),pp. 770–778. IEEE, Piscataway (2016)\n7. Howard, A.G., et al.: MobileNets: eﬃcient convolutional neural networks for mobile\nvision applications. arXiv preprint arXiv:1704.04861 (2017)\n8. Huisman, T.A.: Tumor-like lesions of the brain. Cancer Imaging 9(Special issue\nA), S10–S13 (2009)\n9. Iandola, F., Moskewicz, M., Karayev, S., Girshick, R., Darrell, T., Keutzer, K.:\nDenseNet: implementing eﬃcient ConvNet descriptor pyramids. arXiv preprint\narXiv:1404.1869 (2014)\n\nRCS-YOLO: A Fast and High-Accuracy Object Detector 609\n10. Jocher, G.: YOLOv5 (6.0/6.1) brief summary. GitHub (2022). https://github.com/\nultralytics/yolov5/issues/6998\n11. Jocher, G., Chaurasia, A., Qiu, J.: YOLO by Ultralytics (version 8.0.0). GitHub\n(2023). https://github.com/ultralytics/ultralytics\n12. Kumar, V.V., Prince, P.G.K.: Brain lesion detection and analysis - a review. In:\n2021 Fifth International Conference on I-SMAC (IoT in Social. Mobile, Analytics\nand Cloud) (I-SMAC), pp. 993–1001. Piscataway, IEEE (2021)\n13. Lather, M., Singh, P.: Investigating brain tumor segmentation and detection tech-\nniques. Procedia Comput. Sci. 167, 121–130 (2020)\n14. Lee, Y., Hwang, J.-W., Lee, S., Bae, Y., Park, J.: An energy and GPU-computation\neﬃcient backbone network for real-time object detection. In: 2019 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp.752–760. IEEE, Piscataway (2019)\n15. Lee, Y., Park, J.: CenterMask: real-time anchor-free instance segmentation.\nIn: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pp. 13903–13912. IEEE, Piscataway (2020)\n16. Li, C., et al.: YOLOv6: a single-stage object detection framework for industrial\napplications. arXiv preprint arXiv:2209.02976 (2022)\n17. Li, C., et al.: YOLOv6 v3.0: a full-scale reloading. arXiv preprint arXiv:2301.05586\n(2023)\n18. Lin, T.-Y., et al.: Microsoft COCO: common objects in context. In: Fleet, D.,\nPajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp.\n740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1_48\n19. Liu, S., Qi, L., Qin, H., Shi, J., Jia, J.: Path aggregation network for instance\nsegmentation. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 8759–8768. IEEE, Piscataway (2018)\n20. Lotlikar, V.S., Satpute, N., Gupta, A.: Brain tumor detection using machine learn-\ning and deep learning: a review. Curr. Med. Imaging 18(6), 604–622 (2022)\n21. Ma, N., Zhang, X., Zheng, H.-T., Sun, J.: ShuﬄeNet V2: practical guidelines for\neﬃcient CNN architecture design. In: Ferrari, V., Hebert, M., Sminchisescu, C.,\nWeiss, Y. (eds.) ECCV 2018. LNCS, vol. 11218, pp. 122–138. Springer, Cham\n(2018). https://doi.org/10.1007/978-3-030-01264-9_8\n22. Nazir, M., Shakil, S., Khurshid, K.: Role of deep learning in brain tumor detection\nand classiﬁcation (2015 to 2020): a review. Comput. Med. Imaging Graph. 91,\n101940 (2021)\n23. Rehman, A., Butt, M.A., Zaman, M.: A survey of medical image analysis using deep\nlearning approaches. In: 2021 2nd International Conference on Smart Electronics\nand Communication (ICOSEC), pp. 1115–1120. IEEE, Piscataway (2021)\n24. Shanishchara, P., Patel, V.D.: Brain tumor detection using supervised learning: a\nsurvey. In: 2022 Third International Conference on Intelligent Computing Instru-mentation and Control Technologies (ICICICT), pp. 1159–1165. IEEE, Piscataway\n(2022)\n25. Shirwaikar, R.D., Ramesh, K., Hiremath, A.: A survey on brain tumor detection\nusing machine learning. In: 2021 International Conference on Forensics. Analytics,Big Data, Security (FABS), pp. 1–6. IEEE, Piscataway (2021)\n26. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Conferenceon Learning Representations (ICLR) (2015)\n27. Sravya, V., Malathi, S.: Survey on brain tumor detection using machine learning\nand deep learning. In: 2021 International Conference on Computer Communication\nand Informatics (ICCCI), pp. 1–3. IEEE, Piscataway (2021)\n\n610 M. Kang et al.\n28. Szegedy, C., et al.: Going deeper with convolutions. In: 2015 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 1–9. IEEE, Piscataway\n(2015)\n29. Wang, C.-Y.: Yolov7.yaml. GitHub (2022). https://github.com/WongKinYiu/\nyolov7/blob/main/cfg/training/yolov7.yaml\n30. Wang, C.-Y.: Yolov4-csp.yaml. GitHub (2022). https://github.com/WongKinYiu/\nyolov7/blob/main/cfg/baseline/yolov4-csp.yaml\n31. Wang, C.-Y., Bochkovskiy, A., Liao, H.-Y.M.: YOLOv7: trainable bag-of-\nfreebies sets new state-of-the-art for real-time object detectors. arXiv preprint\narXiv:2207.02696 (2022)\n32. Wang, C.-Y., Liao, H.-Y.M., Wu, Y.-H., Chen, P.-Y., Hsieh, J.-W., Yeh, I.-H.:\nCSPNet: a new backbone that can enhance learning capability of CNN. In: 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n(CVPRW), pp. 1571–1580. IEEE, Piscataway (2020)\n33. Wang, C.-Y., Liao, H.-Y.M., Yeh, I.-H.: Designing network design strategies\nthrough gradient path analysis. arXiv preprint arXiv:2211.04800 (2022)\n34. Zhang, X., Zhou, X., Lin, M., Sun, J.: ShuﬄeNet: an extremely eﬃcient convo-\nlutional neural network for mobile devices. In: 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 6848–6856. IEEE, Piscat-\naway (2018)\n\n\n</文献>\n\n<我的研究方法>\n在本硕士研究中，我围绕血液试管的自动化识别与分类展开，设计并实现了一个基于计算机视觉的多模态检测系统 —— BloodScan。该系统以深度学习为核心，结合机械臂控制与高分辨率图像采集，实现了对血液样本中不同分层（血清层、白膜层、血浆层）的高精度识别与评估。\n\n研究的核心创新在于提出了跨模态注意力机制（CrossModalAttention），通过蓝光与白光图像的融合，提升了血液分层边界的识别精度。在模型设计上，采用双Backbone结构分别提取不同光源下的特征，并在多个尺度上实现跨模态特征融合，兼顾检测精度与计算效率。相比传统YOLO架构，本研究提出的跨模态注意力策略在准确率、召回率上取得显著提升。\n\n系统模块涵盖了数据采集与增强、YOLO模型训练与评估、注意力可视化、以及机械臂和摄像头的软硬件联动控制。为提升模型训练效率，还构建了6通道融合数据集，并制定了严格的“精确一次检测”医学评估标准。\n\n本项目计划不仅在方法上具有创新性，也希望在工程实现层面达到高完整度，具备良好的可复现性和扩展性，为临床血液分析的自动化提供了有力支撑。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在血液分析检测、图像识别、深度神经网络、图像处理、计算机视觉、多模态融合或医学图像分析等方面的内容\n2. 总结该文献的提纲和关键信息\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息和文章结构提纲。你的分析将会帮助用户了解文献的具体的工作，并用于用户的论文撰写灵感、论文提纲设计和论文投稿，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nIEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 29, NO. 1, JANUARY 2025 371\nCAISeg: A Clustering-Aided Interactive Network\nfor Lesion Segmentation in 3D Medical Imaging\nYukang Sun , Shujun Zhang , Jinsong Li ,Q iH a n , and Yuhua Qin\nAbstract —Accurate lesion segmentation in medical\nimaging is critical for medical diagnosis and treatment.\nLesions’ diverse and heterogeneous characteristics often\npresent a distinct long-tail distribution, posing difﬁculties\nfor automatic methods. Currently, interactive segmentation\napproaches have shown promise in improving accuracy,\nbut still struggle to deal with tail features. This triggers\na demand of effective utilizing strategies of user interac-\ntion. To this end, we propose a novel point-based interac-\ntive segmentation model called Clustering-Aided Interac-\ntive Segmentation Network (CAISeg) in 3D medical imaging.\nA customized Interaction-Guided Module (IGM) adopts the\nconcept of clustering to capture features that are semanti-\ncally similar to interaction points. These clustered features\nare then mapped to the head regions of the prompted cat-\negory to facilitate more precise classiﬁcation. Meanwhile,\nwe put forward a Focus Guided Loss function to grant the\nnetwork an inductive bias towards user interaction through\nassigning higher weights to voxels closer to the prompted\npoints, thereby improving the responsiveness efﬁciency\nto user guidance. Evaluation across brain tumor, colon\ncancer, lung cancer, and pancreas cancer segmentation\ntasks show CAISeg’s superiority over the state-of-the-art\nmethods. It outperforms the fully automated segmentation\nmodels in accuracy, and achieves results comparable to\nor better than those of the leading point-based interactive\nmethods while requiring fewer prompt points. Furthermore,\nwe discover that CAISeg possesses good interpretability at\nvarious stages, which endows CAISeg with potential clini-\ncal application value.\nIndex Terms —Deep learning, Interactive segmentation,\nMedical image analysis.\nI. INTRODUCTION\nMEDICAL image segmentation stands as a pivotal task at\nthe intersection of computer vision and medical research,\naiming to accurately extract regions of interest from medical\nimages. This provides a foundation for physicians in their\nReceived 16 December 2023; revised 28 August 2024; accepted 21\nSeptember 2024. Date of publication 25 September 2024; date of cur-\nrent version 8 January 2025. This work was supported by the Qingdao\nMunicipal Demonstration Project of Science and Technology Beneﬁting\nthe People Number 23-2-8-smjk-20-nsh. (Corresponding author: Shujun\nZhang.)\nYukang Sun, Shujun Zhang, Jinsong Li, and Qi Han are with the\nCollege of Data Science, Qingdao University of Science and Technology,\nQingdao 266061, China (e-mail: yakecan@mails.qust.edu.cn; zhangsj\n@qust.edu.cn; plaitkol@mails.qust.edu.cn; HanQi@mails.qust.edu.cn).\nYuhua Qin is with the College of Information Science and Technology,\nQingdao University of Science and Technology, Qingdao 266061, China\n(e-mail: yqin@qust.edu.cn).\nDigital Object Identiﬁer 10.1109/JBHI.2024.3467279detailed analysis and quantitative assessment of physiological\nstructures and tissues. With advancements in deep learning and\ncomputer vision, fully automatic medical image segmentation,\nsuch as [1],[2], and [3], has achieved expert-level performance\nin many tasks. However, these high-performing models predom-\ninantly rely on fully supervised training strategies, which means\nthey are heavily dependent on extensive, high-quality annotated\ndatasets. Particularly in the task of lesion segmentation, the\nimmense variability in lesion shapes and locations intensiﬁes the\nchallenge of creating a thorough dataset. Consequently, these\nautomatic models might struggle when encountering lesion\nfeatures not well-represented within their training datasets—\nessentially the tail-end features. Interactive segmentation is a\npromising avenue for tackling this challenge as it allows the\nsegmentation network to capitalize on the doctor’s expertise to\nmore effectively identify these tail features.\nInteractive segmentation techniques enable users to convey\nprior knowledge to the network by drawing scribbles, outlin-\ning bounding boxes, or clicking on points. Considering the\nthree-dimensional structure of most medical images in clini-\ncal settings, the method of interaction through point clicking\nstands out as the most straightforward and intuitive strategy.\nIn prior research on point-based interactive segmentation of\nmedical images, most approaches drew inspiration from inter-\nactive segmentation techniques in the realm of natural images.\nThese methods initially encode user interaction points, employ-\ning methods like Euclidean distance encoding [4], disk encod-\ning[5],[6], Gaussian distance encoding [7], geodesic distance\nencoding [8],[9], etc., and utilize the resulting encoded output\nas a cue map to guide the network in segmentation. However,\nthe semantic information conveyed by these simple distance or\ngradient encodings tends to be shallow and simplistic. Therefore,\nwhen it comes to tail features in medical images, these methods\noften fail to make accurate predictions, even when precise inter-\nactive points and their associated label information are provided\nby physicians. Fortunately, recent works [10],[11],[12],[13]\nthat revisit Mask Transformers [14],[15] from the perspective of\nclustering have inspired us to develop a novel point-based inter-\nactive segmentation network, named Clustering-Aided Interac-\ntive Segmentation Network (CAISeg). This framework advances\ninteractive segmentation by introducing a two-stage process:\ncluster assignment and head feature reﬁnement. Initially, we\nperform cluster assignment to extract semantic feature vectors\nthat closely resemble user interaction points through clustering\nwithin the feature map. Following this, head feature reﬁnement\nis applied to adjust interactive feature vectors along with those\n2168-2194 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\n372 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 29, NO. 1, JANUARY 2025\nhighly similar vectors within their clusters, enabling them to be\nmapped to the head distribution of the prompted label categories.\nThis interaction-guided approach ensures that CAISeg retains its\neffectiveness when addressing tail features.\nPrevious work on point-based interactive segmentation com-\nmonly relied on loss functions tailored for general segmentation\ntasks. These include Binary Cross Entropy Loss (BCE Loss),\nDice Loss [16] designed for sparse sample scenarios, and Focal\nLoss [17] aimed at hard cases. Yet, all of these loss functions\nwere originally conceived for fully automatic segmentation,\nand they may not take into full account the context of user\ninteractions. This oversight can prevent models from effectively\nemphasizing areas pointed out by users. In the realm of inter-\nactive segmentation, it’s crucial for the model to hone in on\nuser-speciﬁed regions and achieve heightened accuracy therein.\nTo bridge this gap, we introduced Focus Guided Loss (FG Loss).\nDistinct from conventional loss functions, FG Loss is intricately\ncrafted to resonate with the speciﬁcs of user interactions. By\napplying a higher penalty to the regions where users click, it\nbolsters the model’s capacity to learn effectively from those\ninteractions, ensuring a profound comprehension and response\nto such cues. Leveraging this approach, our network not only\ndelineate user-designated structures with enhanced precision but\nalso to swiftly recalibrate in light of user feedback.\nWe summarize our main contributions as follows:rTo the best of our knowledge, we are the ﬁrst to apply\nthe concept of clustering to deep learning-based interac-\ntive segmentation networks, and the clustering method is\neffective in aiding CAISeg to seize the regions of interest\nbased on the interaction points. Moreover, the approach\nof aligning features within these regions with the head\nfeatures of the prompted categories enables CAISeg to\nmaintain a high recognition capability for tail features.rWe propose a novel loss function, Focus Guided Loss,\na tailored loss function that emphasizes the predictive\naccuracy around the proximity of user interaction points.\nBy assigning higher weights to voxels closer to these\ninteraction points, we encourage CAISeg to focus more on\nthese critical regions, which enhances its ability to deduce\nand prioritize user intent.rWe compared CAISeg with the state-of-the-art segmenta-\ntion methods on three distinct lesion segmentation tasks:\nbrain tumors, colon cancer, and lung cancer. CAISeg ex-\nhibits superior segmentation quality compared to existing\nfully automated segmentation models. Compared to cur-\nrent interactive segmentation methods, CAISeg achieves\nmore precise segmentation with fewer, unrestricted inter-\naction points.\nII. R ELATED WORK\nA. Automatic Deep Learning-Based Medical Image\nSegmentation Methods\nApplying deep learning to medical image segmentation\nhas long been challenging in computer vision. The Fully\nConvolutional Network [18] laid the foundation, but the\nreal breakthrough came with U-Net [19], whose symmetricencoder-decoder structure and skip-connections effectively cap-\nture intricate structures in medical images. Numerous U-Net\nvariants ( [1],[16],[20],[21]) followed, with nnU-Net [1]stand-\ning out for its auto-adjusting capabilities and robustness across\nvarious segmentation tasks. The advent of ViT [22] initiated the\nintegration of Transformers with CNNs, as seen in models like\nUNeTR [23], which combines CNNs for local feature extrac-\ntion with Transformers for long-range contextual understand-\ning. Further developments ( [24],[2]) replaced convolutional\nmodules with the shifted window self-attention mechanism of\nthe Swin Transformer [25], creating a pure Transformer archi-\ntecture. Liu et al. [3]introduced contrastive language-image\npre-training embeddings into segmentation models, leading to\nthe highly generalized Universal Model. Several studies in-\ntroduced effective data augmentation techniques to enhance\nmedical image segmentation models. Liang et al. [26] gen-\nerated new samples by extracting inter-patient deformations\nthrough learning-based deformable registration, and creating\nintra-patient deformations using random 3D Thin-Plate-Spline\ntransformations. Furthermore, He et al. [27] increased the diver-\nsity of generated samples through KL transform-based statistical\nanalysis. Despite these advancements, handling tail features in\nmedical images remains difﬁcult. Our method, however, lever-\nages interactive segmentation by incorporating expert guidance\nfrom physicians to identify and cluster tail features, allowing for\nmore precise delineation of regions misjudged by the network.\nB. Interactive Segmentation Methods\nTraditional interactive segmentation methods such as Level\nSet[28], Graph Cut [29], Random Walker [30],R e g i o nG r o w -\ning[31], and Grow Cut [32] are notably representative. The\nLevel Set method uses partial differential equations to capture\ntarget boundaries, excelling in managing topological changes.\nGraph Cut segments images into foreground and background\nusing the min-cut algorithm and user-annotated seed points. The\nRandom Walker method labels pixels based on the probability\nof a random walk to seed points, while Region Growing incre-\nmentally expands segmentation areas from seed points based on\nlocal similarity. Grow Cut, based on cellular automata theory,\niteratively updates pixel labels until a stable state is reached.\nHowever, despite their innovative designs, these methods face\nchallenges in 3D medical image segmentation, including high\ncomputational complexity, sensitivity to noise, difﬁculties with\ncomplex structures, and susceptibility to local optima.\nIn deep learning-based interactive segmentation, a key chal-\nlenge is enabling networks to effectively comprehend user in-\nteraction cues. Xu et al. [4], the ﬁrst model to integrate inter-\nactive concepts into deep learning for 2D image segmentation,\nusing Euclidean distance maps to encode positive and negative\nuser sample points, thereby providing spatial information to\nthe network. DEXTR [33] required user points at the extreme\nedges of the target for precise location information. Wang\net al. [8]and Luo et al. [9]used geodesic distance transforms\nto encode interaction points, aiming to enrich the network’s\nprior information. Wang’s model [8]started with a coarse seg-\nmentation, reﬁning it using geodesic distance encoding, while\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\nSUN et al.: CAISEG: A CLUSTERING-AIDED INTERACTIVE NETWORK FOR LESION SEGMENTATION IN 3D MEDICAL IMAGING 373\nLuo [9]placed interaction points near the foreground bound-\nary, utilizing an exponential geodesic distance transform for\nmore accurate edge information. Soﬁiuk et al. [34] analyzed\nand reﬁned interaction encoding methods, training strategies,\nand loss functions, leading to the development of the RITM\ninteractive segmentation network, which introduced sparse en-\ncoding and iterative training as a new standard. Jian et al. [7]\nproposed DINs, integrating sparse Gaussian distance encoding\ninto multi-scale feature maps, which enhanced the transmis-\nsion of sparse interaction information through the network. Liu\net al. [35] used cross-attention between interaction point vectors\nand feature maps to incorporate category features directly. The\nVMN developed by Zhou et al. built upon [33] by performing\nsegmentation on 2D slices and then bi-directionally propagat-\ning the segmentation mask, incorporating a quality assessment\nmodule to reﬁne the process. While these methods encoded user\ninteraction as shallow semantic information, they limited the\nprior knowledge conveyed to the network. The MedSAM [36],\nutilizing the pre-trained SAM [37] with transfer learning on\na large medical dataset, introduces effective segmentation for\nmedical images. On the other hand, MedLSAM [38] employs\nan additional network to regress six extreme points of a 3D object\nto anchor the corresponding bounding box, which is then seg-\nmented slice-by-slice using the SAM model. SAM-Med3D [39]\nfurther modiﬁed the SAM model into a 3D structure, allowing\npoint-based interaction for 3D medical images. However, these\nSAM-based methods, while effective for organ segmentation,\nstruggle with the heterogeneity of medical lesions, as their output\ntokens ﬁnd it difﬁcult to identify tail features within the target\nbased on interaction cues. Our approach focuses on capturing\nsemantic features deeply embedded in the network that closely\nresemble the vectors corresponding to user interaction points\nthrough clustering. For challenging tail features, we reposition\nthem within the head distribution space to improve segmentation\naccuracy, aligning more closely with the physician’s intent.\nC. Mask T ransformers\nInstead of directly employing Transformers as network back-\nbones for image segmentation, Mask Transformers enhance\nCNN-based architectures with stand-alone blocks that leverage\nmasked attention mechanisms. In MaX-Deeplab [40], Mask-\nFormer [15], and Mask2Former [41], mask embedding queries\nare used in the decoder to perform dot-product operations\nwith per-pixel features, generating the predicted binary masks.\nBuilding on this, CMT-Deeplab [10] and KMaX-Deeplab [11]\nintroduced the concept of treating queries as clustering cen-\nters, incorporating constraints to improve cluster representation\nlearning within the network. Yuan et al. [12] segmented unseen\nobjects in medical scans by identifying outlier features within\nthe clustering results. S2VNet [13] addresses 3D medical image\nsegmentation by processing slices individually, initializing clus-\nter centers based on the clustering results from previous slices,\nthereby leveraging prior knowledge to assist in the segmentation\nof current slices. Inspired by these advancements, we extend\nthis clustering-based approach to interactive segmentation with\nCAISeg. In CAISeg, user-provided interaction points serve asclustering centers within a designed masked attention module,\nallowing the model to identify semantically similar regions and\nmore effectively recognize tail features.\nIII. M ETHOD\nIn this section, we ﬁrst outline the problem scenario for\nour task. Then, we will sequentially introduce the overview of\nCAISeg, the Feature Clustering Module, the method for loss\ncalculation, and the interaction simulation strategy during the\ntraining phase.\nA. Problem Scenario\nThe objective of an interactive segmentation network is\nto infer the user’s area of interest and delineate it from the\nbackground based on user-provided interaction cues. Conse-\nquently, irrespective of the number of target classes present in\nthe dataset, interactive segmentation tasks consistently revolve\naround pixel-level or voxel-level binary classiﬁcation tasks. In\nour problem, users guide the network to segment all lesion\nregions speciﬁc to a particular disease from three-dimensional\nmedical images by supplying positive and negative interaction\npoints, representing the foreground region of the target lesion\nand the non-lesion region, respectively. Here, let’s denote the\nthree-dimensional medical image as I∈RD×H×W. The output\nof the model is a predicted mask of the region of lesions, denoted\nasˆY∈{0,1}D×H×W.\nB. Method Overview\nThe pipeline of CAISeg is depicted in Fig. 1. It comprises three\nmain components: a Feature Encoder-Decoder for semantic fea-\nture extraction, a multi-scale Cross-Attention Module to capture\nforeground and background head features in the image, and an\nInteraction-Guided Module (IGM). The IGM, a comprehensive\ncomponent, encompasses both the Feature Clustering Block and\nthe feature adjustment method.\nWe utilize a CNN backbone to extract semantic features. The\nfeature map undergoes a certain degree of downsampling, main-\ntaining a maximum downsampling rate of1\n2in each direction.\nAfter reshaping, this feature map is represented as F∈Rdhw×C.\nIn the multi-scale Cross-Attention Module, we initially deﬁne\ntwo learnable embedding vectors, qforeandqback, as head feature\nqueries. These queries engage in cross-attention computations\nwith the feature map F/primeat various scales within the decoder of\nthe feature extractor. They selectively integrate head features\nof foreground and background from the medical image for\nprocessing. The deﬁnition of the Cross-Attention Module is as\nfollows:\nqcls←qcls+MLP/parenleftBigg\nqcls+Softmax/parenleftBigg/parenleftBigg\nqcls·(KF/prime)T\n√dk/parenrightBigg\n·VF/prime/parenrightBigg\n,\n(1)\nwhereqclsrepresents either qforeorqback, whileKF/primeandVF/primeare\nboth linear transformations of F/prime. Additionally, if necessary, we\nperform a fully connected layer mapping on qclsto change its\ndimensions, aligning it with the feature map at following scale\nfor cross-attention computation.\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\n374 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 29, NO. 1, JANUARY 2025\nFig. 1. The pipeline of CAISeg. Here, we assume that the user has provided a total of 2 background interaction points and 3 foreground interaction\npoints. (a)A CNN backbone for feature extraction; (b)A multi-scale Cross-Attention Module; (c)The Interaction-Guided Module, which encompasses\nthe Feature Clustering Block and a feature adjustment method. In the clustered afﬁnity map, the color gray represents a ﬁxed value of 0.\nIn the IGM, we ﬁrst downsample the coordinates of the\nuser’s interaction points based on the corresponding axis’s\ndownsampling ratio in the feature map F. Subsequently, using\nthese coordinates, we index the corresponding position’s feature\nvectors from the feature map Fand concatenate them into a\nnew matrix Mc∈Rnc×C, wherencrepresents the number of\nuser interaction points. It should be noted that the two vectors\nobtained from (1)respectively represent the primary feature of\nthe lesion category and the non-lesion category in the instance\nto be segmented, which are the targets for adjusting the tail\nfeatures indicated by medical experts that the model cannot\ncorrectly identify. Therefore, for any row vector vcinMc, its\ncorresponding adjustment vector vadjis deﬁned as:\nvadj=qcls−vc. (2)\nHere, the value of qclsis contingent upon the user’s annotation\nforvc.I fvcis labeled as foreground by the user, then qclstakes\nthe value qfore; on the other hand, if vcis labeled as background,\nqclsisqback. These adjustment vectors are allocated to the feature\nvectors within their respective clusters. After being adjusted by\nthe adjustment vectors, those potential tail features that closely\nalign with user interactions are mapped to a more appropriate\nposition within the feature space. Speciﬁcally:\nFnew=F+CA×Madj, (3)\nwhereMadj∈Rnc×Cis a matrix composed of adjustment vec-\ntorsvadjfrom all prior features annotated by the user. Fnewis the\nadjusted new feature map. CA∈Rdhw×ncis a clustered afﬁnity\nmap output by the Feature Clustering Block, exclusively captur-\ning the afﬁnity of each feature vector to its closest foreground\nand background clustering centers marked by the user. Detailed\ninformation on the Feature Clustering Block can be found in\nSection III-C .\nC. Feature Clustering Block\nThe Feature Clustering Block is designed to use the features\nat the user interaction points as cluster centers, assigning eachfeature to the clusters of its most similar foreground and back-\nground interaction points. The corresponding similarity guides\nthe assignment of adjustment vectors from these cluster centers.\nThe elucidation of the Feature Clustering Block is delineated\nin Fig. 1. In this Block, the preliminary step entails the in-\ncorporation of the segmentation mask Sp, acquired from the\npreceding interaction round, into the feature representations.\nThis mask is initialized with zero values in the ﬁrst round\nof segmentation. Following mask fusion, the enriched feature\nmap and the feature vectors corresponding to user interaction\npoints, undergo transformation via a reshape operation and a\nshared linear layer, culminating into Q∈Rnh×dhw×C\nnhand\nK∈Rnh×nc×C\nnhrespectively. In every head, Each row vector\nwithinQandKis henceforth denoted as qandk. For tail\nfeatures, both the direction and magnitude of their vectors\nbear signiﬁcant importance, rendering the Euclidean distance\npreferable over cosine similarity under such circumstances.\nThe spatial proximity between qandkalso impinges on the\nsimilarity assessment. Therefore, the afﬁnity metric between q\nandkis indirectly manifested through a linear combination of\ntheir Euclidean and spatial distances. Subsequently, a Gaussian\nRadial Basis Function kernel is employed to transmute this\ncombined distance measure into a similarity metric, nurturing\na more nuanced comprehension of feature interrelations. The\nafﬁnity between qandkis deﬁned as:\nD(q,k)=/bardblq−k/bardbl2+α×norm (/vextenddouble/vextenddouble(χq−χk)·sT/vextenddouble/vextenddouble\n2),(4)\nA(q,k)=e−D2(q,k), (5)\nwhereχqandχkare the coordinates of qandkin the original\nfeature map before the reshape operation, respectively. /bardbl·/bardbl 2\nrefers to min-max normalization. αis a learnable parameter,\nandnorm (·)represents the L2 norm operator. sTis the distance\ncorrection vector that adjusts voxel distances to actual distances,\nproviding more accurate spatial relationships. sTwill be detailed\nin Section IV-B1 . We then merge the afﬁnity maps of multiple\nheads into one by averaging.\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\nSUN et al.: CAISEG: A CLUSTERING-AIDED INTERACTIVE NETWORK FOR LESION SEGMENTATION IN 3D MEDICAL IMAGING 375\nFurthermore, we classify categorize clustering centers based\non user annotations. Those marked as the foreground are clas-\nsiﬁed into the set Sfore, and those as the background into Sback.\nConsequently, the element mask q,kin the cluster mask is de-\nﬁned as\nmask q,k=/braceleftbigg1,A (q,k)=m a x k/prime∈SkA(q,k/prime)\n0, otherwise, (6)\nwhere the Skrefers to the speciﬁc set to which kbelongs, namely\nSforeorSback. Lastly, the Feature Vector Clustering Module\nproduces an output termed the clustered afﬁnity map CA,i n\nwhich the element aq,kis deﬁned as\naq,k=A(q,k)×mask q,k. (7)\nObviously, when the user provides only one category interac-\ntion point, the clustered afﬁnity map CAmeasures the similarity\nbetween each feature vector in the feature map and the most\nsemantically similar cluster center. However, when the user\nprovides both foreground and background category interaction\npoints, the CA assesses the similarity between each feature\nand the two distinct clusters that are most semantically similar.\nThis clustering strategy minimizes potential class imbalances\nresulting from varying quantities of user-provided foreground\nand background interaction points.\nD. Focus Guided Loss\nA robust interactive segmentation network should fully lever-\nage user interaction information as much as possible. Achieving\nthis demands not only efﬁcient encoding of interaction details\nand meticulous design of network structure and modules but\nalso the ﬁne-tuning of network parameters. In practical ap-\nplications, we expect the network to offer more precise pre-\ndictions for voxels close to interaction points in space. This\nnecessitates that network layers responsible for integrating prior\ninformation better understand user priors and predict user in-\nteraction intent. This inductive bias can be instilled in the\nnetwork through the design of loss functions. Just as Dice\nLoss [16] focuses more on classes with fewer samples and Focal\nLoss [17] concentrates on challenging samples, our proposed\nFocus Guided Loss (FG Loss) prioritizes voxels near interac-\ntion points during training. Thus, for a speciﬁc voxel with a\nlabel y and a predicted value ˆp, the FG Loss /lscriptFGis deﬁned\nas:\n/lscriptFG(y,ˆp)=/parenleftBigg\n1+/parenleftbiggdmax−d\ndmax/parenrightbiggλ/parenrightBigg\n×/lscriptBCE(y,ˆp).(8)\nWherein, dis the distance between the voxel and the nearest\nuser interaction point; dmax is the maximum distance among\nthe distances to the voxel from all user interaction points closest\nto that voxel on this image; λis a hyperparameter; /lscriptBCE refers\nto the Binary Cross Entropy Loss. The proposed FG Loss places\nmore emphasis on the prediction accuracy of the interaction\npoint area than other general loss functions, thereby making the\ntrained network have a stronger preference for user interaction\ninformation.E. The Loss Calculation for CAISeg\nIdeally, our network needs to have the following capabilities:rThe adjusted qforeandqbackshould be as close as possible\nto the representative foreground and background features\nin the instance.rThe feature clustering block should be able to measure\nthe similarity between feature vectors and prior features\nas accurately as possible.\nThe former determines the adjustment target of tail features,\nwhile the latter determines the degree of adjustment of the\nfeature vectors. Simply computing the loss between the network\noutput and the ground truth is insufﬁcient to train the network\nto this ideal state. To address this, We perform voxel-level\nclassiﬁcation on the adjusted feature map Fnewbased on the\nadjustedqforeandqback, thereby achieving deep supervision for\nIGM. Speciﬁcally, we ﬁrst perform cross-attention between Fnew\nand the adjusted qforeandqback, i.e.\nqh=concat (qfore,qback), (9)\nˆPcls=Softmax/parenleftBigg\nFnew·(qh)T\n√dk/parenrightBigg\n. (10)\nContinuing, we reshape ˆPclsand then upsample it through inter-\npolation to restore it to the original image shape. The resulting\nnew ˆPclswill be involved in the computation of network loss.\nIn our work, we linearly combine FG Loss /lscriptFGand Dice\nLoss/lscriptDice to form a comprehensive loss function /lscriptcomused to\ncalculate the loss between predicted values ˆPand ground truth\nY. Thus, the loss function /lscriptcomis deﬁned as:\n/lscriptcom(Y,ˆP)=/summationdisplay\ni(μ×/lscriptFG(yi,ˆpi)+(1−μ)×/lscriptDice(yi,ˆpi)),\n(11)\nand the ﬁnal loss function used to supervise the training of\nCAISeg is\n/lscript=ν×/lscriptcom(Y,ˆY)+( 1−ν)×/lscriptcom(Y,ˆPcls), (12)\nwhereμandνare hyperparameters.\nF . Interaction Simulation Strategy\nIn our network, user interaction is automatically simulated\nduring the training process, eliminating the need for manual\ninvolvement. We adopt the iterative training approach from\nRITM [34], wherein, during training phase, a batch undergoes n\ncontinuous rounds of prediction. Only the output from the ﬁnal\nround is compared with the ground truth to compute the loss,\nwhich is then back-propagated to adjust network parameters.\nFor the initial round of point-interaction simulation, we adapt\nstrategies from iFCN [4]with a few modiﬁcations:rFor foreground points, we don’t randomly sample across\nthe entire foreground region. Instead, we ﬁrst erode the\nforeground morphologically, then randomly sample from\nthe remaining area.rFor background points, we sample uniformly from the\nbackground region within a certain range outside the target\nboundary.\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\n376 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 29, NO. 1, JANUARY 2025\nThe ﬁrst modiﬁcation is motivated by the observation that user\ninteraction points rarely fall on the object’s edge in the initial\nround, especially in medical images where many boundaries\nare ambiguous. The second alteration stems from our objective\nto segment all lesion areas in medical images under physician\nguidance, rather than selecting speciﬁc areas of interest from\nmultiple lesion areas. For interaction simulations beyond the\nﬁrst round, we ﬁrst compare the differences between the seg-\nmentation results from the previous round and the ground truth.\nThen we select connected regions from these differential areas\nthat exceed a predetermined threshold in size. Subsequently, a\npoint is sampled from each of these selected connected regions.\nThe binary mask of the segmentation result from the previous\nround is used as Spin Fig. 1for the subsequent round after the\nﬁrst.\nIV. E XPERIMENT SETTING\nA. Datasets and Preprocessing\nWe subject our proposed CAISeg to evaluation using four\nprimary lesion segmentation tasks from the MSD2018 and 2020\nchallenges [42]: Brain Tumor, Colon Cancer, Lung Cancer and\nPancreas Cancer segmentation. Here, we present the partitioning\nand preprocessing strategies for four datasets. The preprocessing\nsteps implemented through TorchIO’s [43] provided interface.\nBrain Tumor dataset comprising 484 cases, each is equipped\nwith lesion annotations. Every case offers four magnetic reso-\nnance imaging (MRI) sequences: Flair, T1w, t1gd, and T2w. The\nannotations delineate into four distinct categories: background,\nedema, non-enhancing tumor, and enhancing tumor. Aligning\nwith our experimental objectives, we selectively employ the\nFlair sequence for the prediction of the entire brain tumor re-\ngion, which amalgamates the sections of edema, non-enhancing\ntumor, and enhancing tumor. The 484 cases were randomly\nstratiﬁed into three segments: 338 (or 70%) for training, 73\n(or 15%) for testing, and another 73 (or 15%) for validation.\nFor computational feasibility, voxel spacings underwent a re-\nsampling from an initial 1.0 mm ×1.0 mm×1.0 mm to\n1.5 mm×1.5 mm×1.5 mm, yielding a uniform scan resolution\nof 104×160×160. Extraneous portions of the image were\nexcised by cropping from the scan’s center to attain dimensions\nof 96×128×128. Additionally, each MRI scan was subjected\nto a series of augmentations: a 50% probability of the intro-\nduction of random MRI bias ﬁeld artifacts, a 50% likelihood of\nGaussian noise addition, a 50% chance of undergoing random\nﬂips, and a 50% propensity for afﬁne transformations that might\ninclude scaling between 0.9 and 1.1 times and rotations within a\n±10◦spectrum. Each scan, in the last leg of preprocessing, was\nindividually subjected to z-score normalization.\nColon Cancer, Lung Cancer, and Pancreas Cancer datasets\nwere also utilized in our study. The Colon Cancer dataset\nconsists of 126 cases, divided into 88 for training (70%), 19\nfor testing (15%), and 19 for validation (15%). Preprocessing\ninvolved truncating image signal intensities between 0.5% and\n99.5%, followed by resampling voxel spacings to a median\nof 5.00 mm ×0.78 mm ×0.78 mm, resulting in an average\nresolution of 89 ×512×512. Each CT image was then croppedor padded to a ﬁnal size of 80 ×512×512. The Lung Can-\ncer dataset comprises 63 CT cases, split into 37 for training\n(60%), 13 for testing (20%), and 13 for validation (20%). The\npreprocessing steps included truncating the CT signal between\n0.5% and 99.5%, adjusting voxel spacings to a median of\n1.24 mm ×0.79 mm ×0.79 mm, and yielding an average\nresolution of approximately 264 ×515×515 after resampling.\nThe ﬁnal images were then cropped or padded to a size of\n256×512×512. The Pancreas Cancer dataset includes 281 CT\ncases, divided into 197 for training (70%), 42 for testing (15%),\nand 42 for validation (15%). Preprocessing involved truncating\nsignal intensities between 0.5% and 99.5%, resampling voxel\nspacings to a median of 2.50 mm ×0.80 mm ×0.80 mm,\nresulting in an average resolution of 102 ×520×520. Each\nCT image was then cropped or padded to 96 ×512×512. For\nall three CT datasets, the same data augmentation techniques\nwere applied as those used in the Brain Tumor dataset, including\nrandom ﬂips, Gaussian noise, afﬁne transformations, and other\nstandard augmentations, with the exception of MRI-speciﬁc\nartifacts.\nB. Implementation Details\n1) Network Architecture: We adopt the VNet [16] as our\nbackbone network. Since the voxel spacing in all three directions\nin the Brain Tumor dataset is the same during preprocessing, the\nfeature map size we use isD\n2×H\n2×W\n2. However, for the Colon\nCancer and Lung Cancer datasets, where the voxel spacing in\nthe vertical direction is larger than the other two directions, the\nfeature map size we use is D×H\n2×W\n2. Four Cross-Attention\nModules are integrated to facilitate cross-attention with the VNet\ndecoder outputs at strides of 16, 8, 4, and 2, with head counts set\nto 16, 16, 8, and 4, respectively. All the parameters in the network\nare initialized using He initialization [44]. Additionally, given\nthe varying pixel distance ratios in medical images across the\nthree dimensions, actual distances, rather than voxel distances,\nare utilized for the distance parameters in (8). Similarly, the\ndistance correction vector sTin(4)is used to convert the distance\nof feature vectors in the feature map to actual distances, where\nthe value of sTis directly related to the downsampling ratio of\nthe feature map and the voxel spacing after resampling. sTis\ndeﬁned as\nsT=(sD×rD,sH×rH,sW×rW)T, (13)\nwheresD,sH, andsWrepresent the voxel spacing in the three\ndirections; rD,rH, andrWare the downsampling ratios of\nthe feature map compared to the original image in the three\ndirections.\n2) Setting of Interactive Simulation Strategy: In the training\nphase, we set the iterative count nas mentioned in Section III-F\nto 3. During the ﬁrst round of interactive sampling, the network\nrandomly samples between 1 to 10 points from the foreground\nregion and 0 to 20 points from the background region. In\nsubsequent rounds of interactive point simulation, the threshold\nmentioned in Section III-F is set to 10% of the lesion volume in\nthe image being processed. During validation, we do not provide\nany interaction information to CAISeg in the initial segmentation\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\nSUN et al.: CAISEG: A CLUSTERING-AIDED INTERACTIVE NETWORK FOR LESION SEGMENTATION IN 3D MEDICAL IMAGING 377\nround, while subsequent rounds exclusively sample one point\nfrom the largest connected region of prediction errors.\n3) T raining Strategy Conﬁguration: Our model was trained\non a single RTX 3090 GPU with 24 GB of memory. The CPU\nused was an Intel Xeon Gold 6326. The training procedure\nemployed the preprocessed images directly for the Brain Tu-\nmor dataset. However, for the Colon Cancer and Lung Can-\ncer datasets, due to the high resolution of the preprocessed\nimages, we employed a Patch-based training and inference\nscheme provided by TorchIO [43]. The patch sizes were set to\n48×128×128 for both Colon Cancer and Pancreas Cancer, and\n64×128×128 for Lung Cancer. The batch size during training\nfor all datasets was consistently set to 2. The fully connected\nlayers in multi Cross-Attention Module were implemented with\nthe drop path strategy [45], having a regularization probability\nof 0.2. Training was conducted using the Adam optimizer [46]\nwith an initial learning rate of 0.001, coupled with L2 regular-\nization with a hyperparameter value of 0.0001. The learning\nrate adjustment followed a step decay strategy. Initially, the\nVNet was pre-trained for 200 epochs using a combined loss of\nDiceLoss [16]and BCELoss. Subsequently, CAISeg was trained\nfor another 200 epochs using the loss described in III-D .T h e\nhyperparameter λin(8)were set as constant values of 2. For\n(11), the hyperparameter μwas dynamically incremented with\nepochs, progressing from 0.3 to 0.6 by the end of training. For\n(12), the hyperparameter νwas set at a constant value of 0.5.\nC. The State-of-The-Art Approaches for Comparison\nWe assess CAISeg alongside various methodologies through\nqualitative and quantitative evaluations, categorizing the meth-\nods into three distinct groups. The ﬁrst encompasses deep\nlearning-based fully automatic segmentation techniques, includ-\ningnnUNet [1]andUniversal Model [3]. The second cate-\ngory consists of traditional interactive segmentation algorithms\nsuch as Random Walker [30] andGraph Cuts [29]. Lastly,\nwe examine deep learning-based interactive segmentation ap-\nproaches, among which are 3D RITM [34],MIDeepSeg [9],\nDINs [7],VMN [6],SAM-Med3D [39],MedLSAM [38], and\nMedSAM [36]. This structured comparison allows for a com-\nprehensive evaluation across different segmentation method-\nologies. Given the diverse interaction requirements of these\nmethods, we specify their interaction simulation strategies in\nour experiments:rFor Graph Cuts, Random Walker, RITM, DINs, and SAM-\nMed3D, seed points located at the lesion’s central area are\nprovided in the ﬁrst segmentation round, while subsequent\ninteraction strategies remain consistent with CAISeg.rMIDeepSeg requires users to provide points describing\nthe target contour in the ﬁrst interaction. We sample six\npoints near the inner boundary of each lesion in different\ndirections within a maximum distance of three voxels from\ncorresponding extreme points on the scan in the ﬁrst round.rVMN requires users to indicate four extreme points on\nthe slice that delineate the target of interest. Our sampling\nstrategy for these extreme points is consistent with that\nof MIDeepSeg. The slices needed for the next round ofinteraction are selected by the assessment module within\nthe model.rMedLSAM and MedSAM encode the bounding box for\ninteractive segmentation. We simulate user interaction by\njittering each connected lesion’s adjacent bounding box\noutward by 1-3 voxels in each direction.\nGiven the different interaction strategies and approaches of\nthese methods, it is impossible to conduct an entirely fair com-\nparison. To mitigate this issue, we used a shared random seed,\nensuring that all methods are evaluated under similar conditions,\nthereby making the comparison results more reliable.\nD. Evaluation Metrics\nTo comprehensively evaluate the performance of the net-\nwork architecture, we employ several key metrics: the Dice\nSimilarity Coefﬁcient (DSC), Recall, Precision, Jaccard Index,\n95th percentile Hausdorff Distance (Hd 95), and Average Surface\nDistance (ASD). For interactive segmentation methods, where\nthe segmentation accuracy can be inﬂuenced by the positioning\nof interaction points, we conducted 10 rounds of validation using\ndifferent random seeds. The mean and standard deviation of\nthese ten results are reported to provide a detailed assessment of\nperformance under varying conditions.\nAdditionally, we use the mean number of clicks (mNoC)\nto assess the efﬁciency of the interactive image segmentation\nalgorithm. This metric represents the average number of clicks\nrequired to attain the speciﬁed DSC. A smaller mNoC indicates\na reduced interaction burden needed to achieve the desired\naccuracy level.\nV. R ESULTS AND DISCUSSION\nA. Comparison With Other Methods\nTables IandIIpresent the segmentation performance metrics\nand mNOC values for various segmentation algorithms across\nthree datasets. The performance metrics reﬂect the best results\nobtained after sampling 20 points for all point-based interactive\nmethods, or the results from a single round using the bound-\ning box interaction method. mNOC@x indicates the number\nof clicks required by an algorithm to achieve x% DSC. The\nsummarized results are as follows:rCAISeg demonstrated strong performance across multiple\nkey metrics, particularly in Dice coefﬁcient and mNOC.\nFor instance, CAISeg achieved Dice scores of 93.4% on\nthe Brain Tumor dataset and 81.1% on the Colon Cancer\ndataset, showcasing its effectiveness in accurately captur-\ning and segmenting target regions based on user guidance\nin complex medical image segmentation tasks. In terms of\nmNOC, CAISeg performed exceptionally well across all\ndatasets, especially in the Pancreas Cancer dataset, where\nit required only 2.9 clicks to achieve high segmentation ac-\ncuracy. Additionally, CAISeg exhibited superior Precision\nand Recall on certain datasets, indicating its advantage in\nreducing false positives and false negatives. However, it\nis worth noting that on some datasets and metrics, such as\nHD95 and ASD on the Lung Cancer dataset, CAISeg’s\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\n378 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 29, NO. 1, JANUARY 2025\nTABLE I\nCOMPARISON OF CAIS EGAGAINST OTHER APPROACHES ACROSS BRAIN TUMOR AND COLON CANCER SEGMENTATION TASKS IN\nTERMS OF VARIOUS METRICS\nTABLE II\nCOMPARISON OF CAIS EGAGAINST OTHER APPROACHES ACROSS LUNG CANCER AND PANCREAS CANCER SEGMENTATION TASKS IN\nTERMS OF VARIOUS METRICS\nperformance was comparable to or slightly lower than\nother advanced methods.rT-test analysis shows that CAISeg’s improvements in key\nmetrics, such as Dice coefﬁcient and mNOC on the Brain\nTumor and Colon Cancer datasets, are statistically signiﬁ-\ncant (p<0.05) compared to other methods. However, it is\nimportant to note that these improvements are not statisti-\ncally signiﬁcant in all cases. For instance, while CAISeg\nperformed well on the Lung Cancer and Pancreas Cancer\ndatasets, its improvements over the best-performing meth-\nods in these metrics did not reach statistical signiﬁcance.rCAISeg exhibited low standard deviation on certain met-\nrics, reﬂecting a high degree of consistency under different\ninteraction sampling conditions, particularly in mNOC\nand Dice coefﬁcient on the Brain Tumor dataset. How-\never, CAISeg showed higher standard deviation on other\nmetrics, such as Precision and Recall on the Lung Cancer\ndataset, indicating some variability in performance under\nthese conditions.\nFig. 2presents the segmentation outcomes from eight in-\nteractive neural networks after user interaction on the same\ntissue slice. The analysis illustrates that MIDeepSeg, VMN,SAM-Med3D, MedLSAM, and MedSAM exhibit certain con-\nstraints in terms of interaction ﬂexibility. Notably, MedLSAM\nand MedSAM lack the capacity to reﬁne segmentation results\neffectively. Concurrently, MIDeepSeg and VMN impose posi-\ntional requirements on user-provided interaction points, limiting\ninteraction ﬂexibility and indirectly raising the effort for users\nto interact proﬁciently. In contrast, DINs, 3D RITM, SAM-\nMed3D, and CAISeg are more straightforward and unrestricted,\nallowing users to place interaction points anywhere on the scan.\nThis increase in freedom, however, may lead to a decrease in the\namount of information conveyed through interactions. Among\nthese, CAISeg utilizes clustering analysis to capture the deep\nsemantic similarity in user interactions, offering a more accurate\nunderstanding and response to user intent compared to DINs,\n3D RITM and SAM-Med3D. Fig. 3demonstrates the capability\nof CAISeg to understand user interaction intent through the\nsegmentation results of multiple adjacent slices.\nB. Ablation Studies\nTo gain more insights into our model, we investigate the\ninﬂuence of essential components in CAISeg.\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\nSUN et al.: CAISEG: A CLUSTERING-AIDED INTERACTIVE NETWORK FOR LESION SEGMENTATION IN 3D MEDICAL IMAGING 379\nFig. 2. Demonstations of segmentation results from different interactive neural networks for different lesion segmentation tasks. In the ﬁgure, from\ntop to bottom, the lesions sequentially represent brain tumors, colon cancer, lung cancer, and pancreas cancer. In the second column, the yellow\nboxes represent the bounding boxes required by MedSAM and MedLSAM; blue interaction points denote the extreme points needed by MIDeepSeg\nand VMN; red and green points indicate the foreground and background interaction points required by DINs, 3D RITM, SAM-Med3D and CAISeg,\nrespectively. The regions outlined by cyan lines represent the ground truth, while the regions enclosed by magenta lines correspond to the predicted\nmasks by the respective methods.\nFig. 3. Examples of CAISeg’s mask reﬁnement using interaction points. The series showcases corrections applied by CAISeg to a prior round’s\nsegmentation mask across ﬁve adjacent slices. Regions enclosed within cyan and magenta lines represent the actual lesion masks and the\nnetwork’s prediction, respectively. Annotations on the optimized masks indicate the locations and types of user interaction points.\n1) Quantitative Analysis of Feature Clustering Block: We\nconducte a quantitative analysis on the afﬁnity computation\napproach implemented in our Feature Clustering Block. Diverg-\ning from common practices of absolute position embedding to\narticulate positional relationships, our module directly assessesthe actual distances between voxels and interaction points. As for\nmeasuring vector similarity, we opted for an encoding method\ncombining Euclidean distance with a Gaussian radial basis\nfunction, instead of the usual cosine similarity. Table IIIshow-\ncases the appropriateness of our chosen similarity computation\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\n380 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 29, NO. 1, JANUARY 2025\nTABLE III\nABLATION EXPERIMENTS ON THE FEATURE CLUSTERING BLOCK ON BRAIN TUMOR DATASET\nFig. 4. Left: Two examples of heatmap results yielded by the Feature Clustering Block utilizing different similarity calculation methods. In the ﬁrst\ncolumn, the cyan line is the outline of the ground truth; the green points are the foreground interaction points; Middle: Heatmap results of the CAISeg\nwith and without deep supervision. The cyan line in the picture is the outline of the ground truth, and the red points are the background interaction\npoints; Right: Experiment results of different λin the Focus Guided Loss.\nstrategy within the context of our speciﬁc task. Insights from\nTable IIIreveal that:rPositional information plays a pivotal role in assessing the\nsemantic similarity between feature vectors and interac-\ntion points, to the extent that it is a key determinant in the\nnetwork’s trainability. Furthermore, given that CAISeg ex-\nclusively operates with sparse feature vectors correspond-\ning to interaction points for cross-computation against the\nfeature map, learning the parameters for absolute position\nembedding becomes particularly challenging.rThe fusion of Euclidean distance with Gaussian RBF\nsigniﬁcantly (p <0.05) enhances the delineation of feature\nvectors that are strikingly similar to the interaction points.\nThis is because cosine similarity only accounts for vec-\ntor orientation—pattern similarity—whereas identifying\ninconspicuous tail-end features requires evaluating both\nvector direction and magnitude. Euclidean distance excels\nin capturing this comprehensive relationship. Left picture\nin Fig. 4shows the clustering heatmap results obtained\nusing these two different methods of calculating vector\nsimilarity. Notably, the method that utilizes Euclidean dis-\ntance in conjunction with a Gaussian radial basis function\ndemonstrates a superior ability to delineate areas that share\na high semantic resemblance with the user interaction\npoints.rEmploying a multi-head mechanism to break\ndown high-dimensional feature vectors into severalTABLE IV\nABLATION EXPERIMENTS ON THE DEEPSUPERVISION OF THE\nINTERACTION -GUIDED MODULE\nlower-dimensional ones allows Euclidean distance to\ncompute feature similarities with enhanced accuracy.\nThis conﬁguration achieves the best performance in\nour experiments, with the highest DSC score and the\nlowest mNOC metric. The lower variability observed\nin the results indicates a more consistent and reliable\nperformance across different random seeds, further\nsupporting the effectiveness of this design.\n2) The Effects of Deep Supervision of the Interaction-Guided\nModule: Implementing deep supervision on the Interaction-\nguided Module enhances the model’s ability to effectively learn\nhead feature queries and parameters within both the cross-\nattention and feature clustering modules. Table IVillustrates\nthe effects of deep supervision on the CAISeg’s performance\nacross three datasets. It is evident that the use of deep supervision\nnot only improves the quality of the network’s segmentation\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\nSUN et al.: CAISEG: A CLUSTERING-AIDED INTERACTIVE NETWORK FOR LESION SEGMENTATION IN 3D MEDICAL IMAGING 381\nFig. 5. Validation DSC over training epochs for CAISeg on the Pan-\ncreas dataset using different loss functions.\ncapabilities but also contributes to greater stability in the model’s\nperformance. Middle picture in Fig. 4provides a visual com-\nparison of similarity heatmaps both before and after applying\nthe Interaction-guided Module adjustments, with and without\nthe strategy of deep supervision. The comparative analysis of\npre-adjustment heatmaps reveals that CAISeg, with deep super-\nvision, is more adept at learning head features. Meanwhile, the\npost-adjustment heatmap comparison shows that CAISeg gains\na better understanding of the user’s interactive intent with deep\nsupervision implemented.\n3) The Effects of the Focus Guided Loss: The Focus Guided\nLoss function, by employing a weighted approach, steers the\nnetwork’s attention toward boosting prediction accuracy for\nvoxels near user interaction points. Here, the hyper-parameter λ\ndetermines the scope of inﬂuence for the signiﬁcance weighting.\nIn extreme cases, when λequals zero, the Focus Guided Loss\ndegenerates to BCE Loss. Right picture in Fig. 4showcases\nhow different values of λaffect the CAISeg’s metrics on brain\ntumor dataset. It is observable that varying λvalues can impact\nboth the quality of the network’s initial segmentation and its\nresponsiveness to interaction points. With a lower λvalue, the\nnetwork performs better in segmentation quality with fewer\ninteraction points but demonstrates weaker comprehension of\nuser interactions. Conversely, with an overly high λvalue, the\nnetwork’s capability to extract visual features is compromised,\nwhich in turn affects the results of similarity computations\nbetween feature vectors within the Feature Clustering Mod-\nule, leading to a decline in the network’s overall segmentation\nabilities.\nFurthermore, we evaluated the performance of CAISeg by\ntraining the model using three different loss functions: Dice\nLoss, Focal Loss, and our proposed Focus Guided Loss. The\nresults, shown in Fig. 5, reﬂect the Dice Similarity Coefﬁcient\n(DSC) values after three rounds of interactions on the vali-\ndation set across 200 epochs. The model trained with Focus\nGuided Loss consistently achieved higher DSC values through-\nout the training process. In comparison, the model trained with\nDice Loss displayed moderate performance, reaching a plateau\naround the 125th epoch. Meanwhile, the model trained withFocal Loss exhibited greater variability and slower convergence,\nresulting in signiﬁcantly lower DSC values. These ﬁndings high-\nlight that Focus Guided Loss offers a more stable and effective\nlearning process, leading to superior segmentation performance,\nparticularly in the later stages of training. This emphasizes\nthe practical advantages of Focus Guided Loss for interactive\nsegmentation tasks, where precise alignment with user-guided\nfeatures is essential.\nVI. C ONCLUSION\nIn this work, we presented CAISeg, an advanced interactive\nneural network designed for the segmentation of lesions in med-\nical images. CAISeg requires only minimal input from medical\nexperts, such as simple clicks within areas of misprediction on\nexisting segmentation masks. Leveraging a clustering approach,\nthe network effectively captures and realigns semantic features\nthat closely resemble the interaction points, ensuring accurate\ncorrection of segmentation errors by mapping these features\nto the appropriate head feature distribution. Furthermore, our\nproposed Focus Guided Loss has demonstrated superior effec-\ntiveness compared to conventional loss functions in enhancing\nthe performance of point-based interactive segmentation net-\nworks. However, it is important to note that the efﬁcacy of\nCAISeg’s Interaction-Guided Module (IGM) is contingent on\nthe availability of a sufﬁciently large training dataset. Smaller\ndatasets can lead to the network becoming overly sensitive to\ninteraction points, resulting in inconsistent segmentation quality.\nThis highlights the need for adequately sized datasets to fully\nharness the potential of CAISeg, ensuring stable and reliable seg-\nmentation performance across various medical imaging tasks.\nREFERENCES\n[1] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,\n“NNU-Net: A self-conﬁguring method for deep learning-based biomedical\nimage segmentation,” Nat. Methods , vol. 18, no. 2, pp. 203–211, 2021.\n[2] A. Hatamizadeh, V . Nath, Y . Tang, D. Yang, H. R. Roth, and D. Xu,\n“Swin unetr: Swin transformers for semantic segmentation of brain tumors\nin MRI images,” in Proc. Int. MICCAI Brainlesion Workshop , 2021,\npp. 272–284.\n[3] J. Liu et al., “Clip-driven universal model for organ segmentation and\ntumor detection,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2023,\npp. 21152–21164.\n[4] N. Xu, B. Price, S. Cohen, J. Yang, and T. S. Huang, “Deep interactive\nobject selection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,\n2016, pp. 373–381.\n[5] Q. Liu, Z. Xu, Y . Jiao, and M. Niethammer, “isegformer: Interactive\nsegmentation via transformers with application to 3 D knee mr images,”\ninProc. Int. Conf. Med. Image Comput. Comput.- Assist. Interv. , 2022,\npp. 464–474.\n[6] T. Zhou, L. Li, G. Bredell, J. Li, J. Unkelbach, and E. Konukoglu, “V ol-\numetric memory network for interactive medical image segmentation,”\nMed. Image Anal. , vol. 83, 2023, Art. no. 102599.\n[7] J.-W. Zhang et al., “DINs: Deep interactive networks for neuroﬁbroma\nsegmentation in neuroﬁbromatosis type 1 on whole-body MRI,” IEEE J.\nBiomed. Health Inform. , vol. 26, no. 2, pp. 786–797, Feb. 2022.\n[8] G. Wang et al., “Deepigeos: A deep interactive geodesic framework for\nmedical image segmentation,” IEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 41, no. 7, pp. 1559–1572, Jul. 2019.\n[9] X. Luo et al., “Mideepseg: Minimally interactive segmentation of unseen\nobjects from medical images using deep learning,” Med. Image Anal. ,\nvol. 72, 2021, Art. no. 102102.\n[10] Q. Yu et al., “CMT-deeplab: Clustering mask transformers for panoptic\nsegmentation,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. ,\n2022, pp. 2560–2570.\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\n382 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 29, NO. 1, JANUARY 2025\n[11] Q. Yu et al., “K-means mask transformer,” in Proc. Eur. Conf. Comput.\nVis., 2022, pp. 288–307.\n[12] M. Yuan et al., “Devil is in the queries: Advancing mask transformers\nfor real-world medical image segmentation and out-of-distribution local-\nization,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2023,\npp. 23879–23889.\n[13] Y . Ding, L. Li, W. Wang, and Y . Yang, “Clustering propagation for\nuniversal medical image segmentation,” in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. , pp. 3357–3369, 2024.\n[14] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S.\nZagoruyko, “End-to-end object detection with transformers,” in Proc. Eur.\nConf. Comput. Vis. , 2020, pp. 213–229.\n[15] B. Cheng, A. Schwing, and A. Kirillov, “Per-pixel classiﬁcation is not all\nyou need for semantic segmentation,” in Proc. Adv. Neural Inf. Process.\nSyst. , 2021, vol. 34, pp. 17864–17875.\n[16] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-Net: Fully convolutional\nneural networks for volumetric medical image segmentation,” in Proc. 4th\nInt. Conf. 3D Vis. , 2016, pp. 565–571.\n[17] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for\ndense object detection,” in Proc. IEEE Int. Conf. Comput. Vis. , 2017,\npp. 2980–2988.\n[18] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. , 2015, pp. 3431–3440.\n[19] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-\nworks for biomedical image segmentation,” in Proc. Med. Image Comput.\nComput.-Assist. Interv. 2015, 18th Int. Conf. , 2015, pp. 234–241.\n[20] X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, and P.-A. Heng, “H-DenseUNet:\nHybrid densely connected UNet for liver and tumor segmentation from\nCT volumes,” IEEE Trans. Med. Imag. , vol. 37, no. 12, pp. 2663–2674,\nDec. 2018.\n[21] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “UNet :\nRedesigning skip connections to exploit multiscale features in image\nsegmentation,” I E E ET r a n s .M e d .I m a g . , vol. 39, no. 6, pp. 1856–1867,\nJun. 2020.\n[22] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” in Proc. ICLR , 2021.\n[23] A. Hatamizadeh et al., “UNetR: Transformers for 3D medical image\nsegmentation,” in Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis. , 2022,\npp. 574–584.\n[24] H. Cao et al., “Swin-UNet: Unet-like pure transformer for medical image\nsegmentation,” in Proc. Eur. Conf. Comput. Vis. , 2022, pp. 205–218.\n[25] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2021,\npp. 10012–10022.\n[26] X. Liang, N. Li, Z. Zhang, J. Xiong, S. Zhou, and Y . Xie, “Incorporating the\nhybrid deformable model for improving the performance of abdominal CT\nsegmentation via multi-scale feature fusion network,” Med. Image Anal. ,\nvol. 73, 2021, Art. no. 102156.\n[27] W. He et al., “A statistical deformation model-based data augmentation\nmethod for volumetric medical image segmentation,” Med. Image Anal. ,\nvol. 91, 2024, Art. no. 102984.[28] S. Osher and J. A. Sethian, “Fronts propagating with curvature-dependent\nspeed: Algorithms based on Hamilton-Jacobi formulations,” J. Comput.\nPhys. , vol. 79, no. 1, pp. 12–49, 1988.\n[29] Y . Y . Boykov and M.-P. Jolly, “Interactive graph cuts for optimal boundary\n& region segmentation of objects in nd images,” in Proc. 8th IEEE Int.\nConf. Comput. Vis. , 2001, vol. 1, pp. 105–112.\n[30] L. Grady, “Random walks for image segmentation,” IEEE Trans. Pattern\nAnal. Mach. Intell. , vol. 28, no. 11, pp. 1768–1783, Nov. 2006.\n[31] R. Adams and L. Bischof, “Seeded region growing,” IEEE Trans. Pattern\nAnal. Mach. Intell. , vol. 16, no. 6, pp. 641–647, Jun. 1994.\n[32] V . Vezhnevets and V . Konouchine, “Growcut: Interactive multi-label nd\nimage segmentation by cellular automata,” in Proc. Of Graphicon , 2005,\nvol. 1, pp. 150–156.\n[33] K.-K. Maninis, S. Caelles, J. Pont-Tuset, and L. Van Gool, “Deep extreme\ncut: From extreme points to object segmentation,” in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. , 2018, pp. 616–625.\n[34] K. Soﬁiuk, I. A. Petrov, and A. Konushin, “Reviving iterative training\nwith mask guidance for interactive segmentation,” in Proc. IEEE Int. Conf.\nImage Process. , 2022, pp. 3141–3145.\n[35] W. Liu, C. Ma, Y . Yang, W. Xie, and Y . Zhang, “Transforming the\ninteractive segmentation for medical imaging,” in Int. Conf. Med. Image\nComput. Comput.- Assist. Interv. , 2022, pp. 704–713.\n[36] J. Ma, Y . He, F. Li, L. Han, C. You, and B. Wang, “Segment\nanything in medical images,” Nat. Commun. , vol. 15, no. 1, 2024,\nArt. no. 654.\n[37] A. Kirillov et al., “Segment anything,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis. , 2023, pp. 4015–4026.\n[38] W. Lei, X. Wei, X. Zhang, K. Li, and S. Zhang, “Medlsam: Lo-\ncalize and segment anything model for 3D medical images,” 2023,\narXiv:2306.14752 .\n[39] H. Wang et al., “Sam-Med3d,” 2023, arXiv:2310.15161 .\n[40] H. Wang, Y . Zhu, H. Adam, A. Yuille, and L.-C. Chen, “Max-\ndeeplab: End-to-end panoptic segmentation with mask transformers,”\ninProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2021,\npp. 5463–5474.\n[41] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, “Masked-\nattention mask transformer for universal image segmentation,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2022, pp. 1290–1299.\n[42] M. Antonelli et al., “The medical segmentation decathlon,” Nat. Commun. ,\nvol. 13, no. 1, 2022, Art. no. 4128.\n[43] F. Pérez-García, R. Sparks, and S. Ourselin, “Torchio: A python library for\nefﬁcient loading, preprocessing, augmentation and patch-based sampling\nof medical images in deep learning,” Comput. Methods Programs Biomed. ,\nvol. 208, 2021, Art. no. 106236.\n[44] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:\nSurpassing human-level performance on imagenet classiﬁcation,” in Proc.\nIEEE Int. Conf. Comput. Vis. , 2015, pp. 1026–1034.\n[45] G. Huang, Y . Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep networks\nwith stochastic depth,” in Proc. Comput. Vis.–ECCV 2016, 14th Eur. Conf. ,\n2016, pp. 646–661.\n[46] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\n2014, arXiv:1412.6980 .\nAuthorized licensed use limited to: Shanghai Jiaotong University. Downloaded on August 28,2025 at 18:35:35 UTC from IEEE Xplore.  Restrictions apply. \n\n\n</文献>\n\n<我的研究方法>\n在本硕士研究中，我围绕血液试管的自动化识别与分类展开，设计并实现了一个基于计算机视觉的多模态检测系统 —— BloodScan。该系统以深度学习为核心，结合机械臂控制与高分辨率图像采集，实现了对血液样本中不同分层（血清层、白膜层、血浆层）的高精度识别与评估。\n\n研究的核心创新在于提出了跨模态注意力机制（CrossModalAttention），通过蓝光与白光图像的融合，提升了血液分层边界的识别精度。在模型设计上，采用双Backbone结构分别提取不同光源下的特征，并在多个尺度上实现跨模态特征融合，兼顾检测精度与计算效率。相比传统YOLO架构，本研究提出的跨模态注意力策略在准确率、召回率上取得显著提升。\n\n系统模块涵盖了数据采集与增强、YOLO模型训练与评估、注意力可视化、以及机械臂和摄像头的软硬件联动控制。为提升模型训练效率，还构建了6通道融合数据集，并制定了严格的“精确一次检测”医学评估标准。\n\n本项目计划不仅在方法上具有创新性，也希望在工程实现层面达到高完整度，具备良好的可复现性和扩展性，为临床血液分析的自动化提供了有力支撑。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在血液分析检测、图像识别、深度神经网络、图像处理、计算机视觉、多模态融合或医学图像分析等方面的内容\n2. 总结该文献的提纲和关键信息\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4.1", "messages": [{"role": "system", "content": "你是一位专业的学术文献分析专家。你的任务是分析给定的文献，提取关键信息和文章结构提纲。你的分析将会帮助用户了解文献的具体的工作，并用于用户的论文撰写灵感、论文提纲设计和论文投稿，请你返回大约500字左右的分析结果。"}, {"role": "user", "content": "\n<文献>\nDeep MultimodalData Fusion\nFEI ZHAO ,TheUniversityof Alabama at Birmingham,Birmingham,AL, USA\nCHENGCUIZHANG ,TheUniversityof Alabama at Birmingham,Birmingham,AL, USA\nBAOCHENG GENG ,TheUniversityof Alabama at Birmingham,Birmingham,AL, USA\nMultimodal Artificial Intelligence (Multimodal AI), in general, involves various types of data (e.g., images,\ntexts,ordatacollectedfromdifferentsensors),featureengineering(e.g.,extraction,combination/fusion),and\ndecision-making(e.g.,majorityvote).Asarchitecturesbecomemoreandmoresophisticated,multimodalneu-\nral networks can integrate feature extraction, feature fusion, and decision-making processes into one single\nmodel.Theboundariesbetweenthoseprocessesareincreasinglyblurred.Theconventionalmultimodaldata\nfusion taxonomy (e.g., early/late fusion), based on which the fusion occurs in, is no longer suitable for the\nmodern deep learning era. Therefore, based on the main-stream techniques used, we propose a new fine-\ngrained taxonomy grouping the state-of-the-art (SOTA) models into five classes: Encoder-Decoder methods,\nAttention Mechanism methods, Graph Neural Network methods, Generative Neural Network methods, and\nother Constraint-based methods. Most existing surveys on multimodal data fusion are only focused on one\nspecifictaskwithacombinationoftwospecificmodalities.Unlikethose,thissurveycoversabroadercombi-\nnationofmodalities,includingVision+Language(e.g.,videos,texts),Vision+Sensors(e.g.,images,LiDAR),\nand so on, and their corresponding tasks (e.g., video captioning, object detection). Moreover, a comparison\namong thesemethods is provided,as well as challenges andfuturedirections in thisarea.\nCCS Concepts: • Computing methodologies →Artificial intelligence ;Natural language processing ;\nComputervision ;Machinelearning;\nAdditional Key Words andPhrases:Datafusion,neuralnetworks,multimodal deep learning\nACM Reference Format:\nFeiZhao,ChengcuiZhang,andBaochengGeng.2024.DeepMultimodalDataFusion. ACMComput.Surv. 56,\n9, Article216 (April 2024), 36pages. https://doi.org/10.1145/3649447\n1 INTRODUCTION\nData,withoutadoubt,isanextremelyimportantcatalystintechnologicaldevelopment,especially\ninArtificial Intelligence (AI) field. In the last 20 years, the amount of data generated in this\nperiodaccountsforabout90%ofalldataavailableintheworld.Moreover,therateofdatagrowth\nis stillaccelerating.Theexplosionof data providesanunprecedentedchancefor AIto thrive.\nWith the advancement of sensor technologies, not only the amount and quality of data is in-\ncreasedandenhanced,butthediversityofdataisalsoskyrocketing.Thedatacapturedfromdiffer-\nentsensorsprovidepeoplewithdistinct“views”or“perspectives”ofthesameobjects,activities,or\nAuthors’ addresses: F. Zhao, The University of Alabama at Birmingham, University Hall 4105, 1402 10th Ave. S., Birm-\ningham, AL, 35294, USA; e-mail: larry5@uab.edu; C. Zhang, The University of Alabama at Birmingham, University Hall\n4143, 1402 10th Ave. S., Birmingham, AL, 35294, USA; e-mail: czhang02@uab.edu; B. Geng, The University of Alabama at\nBirmingham,University Hall4147,140210thAve. S.,Birmingham,AL, 35294,USA; e-mail:bgeng@uab.edu.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be\nhonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,\nrequires prior specific permission and/or a fee.Request permissions from permissions@acm.org .\n© 2024Copyrightheld bytheowner/author(s). Publicationrights licensed toACM.\nACM 0360-0300/2024/04-ART216\nhttps://doi.org/10.1145/3649447\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n\n216:2 F. Zhao et al.\nFig. 1. The worldhas beenprojected intomultipledimensions/domains.\nphenomena.Inotherwords,peopleareabletoobservethesameobjects,activities,orphenomena\nindifferent“dimensions”or“domains”byusingdifferentsensors.Thesenew“views”helppeople\nobtainabetterunderstandingoftheworld.Forexample,100yearsago,inthemedicalfield,itwas\nextremely difficult for physicians to diagnose whether a patient has a lung tumor due to the lim-\nited way of observing organs. After the invention of the first computerized tomography (CT)\nscannerbasedonX-raytechnology,thedatacapturedfromthemachineprovidemuchricherinfor-\nmation about lungs, enabling physicians to make diagnoses based on CT images alone. With the\nadvancementoftechnology, magneticresonanceimaging (MRI),amedicalimagingtechnique\nthatusesstrongmagneticfieldsandradiowaves,hasbeenusedtodetecttumorsaswell.Nowadays,\nphysicians are able to access multimodal data including CT, MRI, and blood test data, and so on.\nTheaccuracyofdiagnosisbasedonthecombinationofthesedataismuchhigher,comparedwith\nthat based on a single modality alone, e.g., CT, or MRI only. This is because the complementary\nandredundantinformationamongCT,MRI,andbloodtestdatacanhelpphysiciansbuildamore\ncomprehensiveviewofanobservedobject,activity,orphenomenon.EvolutionofAIalsofollowsa\nsimilarpath.Initsinfancy,AIonlyfocusesonsolvingproblemsusingasinglemodality.Nowadays,\nAItoolshavebecomeincreasinglycapableofsolvingreal-worldproblemsbyusingmultimodality.\nWhat is multimodality? In reality, when we experience the world, we see objects, hear sounds,\nfeel textures, smell odors, and taste flavors [ 11]. The world is represented by information in dif-\nferentmediums,e.g.,vision,sounds,andtextures.AvisualizationisshowninFigure 1.Ourrecep-\ntors such as eyes and ears, help us capture the information. Then, our brain will be able to fuse\nthe information from different receptors to form a prediction or a decision. The information ob-\ntainedfromeachsource/mediumcanbeviewedasonemodality.Whenthenumberofmodalities\nis greater than one, we call it multimodality. However, instead of using eyes and ears, machines\nhighlydependonsensorssuchasRGBcameras,microphones,orothertypesofsensors,asshown\nin Figure 2. Each sensor can map the observed objects/activities into its own dimension. In other\nwords, the observed objects/activities can be projected into the dimension of each sensor. Then,\nmachinesorrobotscancollectthedatafromeachsensorandmakeapredictionordecisionbased\nonthem.Intheindustry,therearenumerousapplicationstakingadvantageofmultimodality.For\nexample, autonomous vehicle, which is one of the hottest topics since the 2020s, is a typical ap-\nplication relying on multimodality. Such a system requires multiple types of data from different\nsensors, e.g., LiDAR sensors, Radar sensors, cameras, and GPS. The model will fuse these data to\nmakereal-timepredictions.Inthemedicalfield,moreandmoreapplicationsrelyonthefusionof\nmedicalimagingandelectronichealthrecordstoenablemodelstoanalyzeimagingfindingsinthe\nclinicalcontext,e.g.,CTand MRIfusion.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:3\nFig. 2. Theworld has beenprojected intomultipledimensions/domainsby different typesof sensors.\nFig.3. Thekidisstrikingadrum.Evenifthedrumisnotvisible,basedonthevisionandaudioinformation,\nwe can stillrecognize theactivitycorrectly.\nWhy do we need multimodality? In general, multimodal data refer to the data collected from\ndifferent sensors, e.g., CT images, MRI images, and blood test data for the cancer diagnosis, RGB\ndata and LiDAR data for autonomous driving system, RGB data and infrared data for skeleton\ndetection of Kinect [ 28]. For the same observed object or activity, the data from different modali-\ntiescanhavedistinctexpressionsandperspectives.Althoughthecharacteristicsofthesedatacan\nbe independent and distinct, they often overlap semantically. This phenomenon is called infor-\nmation redundancy. Furthermore, information from different modalities can be complementary.\nHumans can unconsciously fuse the multimodal data, obtain knowledge, and make predictions.\nThecomplementaryandredundantinformationextractedfrommultimodalitiescanhelphumans\nform a comprehensive understanding of the world. As the example shown in Figure 3,w h e na\nkid is drumming, even if we cannot see the drum, we are still able to recognize a drum that is\nbeing struck based on the sounds. In this process, we unconsciously fuse the vision and acoustic\ndata, and extract the complementary information of them, to make a correct prediction. If there\nis only one modality available, e.g., vision modality with the drum object out of sight, we can\nonly tell that a kid is waving two sticks. With only the sound available, we would only be able\nto tell that a drum is being struck without knowing who is drumming. Therefore, in general, the\nindependent interpretation based on individual modality only presents partial information of the\nobservedactivity.However,themultimodality-basedinterpretationcandeliverthe“fullerpicture”\noftheobservedactivity,whichcanbemorerobustandreliablethansingle-modality-basedmodels.\nFor instance, autonomous vehicles containing multiple sensors such as RGB cameras and LiDAR\nsensors, need to detect objects on the road in extreme weather conditions where visibility is near\nzero, e.g., dense fog or heavy rain. A multimodal-based model can still detect objects while the\npure-vision-based models cannot. However, it is extremely hard for machines to understand and\nfigure out how to fuse and take advantages of the complementary nature of multimodal data to\nimprove theprediction/classificationaccuracy.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:4 F. Zhao et al.\nHowtofusemultimodaldata?Inthe1990s,astraditional MachineLearning (ML),asubclass\nofAI,flourished,ML-basedmodelsforaddressingmultimodalproblemsbegantothrive.Itbecame\ncommon for the machine to extract knowledge from multimodal data and make decisions. How-\never,backthenmostoftheworkswerefocusedonfeatureengineering,e.g.,howtoobtainabetter\nrepresentation for each modality. During that time, many modality-specific hand-crafted feature\nextractorswereproposed,whichgreatlyrelyonpriorknowledgeofthespecifictasksandthecor-\nresponding data. Since these feature extractors work independently, they can hardly capture the\ncomplementary and redundant nature of multimodalities. Therefore, such a feature engineering\nprocess inevitably results in a loss of information before the features are sent to the ML-based\nmodel. This leads to a negative impact on the performance of the traditional ML-based models.\nAlthough traditional ML-based models have the ability to analyze multimodal information, there\nisalongwaytoachievetheultimategoalofAI,whichistomimichumansorevensurpasshuman\nperformance. Therefore, how to fuse the data in a way that can automatically learn the comple-\nmentary and redundant information and minimize the manual interference remains a problem in\nthetraditionalMLfield.\nDeeplearningisasub-fieldofML.Itallowscomputationalmodelsthatarecomposedofmultiple\nprocessing layers to learn representations of data with multiple levels of abstraction [ 88]. Its key\nadvantageisthatthehierarchicalrepresentationscanbelearnedinanautomatedway,whichdoes\nnot require domain knowledge or human effort. For example, to the data: X={x1,x2,...,xN},\nY={y1,y2,...,yN}, a two-layer neural network can be defined as the combination of matrices\nW, and non-linear function σ(·)as shown in Equation ( 1). After training process, we can find W\nfor which ˆyiis close to yifor alli≤N. As the depth of the model continues to increase, so does\nitsabilityof featurerepresentation.\nˆyi(xi)=W2·σ(W1xi). (1)\nSince 2010, multimodal data fusion has entered the stage of deep learning in an all-around\nway. Deep learning-based multimodal data fusion methods have demonstrated outstanding re-\nsults in various applications. For video-audio-based multimodal data fusion, the works from\n[35,37,51,163] address the emotion recognition problem by using deep learning techniques,\nincluding convolutional neural networks, long short-term memory (LSTM) networks, at-\ntention mechanisms, and so on. Also, for video-text multimodal data fusion, the works from\n[41,56,68,107,123,124,195]addressthetext-to-videoretrievaltaskbyusingTransformer,BERT,\nattention mechanism, adversarial learning, and a combination of them. There are various other\nmultimodal tasks, e.g., visual question answering (VQA) (text-image: [ 154,220], text-video:\n[82,223]), RGB-depth object segmentation [ 31,39], medical data analysis [ 181,185], and image\ncaptioning[ 216,237].ComparedtotraditionalML-basedmethods, deepneuralnetwork (DNN)-\nbased methods show superior performance on representation learning and modality fusion if the\namountofthetrainingdataislargeenough.Furthermore,DNNisabletoexecutefeatureengineer-\ning by itself, which means a hierarchical representation can be automatically learned from data,\ninsteadofmanuallydesigningorhandcraftingmodality-specificfeatures.Traditionally,themeth-\nodsofmultimodaldatafusionareclassifiedintofourcategories,basedontheconventionalfusion\ntaxonomy shown in Figure 4, including early fusion, intermediate fusion, late fusion, and hybrid\nfusion:(1)earlyfusion:Therawdataorpre-processeddataobtainedfromeachmodalityarefused\nbeforebeingsenttothemodel;(2)intermediatefusion:thefeaturesextractedfromdifferentmodal-\nities are fused together and sent to the model for decision making; (3) late fusion (also known as\n“decision fuse”): the individual decisions obtained from each modality are fused to form the final\nprediction,e.g.,majorityvoteorweightedaverage,orametaMLmodelontopofindividualdeci-\nsions.(4)hybridfusion:acombinationofearly,intermediate,andlatefusion.Withlargeamounts\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:5\nFig. 4. Theconventional taxonomycategorizes fusionmethods intothreeclasses.\nofmultimodaldataavailable,theneedformoreadvancedmethods(VShandpickedwaysoffusion)\nto fuse them has grown unprecedentedly. However, this conventional fusion taxonomy can only\nprovide basic guidance for multimodal data fusion. In order to extract the richer representation\nfrom multimodal data, the architecture of DNN becomes more and more sophisticated, which no\nlongerextractsfeaturesfromeachmodalityseparatelyandindependently.Instead,representation\nlearning, modality fusing, and decision making are interlaced in most cases. Therefore, there is\nno need to specify exactly in which part of the network the multimodal data fusion occurs. The\nmethodoffusingmultimodaldatahaschangedfromtraditionalexplicitways,e.g.,earlyfusion,in-\ntermediatefusion,andlatefusion,tomoreimplicitways.ToforcetheDNNtolearnhowtoextract\ncomplementaryandredundantinformationofmultimodaldata,researchershaveinventedvarious\nconstraintsonDNN,includingspecificallydesignednetworkarchitecturesandregularizationson\nloss functions, and so on. Therefore, the development of deep learning has significantly reshaped\nthe landscape of multimodal data fusion, revealing the inadequacies of the traditional taxonomy\nof fusion methods. The inherent complexity of deep learning architectures often interlaces repre-\nsentation learning, modality fusing, and decision-making, defying the simplistic categorizations\nof the past. Furthermore, the shift from explicit to more implicit fusion methods, exemplified by\nattentionmechanisms,haschallengedthestaticnatureoftraditionalfusionstrategies.Techniques\nsuch asgraph neural networks (GNNs)a n dgenerative neural networks (GenNNs) intro-\nduce novel ways of handling and fusing data that are not aligned with the early-to-late fusion\nframework. Additionally, the dynamic and adaptive fusion capabilities of deep models, coupled\nwiththechallengesposedbylarge-scaledata,necessitatemoresophisticatedfusionmethodsthan\nthe conventional categories can encapsulate. Recognizing these complexities and the rapid evolu-\ntion, it becomes imperative to introduce a taxonomy that delves deeper, capturing the subtleties\nof contemporaryfusionmethods.\nFormultimodaldatafusion,thereareseveralrecentsurveysavailableinthesciencecommunity.\nGaoetal.[ 46]provideareviewonmultimodalneuralnetworksandSOTAarchitectures.However,\nthe review is only focused on a narrow research area: the object recognition task for RGB–depth\nimages. Moreover, this survey is limited to the convolutional neural networks. Zhang et al. [ 235]\npresent a survey on deep multimodal fusion. However, the authors categorize the models using\ntheconventionaltaxonomy:earlyfusion,latefusion,andhybridfusion.Furthermore,thissurvey\nis focused on the image segmentation task only. Abdu et al. [ 2] provide a literature review of\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:6 F. Zhao et al.\nFig. 5. The diagramof our proposedfine-grainedtaxonomy of deepmultimodal datafusionmodels.\nmultimodal sentiment analysis using deep learning approaches. It categorizes the deep learning-\nbasedapproachesintothreeclasses:earlyfusion,latefusion,andtemporal-basedfusion.However,\nsimilartotheabovesurveys,thisreviewisnarrowlyfocusedonsentimentanalysis.Gaoetal.[ 45]\nprovideasurveyonmultimodaldatafusion.Itintroducesthebasicconceptsofdeeplearningand\nseveral architectures of deep multimodal models, including stacked autoencoder-based methods,\nrecurrent neural networks-based methods, convolutional neural network-based methods, and so\non. However, it does not include the SOTA large pre-trained models and GNNs-based methods,\ne.g., the BERT model. Meng et al. [ 121] present a review of ML for data fusion. It emphasizes\nthe traditional ML techniques instead of deep learning techniques. Also, the authors classify the\nmethodsintothreedifferentcategories:signal-levelfusion,feature-levelfusion,anddecision-level\nfusion.Thewayofcategorizingthefusionmethodsissimilartothatoftheconventionaltaxonomy:\nearly fusion, intermediate fusion, and late fusion, which is not new to the community. There are\nseveral other reviews [ 4,128,227] in the field of multimodality, most of which focus on a specific\ncombinationof modalities,e.g.,RGB-depthimages.\nTherefore,inthisarticle,weprovideacomprehensivesurveyandcategorizationofdeepmulti-\nmodaldatafusion.Thecontributionsof thisreview are three-fold:\n—We provide a novel fine-grained taxonomy of the deep multimodal data fusion models, di-\nverging from existing surveys that categorize fusion methods according to conventional\ntaxonomies such as early, intermediate, late, and hybrid fusion. In this survey, we explore\nthe latest advances and group the SOTA fusion methods into five categories: Encoder-\nDecoder Methods, Attention Mechanism Methods, GNN Methods, GenNN Methods, and\notherConstraint-basedMethods,asshownin Figure 5.\n—We provide a comprehensive review of deep multimodal data fusion consisting of various\nmodalities, including Vision+Language, Vision+Other Sensors, and so on. Compared to the\nexisting surveys [ 2,4,45,46,121,128,227,235,243] that usually focus on one single task\n(such as multimodal object recognition) with one specific combination of two modalities\n(suchasRGB+depthdata),thissurveyownsabroaderscopecoveringvariousmodalitiesand\ntheircorrespondingtasks,includingmultimodalobjectsegmentation,multimodalsentiment\nanalysis,VQA, and videocaptioning,and soon.\n—Weexplorethenewtrendsofdeepmultimodaldatafusion,andcompareandcontrastSOTA\nmodels.Someoutdatedmethods,suchasdeepbeliefnetworks,areexcludedfromthisreview.\nHowever,thelargepre-trainedmodels,whicharerisingstarsofdeeplearning,areincluded\ninthereview,e.g.,Transformer-basedpre-trainedmodels.\nThe rest of this article is organized as follows. Section 2introduces Encoder-Decoder-based\nfusion methods, in which the methods are grouped into three sub-classes. Section 3presents the\nSOTAAttentionmechanismsusedinmultimodaldatafusion.Inthissection,thelargepre-trained\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:7\nFig. 6. The general structure of Encoder-Decoder method to fuse multimodal data. The input data of each\nencoder can be the raw data of each modality or the features of each modality. The encoders can be inde-\npendentorshareweights.Thedecodercancontainupsamplingordownsamplingoperations,dependenton\nspecific tasks.\nmodelsareintroduced.InSection 4,weintroduceGNN-basedmethods.InSection 5,weintroduce\nGenNN-based methods, in which two main roles of GenNN-based methods in multimodal tasks\narepresented.Section 6presentstheotherconstraintsadoptedinSOTAdeepmultimodalmodels\nsuch as Tensor-based Fusion. In Section 7, the current notable tasks, applications, and datasets\nin multimodal data fusion will be introduced. Sections 8and9discuss the future directions of\nmultimodal datafusionand theconclusionof thissurvey.\n2 ENCODER-DECODER-BASEDFUSION\nEncoder-Decoderarchitecturehasbeensuccessfullyadoptedinsingle-modaltaskssuchasimage\nsegmentation, language translation, data reduction, and denoising. In such an architecture, the\nentire network can be divided into two major parts: the encoder part and the decoder part. The\nencoderpartusuallyworksasthehigh-levelfeatureextractor,whichprojectstheinputdataintoa\nlatentspacewithrelativelylowerdimensionscomparedtotheoriginalinputdata.Inotherwords,\ntheinputdatawillbetransformedintoitslatentrepresentationbytheencoder.Duringthisprocess,\ntheimportantsemanticinformationoftheinputdatawillbepreserved,whilethenoiseintheinput\ndatawillberemoved.Aftertheencodingprocess,thedecoderwillgeneratea“prediction”fromthe\nlatentrepresentationoftheinputdata.Forexample,inasemanticsegmentationtask,theexpected\noutput of the decoder can be a semantic segmentation map with the same resolution as the input\ndata.Inaseq-2-seqlanguagetranslationtask,theoutputcanbetheexpectedsequenceinthetarget\nlanguage. In datadenoisingtasks,mostworksusea decodertoreconstructtherawinputdata.\nOwing to the strong representation learning ability and good flexibility of the network archi-\ntecture of Encoder-Decoder models, Encoder-Decoder has been adopted in more and more deep\nmultimodal data fusion models in recent years. Based on the differences in terms of the modali-\ntiesandtasks,thearchitecturesofmultimodaldatafusionmodelsvaryfromeachotherwidely.In\nthis survey, we summarize the general idea of the Encoder-Decoder fusion methods and discard\nsome of the task-specific fusion strategies that cannot be generalized. The general structure of\nthe Encoder-Decoder fusion is shown in Figure 6. As we can see, the high-level features obtained\nfrom different individual modalities are projected into a latent space. Then, the task-specific de-\ncoderwillgeneratethepredictionfromthelearnedlatentrepresentationoftheinputmultimodal\ndata. In real scenarios, there exists plenty of variations of this structure. We categorize them into\n3 sub-classes:raw-data-levelfusion,hierarchicalfeaturefusion,and decision-levelfusion.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:8 F. Zhao et al.\nFig.7. Visualizationsof different methods.\n2.1 Raw-data-level Fusion\nIn this fusion, the raw data of each modality or the data obtained from the independent pre-\nprocessingofeachmodalitywillbeintegratedattheinputlevel.Then,theformedinputvectorof\nthe multimodalities will be sent to one encoder for extracting high-level features. The data from\nindividualmodalitiesarefusedatalowlevel(e.g.,theinputlevel),andonlyoneencoderisapplied\ntoextractthehigh-levelfeaturesofmultimodaldata.Forexample,fortheimagesegmentationtask,\nCouprieetal.[ 27]proposethefirstdeeplearning-basedmultimodalfusionmodel.Inthiswork,the\nauthorsfusethemultimodaldataviaaconcatenationoperation,inwhichtheRGBimageandthe\ndepth image are concatenated along the channel axis. Similarly, Liu et al. [ 109] concatenate RGB\nimage and depth image together. The authors utilize depth information to assist color informa-\ntion in detecting salient objects with a lower computational cost compared to the double-stream\nnetwork which consists of two separated sub-networks dealing with RGB data and depth data,\nrespectively. The key advantages of this fusion are that (1) it can maximally preserve the original\ninformation of each modality, and (2) the design of a single backbone of the networks minimizes\nthecomputationalcost.However,withtheincreasingnumberofmodalities,thedimensionofthe\nmerged input data will be extremely high. Therefore, usually, this fusion is only used for fusing\nthe data from two modalities. It is worth mentioning that the raw-data-level combination is only\nsuitable for homogeneous data. When it comes to heterogeneous data, e.g., text data+RGB image\n[207], data pre-processingis required,such as word embedding for text data. The visualization of\nraw-data-level fusion is shown in Figure 7(a). The Merge operation (“M”) in Figure 7(a) is intro-\nducedinFigure 8,whichusuallyinvolveselement-wiseAdditionorMultiplication,Concatenation,\nandCrossProduct.\n2.2 Hierarchical Feature Fusion\nOwing to the powerful ability of hierarchical representation learning of DNNs, unlike the raw-\ndata-level fusion, many works use a well-designed architecture of networks forcing the model\nto fuse multimodal hierarchical features at different levels. The motivation of this fusion method\nis that fusing and aggregating the data from different levels of abstraction better leverages the\nmulti-level features extracted from hierarchical deep networks, which can collectively improve\ntheperformanceof themodels.Thereareplentyof applicationsadoptingthisfusionmethod.For\nexample, [ 69] and [171] did not simply stack depth image (or thermal image) with RGB image to\nform a four channels input data. The authors propose a strategy that fuses the hierarchical fea-\ntures of the two modalities at different levels by using element-wise summation. In this method,\nthe RGB sub-network can be viewed as the backbone of feature extraction. The hierarchical fea-\ntures obtained from the other sub-network (e.g., thermal encoder and depth image encoder) will\nbefusedwiththeRGBfeaturesbyelement-wisesummation.UnliketheaboveworksthatuseRGB\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:9\nFig.8. TheMerge operations:element-wise Additionor Multiplication,Concatenation, andCrossProduct.\nbranch as the backbone of feature extraction, Hu et al. [ 64] build a new fusion sub-network as\nthe backbone. The RGB and depth hierarchical features with the same resolution will be fused\nwith the corresponding features of the fusion branch via an element-wise summation. In hierar-\nchical fusion-based networks, the connection of features from different levels can help the model\ncapture the cross-modality relationship. There are some other variations of this fusion method,\ne.g.,inthemedicalfield,Venugopalanetal.[ 179]proposeanaivefeaturefusionnetworkthatinte-\ngratesthedataofthreemodalitiesbyconcatenatingthelastlayersoftheencoders.Hongetal.[ 59]\npropose an encoder-decoder-based fusion model for hyperspectral and LiDAR data, in which the\nfusionhappensinthedecoderofthenetwork.Inreferringimagesegmentationtask,[ 97]and[65]\npropose fusion models in which the RGB image and the referring expression are fed into convo-\nlutional neural networks or recurrent neural networks to generate their feature representations\nindependently, and then fuse these features in the decoding stage. Similarly, in scene understand-\ning task, [ 173] and [226] hierarchically fuse the features (e.g., low level, middle level, and high\nlevel features) from different modalities to improve the model performance, while [ 170] fuses the\nhigh level features together. The key advantages of this type of fusion include (1) the flexibility\nof the fusion architecture—one can decide where the fusion happens and how many hierarchi-\ncal features are fused for specific tasks, (2) easy to be combined with attention mechanism—the\nconnectionbetweenmultimodalityhierarchicalfeaturesatthesamelevelcanbeupgradedwithat-\ntentionmechanism,whichwillbeintroducedinSection 3.Thisallowsresearcherstoleveragethe\nrelationship among different modalities to enhance the performance of the fusion model. One of\nthemajordrawbacksofthisfusionmethodisthattheindividualsub-networksfordifferentmodali-\ntiesdemandsubstantialcomputationalresourceswhenthenumberofmodalitiesisrelativelyhigh.\nTherefore, this fusion method is usually adopted for the fusion of two or three modalities. The\nvisualization of hierarchicalfeaturefusionisshownin Figure 7(b).\n2.3 Decision-level Fusion\nDifferent from the above hierarchical feature fusion strategy, which provides much flexibility on\narchitecturedesign,decision-levelfusionisrelativelystraightforward,lessflexible,buteasytoim-\nplement.Thefusionoperationinthismethodisfixedattheendofthedecoderorclassifierofindi-\nvidualsub-networks,whichmeansthatthecross-modalinformationisexchangedinthelastlayers\northepenultimatelayersofdecoders.Itprovideslimitedinterpretabilityofthemultimodalinterac-\ntions.Forclassificationtasks,thefinalfusioncanbeaccomplishedbyusingaclassicmajorityvote\northeweightslearnedfrommultilayerperceptron.Forregressiontasks,usuallyalinearregressor\nistrainedtofusethepredictionsofindividualmodalities.Forexample,Zhangetal.[ 234]integrate\nthe outputs of two individual decoders by concatenating them along the channel axis. Similarly,\nAygün et al. [ 9]addressthebraintumorsegmentationproblembyusingdecision-levelfusion.The\nadvantagesofthismethodare(1)itcanbeusedtoexploretherelativecontributionweightofeach\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:10 F. Zhaoetal.\nFig. 9. The generalidea of usingauto-encoder methodtofusethemultimodaldata.\nmodality to generating the final decision, (2) easy to tell whether the prediction result from each\nmodalityiscorrect,and(3)thenetworksareeasytodesignandimplement.However,theobvious\ndrawbacks,comparedtohierarchicalfeaturefusion,arethat(1)theperformanceofthewholenet-\nworks can be limited by one modality (e.g., the branch for one modality does not work properly\nand generates a wrong prediction severely affecting the final prediction negatively), and (2) less\nflexibility in how multimodal information can be fused. The visualization of decision-level fusion\nisshownin Figure 7(c).\nSofar,wehaveexploredthreetypicalarchitecturesofEncoder-Decoderfusion.Inrealscenarios,\ntherecanbenumerousvariantsofthem.Forexample,forthesegmentationtask,theencoderper-\nforms feature extraction with downsampling, and the decoder applies upsamplingto compensate\nforthedownsamplingeffectoftheencoder.However,forsomeoftheothertasks,e.g.,discrimina-\ntiontasks,whichdonotrequireresolutionrecovery,theupsamplingoperationwillnotberequired\ninthedecoders.Then,thedecodersub-networkscanbereplacedwithgenericconvolutionalneu-\nral networks or stacked fully connected layers to handle discrimination tasks. Another common\nvariation is reconstruction loss-based autoencoder-decoder (AE) fusion. The general architecture\nis shown in Figure 9. The final loss function of AE-based fusion consists of reconstruction losses\nand task-specific losses. In the medical field, Wang et al. [ 181] propose an AE-based Drug-Target\ninteractionpredictionmodel.Itusestwoseparateencoder-decodersub-networkstoextractthein-\ndependent latent representations for the two input modalities. Then, the learned representations\nareconcatenatedforthedownstreamtasks.Similarly,Beraetal.[ 16]proposetwoAEsub-networks\nfor representation learning. This method is straightforward. However, the reconstruction of each\nmodality only based on independent high-level features cannot explore the cross-modality infor-\nmation. Also, the simple concatenation operation provides weak constraints for exploiting the\ncross-modal relations. Differently, Hong et al. [ 59] propose an AE-based segmentation model, in\nwhichthereconstructionofeachmodalityisbasedonthecommonlatentrepresentationsofboth\nmodalities. It means the model maps the input data from two different modalities into a common\nspacetoobtainanewrepresentationoftheinputdata,andthen,reconstructseachmodalityfrom\nthelearnedrepresentations.Thisdesignforcesthemodeltoexploitthecross-modalrelationships.\nThetworeconstructionlossesworkasregularizationtermsinthefinallossfunction.Bendreetal.\n[14]proposeamultimodal variationalautoencoder (VAE[83])architecture,whichcanlearnthe\nshared latent space of image features. The model concatenates multimodal data to form a single\nembeddingbeforepassingittotheVAEforlearningthelatentspace.Khattaretal.[ 81]proposea\nVAE-based end-to-end architecture for fake news classification problem. The model concatenates\nthe textual features and visual features together to form the embedding. Then, the autoencoder\nreconstructstheword embeddingfeaturesand visualfeatures.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:11\nIn this section, we have reviewed the encoder-decoder architecture-based fusion models. As\noneofthemostfamousgenerativestructures,thehighflexibilityofneuralnetworkarchitectures\nenablesresearcherstobuildmorepowerfulnetworkstoextractfeaturesandfusethefeaturesfrom\ndifferent modalities. However, during the fusion process, the multiplication, concatenation, cross\nproduct,orelement-wise addition,is relatively naive and bruteforce.More and more researchers\nare spending efforts on how to merge features from different modalities so that the relationship\namong differentmodalities canbeeasily explored.\n3 ATTENTION-BASED FUSION\nThe concept of attention mechanism is firstly introduced by [ 10]. Back then it was only used to\nimprove the encoder-decoder-based neural machine translation system in the natural language\nprocessing (NLP) field. Since Vaswani et al. [ 177] published their groundbreaking work “Trans-\nformers”inthearticle“AttentionIsAllYouNeed”,theattentionmechanismhasbecomeoneofthe\nhottesttopicsinthedeeplearningcommunity.Attentionmechanismsenablemodelstoassigndif-\nferentweightstoeachpartoftheinputdatasothatthemodelcanextractsignificantinformation\nthat is more relevant and critical to the current task. It can help the model make more accurate\npredictionswithoutincreasingthecomputationalcostdrastically.\nLots of variations of the attention mechanism have been proposed, such as channel attention\nand spatial-attention in the computer vision (CV)fi e l d ,e . g . ,[ 233,239], and self-attention and\nmulti-headattentionintheNLPfield,e.g.,[ 177].Thesevariationsenabletheattentionmechanism\nnotonlytoworkforNLPtasksbutalsotobeusedtoaddressproblemsinotherfields.Forexample,\nscaleddot-productattentioncanbeusedinvarioustasks.Theinputconsistsofqueries,keys,and\nvalues.Itcomputesthedotproductsofthequery Qwithallkeys K,divideseachby√dk(inwhich\ndkis the dimension of keys), and applies a softmax function to obtain the weights on the values\n[177].Theequationis shownasEquation( 2):\nAttention (Q,K,V)=softmax/parenleftBigg\nQKT\n/radicalbig\ndk/parenrightBigg\nV. (2)\nRecently,theattentionmechanismhasbecomeoneofthemaintoolsformultimodaldatafusion\ntasks.Theattentionmechanism-basedmultimodal models canbecategorizedinto threeclasses:\n3.1 Intra-modalitySelf-attention\nThegeneralstructureisshowninFigure 10(a).Themotivationofthismethodistoforcethemodel\nto exploit the intra-modality relationship. The attention operation can be dot-product-based [ 85],\nor additive gate-based [ 134], and so on. This means that for a given modality, the attention oper-\nation solely considers data from that specific modality. In the context of the Transformer model\n[177], the Key (K), Query (Q), and Value (V) tensors used in attention computation are identical\nandallderivedfromthesamemodalityorsequence,asshownintheleftpartofFigure 10(c).This\nensuresthattheattentionprocesssolelyconcentratesonthedataforeachsinglemodality,allow-\ningforafocusedandundilutedanalysisofintra-modalityrelationships.Thismethodiscommonly\nusedinmultimodalitytasks.Forexample,Gaoetal.[ 47]proposetheintra-intermodalityattention\nmodule-based model to address the VQA task. The authors applied the intra-modality attention\nmechanism to enhance the feature learning ability of the sub-network of each modality. Simi-\nlarly, Malinowski et al. [ 115] propose a hard-attention-based multimodal fusion method, which\nproducesabinarymaskoverspatiallocationsdeterminingwhichfeaturesarepassedontodown-\nstream processing. Meanwhile, soft-attention mechanisms have gained widespread popularity in\nmultimodal fusion tasks, primarily due to their differentiability, which stands in stark contrast\nto hard-attention mechanisms. For the multimodal image segmentation task, Mohla et al. [ 126]\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:12 F. Zhaoetal.\nFig. 10. Illustration of different attention mechanisms and fusion architectures. (a) shows the attention\nmechanism focusing on intra-modality relationship. (b) shows the attention mechanism focusing on inter-\nmodality relationship. (c) shows Transformer-based architecture consisting of intra-modality self-attention\nandinter-modalitycross-attention.\nproficiently implemented soft-attention mechanisms, including channel- and spatial- attention\nmechanisms, on two specialized sub-networks: the Hyperspectral Imagery (HSI) sub-network\nand the LiDAR sub-network. This approach accentuates the relative importance of crucial areas\nwithineachmodality,withthechannel-attentionmechanismintheHSIsub-networkfocusingon\nthe most informative channels, and the spatial-attention mechanism in the LiDAR sub-network\nemphasizingthemost contributivespatialregions.\nThe intra-modality self-attention mechanism offers several advantages, including its flexibility,\nease of implementation, and relatively low computational cost, primarily because it circumvents\ntheintricateanalysisrequiredtodiscerndifferencesandexploitcorrelationbetweenvariousmodal-\nities. However, by focusing solely on intra-modality relationships, this approach might overlook\nvaluablecomplementarity amongvariousmodalitiesthatcan enhancethemodelperformance.\n3.2 Inter-modality Cross-attention\nThegeneralstructureisshowninFigure 10(b).Complementarytotheintra-modalityself-attention,\ntheinter-modalitycross-attentionmechanismfocusesonexploitingtherelationshipamongdiffer-\nent modalities. The attention scores are computed using multimodal data. This means that each\nattention operation considers data from multiple modalities. In the context of the Transformer\nmodel[177],theQuery(Q)tensorandtheKey(K)andValue(V)tensorsusedinattentioncompu-\ntationarederivedfromtwoormoredifferentmodalitiesorsequences,asshownintherightpartof\nFigure10(c).Sincesomemodalitystreamscancontainmoreinformationforthetaskathandthan\ntheothers,theobtainedattentionweightscanbeappliedtothemoreinformativemodalitiesonly.It\nwillproduceanattention-pooledfeatureforonemodalityconditionedonanothermodality.Forex-\nample,Zhangetal.[ 228]applythedotproductattentionmechanismtoexploretheinter-modality\nrelationshipbetweentextandimagefeatures.Mohlaetal.[ 126]proposeamultimodalmodelbased\nonspatialattentionandchannelattention.Inadditiontotheintra-modalityattentionmechanism,\ntheauthorsadoptedaninter-modalityattentionmechanismtoexploitthecross-modalityrelations\nbetweentheLiDARmodalityandtheHSImodality.ItcanbeviewedasLiDAR-guidedHSIattention\nnetworks.Similarly,Huetal.[ 65]proposeabi-directionalinter-modalcross-attentionmodule,in\nwhichtheauthorscreateavision-guidedlinguisticattentionmoduleandalanguage-guidedvisual\nattentionmoduletoexploittheinter-modalityrelationshipbetweenthevisionmodalityandthelin-\nguisticmodality.Differently,insomecombinationsofmultimodalities,itishardtotellwhetherone\nofthemisrelativelymoreinformative.Therefore,lotsofworksappliedtheattentionscoresonall\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:13\nthemodalitiesinsteadofonthemostinformativemodalityonly.Forexample,Wuetal.[ 200]pro-\nposeaco-attention-basedmultimodalfakenewsdetectionmodel.Inthemodel,beforeeachfusion\noperation, they enhance each modality with the other modality by using the co-attention mech-\nanism. The stacked multiple co-attention layers force the model to fuse the multimodal features\nandtolearninter-dependenciesamongthem.Similarly,Sunetal.[ 169]proposeaninter-modality\ncross-attentionmechanismspecificallydesignedtolearntheassociationsbetweenaudioandtext\nmodalities,calculatingdotproductsoftheQueryandKeyofaudioandtextinacrossedway.The\ncross-attentionmoduleguidesonemodalitytoattendtotheother,updatingthefeaturesinaman-\nner thatreflectsthelearnedinter-modalassociations,therebyenrichingtheunderstandingof the\nlatentrelationshipsbetweenthemodalities.Furthermore,Luetal.[ 111]developadistinctivecross-\nattention mechanism, integrating channel-attention and feature-intersection mechanisms. This\napproach facilitates dynamic information interaction among disparate modalities, allowing the\nmodeltoemphasizemorerepresentativefeatures.Later,Yoonetal.[ 215]proposedasophisticated\nmultimodal encoder that leverages cross-attention to incorporate visual and acoustic representa-\ntions. This approach delves deep into understanding the latent information interwoven between\nthemodalities,providinga holisticview and enrichedrepresentationof themultimodaldata.\nThe inter-modality cross-attention mechanism, while powerful, presents certain challenges\nin its applications. The nature of inter-modality cross-attention, which seeks to manage and\nleverage relationships between different modalities, can inherently introduce computational\nand structural complexities. This becomes particularly pronounced as the number of modalities\nincreases, demanding more computational resources and intricate management. Meanwhile, the\nefficacy of the inter-modality cross-attention mechanism is deeply intertwined with the quality\nand relevance of the modalities it deals with. Consequently, modalities of poor quality or those\nthataremisalignedcansignificantlyimpedetheoptimalperformanceoftheattentionmechanism,\nleading tosubparresults.\nThe intra-modality self-attention and inter-modality cross-attention mentioned above can be\neasily adopted in DNN with great flexibility. They can work together or be mixed with other\ntypes of multimodal fusion methods. For example, Gao et al. [ 47] propose an intra-inter modal-\nityattentionmodule-basedmodeltoaddresstheVQAtask,inwhichtheintra-attentionenhances\nindividualmodalityfeatures,andtheinter-attentioncapturesinteractionsamongvariousmodali-\nties.Otherthanthat,therearemanyinnovativeworksofself-attentiontoexplorecross-modality\nrelationsandreducecomputationalcost.Byconcatenatingrepresentationsofdifferentmodalities\nintoalatentspaceandapplyingself-attentiontothesenewrepresentations,modelscaneffectively\nexploitcross-modalityrelations.Forinstance,theattentionmechanismproposedbyYeetal.[ 211]\nadaptively shifts focus to salient words in the query expression and significant portions of the\ninputimage.\nHowever, traditional attention mechanisms, despite their efficacy in discerning local patterns\nandrelationships,encounterlimitationsinrecognizinglong-rangedependencieswithindata.This\nisattributedtothelocalizednatureoftheirreceptivefields,whichpotentiallyhampersthemodel’s\ncapacitytoassimilateinformationfromdistantsegmentsoftheinput.Tocounteractthislimitation,\nthe concept of non-local attention [ 186] has been introduced. This innovative approach is struc-\nturedtocontemplaterelationshipsthroughouttheentireinputspace,therebyenablingmodelsto\napprehendandutilizelong-rangedependencieseffectively.Anotableimplementationofnon-local\nattentionistheworkbyYuanetal.[ 221],wherenon-localattention-basednetworksareleveraged\nforthefusionofhomogeneousmultimodalimagedata,suchastheintegrationofMRIandPETor\ntheamalgamationofinfraredandvisibleimages.Unlikelocalizedattentionmechanisms,non-local\nattention transcends the constraints of proximity and provides a holistic perspective of the input\nspace,making ita valuableassetintheadvancement of multimodaldatafusion techniques.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:14 F. Zhaoetal.\nTable 1. TheList of theLargePre-trainedModels\nModel Name and Ref. YearTransformerArcht. Modality Tasks\nLXMERT[ 172] 2019Multi-Transformers I m a g e+T e x t VQA\nUniter[24] 2020 Uni-Transformer I m a g e+T e x t VQA, VCR, ITR\nUniT [62] 2021Multi-Transformers I m a g e+T e x t OD,VQA\nClipBERT[ 90] 2021 Uni-Transformer Video+ Text VR\nVLM [201] 2021Multi-Transformers Video+ Text VQA, VR, VC\nVATT [3] 2021 Uni-Transformer Video +Text + Audio AC, AEC\nDeCEMBERT[ 174]2021 Uni-Transformer Video+ Text VR,VC\ninterBERT[ 101] 2020 Uni-Transformer I m a g e+T e x t ITM\nPixel-BERT [ 68] 2020 Uni-Transformer I m a g e+T e x t VQA, ITR\nB2T2[5] 2019 Uni-Transformer I m a g e+T e x t VCR\nVLC-BERT [ 147] 2023Uni-Transformers Video+ Text VQA\nUnicoder-VL[ 92] 2020Multi-Transformers I m a g e+T e x t AC, VC\nHERO [94] 2020Uni-Transformers Video+ Text VQA, VR, VC\nActBERT[ 244] 2020Multi-Transformers Video+ Text VR,VX,AS\nUniVL[113] 2020Multi-Transformers Video+ Text VR,VC,SA\nImageBERT [ 139] 2020 Uni-Transformer I m a g e+T e x t ITM\nVisualBERT[ 96] 2019 Uni-Transformer I m a g e+T e x t VQA, VCR\nViLBERT[ 112] 2019Multi-Transformers I m a g e+T e x t VQA\nVideoBERT[ 168] 2019 Uni-Transformer Video+ Text AC\nCBT [167] 2019Multi-Transformers Video+ Text AC, VC, AS\nZorro[148] 2023Uni-Transformers Video+Audio AC\nX-llm [22] 2023Multi-Transformers I m a g e+T e x t VQA,VC\nMIST[43] 2023Uni-Transformers Video+ Text VQA\nNExT-GPT[ 199] 2023Uni-Transformers Video+ Text VQA,VC, VCR\nSA: sentimentanalysis,ITR:image-text retrieval,VCR: visualcommonsensereasoning,\nAS:actionsegmentation,VR:videoretrieval,VQA: visualquestionanswering,VC:videocaptioning,\nOD: objectdetection,ITM:image textmatching, AC: actionclassification,AEC: audio eventclassification\n3.3 Transformer-based Methods\nBuilding on the idea of non-local attention, the Transformer architecture [ 177] has emerged\nas a groundbreaking solution. At the heart of the Transformer is the self-attention mechanism,\nwhich allows each output element to consider all input sequences (or image patch embeddings)\nsimultaneously, effectively capturing both local and long-range dependencies without being\nbound by the constraints of traditional convolutional or recurrent layers. This global perspective,\ncombined with the architecture scalability, has made Transformers particularly suited for tasks\nthat benefit from understanding intricate relationships across data. Transformer-based large\npre-trainedmodelshavebeendominantonlotsofmultimodaldatafusiontasks,suchasinterBERT\n[101] and videoBERT [ 168]. The state-of-the-art large pre-trained models are summarized and\nsho wninT able 1.\nThe Transformer model is first introduced by [ 177]. It is a combination of Encoder-Decoder\narchitecture and Attention mechanism [ 70], as shown in Figure 10(c). In the Encoder, there are\nstackedself-attention blocks,in which K,Q,Vof thescaleddot productattentionmechanismare\nfromthesametensor,toexploretheintra-modalityrelationshipoftheinput.IntheDecoder,there\nare stacked self-attention blocks and a cross-attention block, in which the K,Q,Vare from differ-\nent modalities, e.g., Qis from the second modality , while K,Vare from the first modality. These\nself-attention and cross-attention blocks help the model capture the intra- and inter-relationship\nwithin and among multimodalies efficiently. Currently, the Transformer-based large pre-trained\nmodels can be divided into two categories: (1) uni-Transformer architecture: in this architecture,\nthe input data from different modalities will be jointly processed by a single encoder or several\nstacked encoders, such as VideoBERT[ 168], HERO [ 94], NExT-GPT [ 199], ClipBERT [ 90], and\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:15\nDeCEMBERT [ 174]; (2) multi-Transformers architecture: in this architecture, the input data from\ndifferent modalities will be encoded separately by modality-specific Transformers before being\njointlymodelled,suchasX-llm[ 22],UniVL[ 113],andActBERT[ 244].Thelargepre-trainedmod-\nelsareabletolearnthecomprehensiverepresentationsformultimodalitiesandobtaincompetitive\nperformanceondownstreamtasks.However,currently,mostlargepre-trainedmodelsarefocused\non the vision-language field. For the other types of modality, the resource of large pre-trained\nmodels is still limited. Therefore, several works build their own transformer-based model for the\nspecific tasks that do not have any pre-trained transformer-based models available. For example,\nthe work in [ 203] leverages the correlation between the MRI and the acoustical signals for vocal\ntractdeformationtaskbyusingacross-modaltransformer-basedarchitecture.Similarily,Hsuetal.\n[60]proposeamultimodaltransformertocapturethelong-rangedependencyamongmultimodal\ndata,e.g.,text,image, numericaldata,and categoricaldata,by theselfattentionmechanism.\n4 GRAPH NEURALNETWORK-BASED FUSION\nSo far, we have reviewed the Encoder-Decoder-based fusion and Attention-based fusion. The\nmodels of these methods have achieved great success in capturing hidden patterns from the\ndata within Euclidean space. However, it is difficult for them to address the data that is gen-\nerated from non-Euclidean domains, represented as graphs with complex relationships and in-\nterdependencies between objects [ 241]. Recently, there are an increasing number of applica-\ntions based on GNNs addressing the multimodal problems related to graph data, e.g., VQA task\n[77,95,100,129,190,214,218], image captioning task [ 42,182,182,208–210], cross-modal re-\ntrievaltask[ 30,204,217],RGB-depthsceneclassificationtask[ 222],multimodalrecommendation\ntask [193,194], neuroimaging-based disease classification task [ 158], 3D object localization with\nnatural language descriptions [ 21], and object segmentation in the 3D scenes according to the\nquery sentences [ 66]. GNN has emerged as a potent tool to process and integrate data structured\nas graphs, especially when modalities are inherently relational or interconnected. Among GNN,\nGraphConvolutionalNetworks (GCNs)standout,leveragingconvolutionallayersadaptedfor\ngraphdatatoaggregateinformationfromneighboringnodes,therebyfacilitatingthefusionofspa-\ntiallylocalizedfeaturesacrossmodalities.Anothernotablesubtype, GraphAttentionNetworks\n(GATs),introducesattentionmechanismstothegraphstructure.Bydynamicallyweighingtheim-\nportanceofneighboringnodes,GAToffersarefinedfocusonrelevantpartsofthegraph,enhanc-\ningthefusionprocessbycapturingintricatepatternsandrelationshipsacrossdiversedatasources.\nThe general strategy of applying GNN to multimodal data fusion can be categorized into two\ndifferent classes:\n4.1 Representation Learning for Individual Modalities\nThe visualization is shown in Figure 11(a). In this strategy, the GNN is used to extract the new\nrepresentation from the graph data only, which means the sub-branches consisting of non-graph\nstructureddatawillnotuseGNNforfeatureextraction.Then,thelearnedrepresentationsfromdif-\nferentmodalitieswillbeintegrated.Forexample,Lotfietal.[ 110]proposeamultimodaldatafusion\nmethod for detecting rumor conversations, in which the authors model the user graph and reply\ntree by using GCN independently. Then, the features obtained from the user graph and the reply\ntreeareconcatenatedandsenttoafullyconnectedlayertodetecttherumorconversation.Unlike\nthe above work that fuses the GCN embeddings from different modalities by using a naive con-\ncatenation operation only, Qian et al. [ 142] adopt an inter-modal attention mechanism to exploit\nthe relationships between visual features and textual features. In this work, the authors use GCN\nand VGG-19 [ 162] to obtain the textual representation and the visual representation of each post,\nrespectively.Duringthefeatureextractionprocess,theauthorsusetext-guidedvisionattentionto\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:16 F. Zhaoetal.\nFig. 11. Illustration of GNN-based multimodal data fusion and integration. (a) shows a general schema of\nGNN-based multimodal data fusion. (b) shows how the integration of multimodal data occurs in the graph\nconstructionprocess.\nenforce an interaction between the textual feature and the visual feature. It can determine which\nvisualfeaturedeservesmoreattention.BeyondmethodsbasedonGCN,therearemanyothertypes\nofGNN-basedmethods.Forinstance,Yangetal.[ 208]introduceaGNNmodelutilizingmulti-head\nattentiontoembedscenegraphseffectively,thustoenhancetheperformanceofimagecaptioning\ntasks. This method focuses on capturing intricate relationships within scene graphs to generate\nmoreaccurateandcontextuallyrelevantcaptionsforimages.Similarly,theworksofTaoetal.[ 175]\nand Jia et al. [ 75] introduce multimodal GATs tailored for personalized recommendations. Those\nworks conduct information propagation within individual single-modal graphs and use attention\nmechanisms to identify varying importance scores of different modalities to user preference. The\nGNN not only works well for language data and vision data but also has achieved success on\nbiomedicaldataandchemicaldata.Forinstance,Wangetal.[ 185]proposemulti-omicsGCNsfor\na biomedical classification task, in which the model utilizes GCN to extract independent features\nfromthreemodalities,i.e.,mRNAexpressiondata,DNAmethylationdata,andmicroRNAexpres-\nsiondata,separately.Then,theycreateacorrelationmatrixbasedonthesefeaturestoexploitthe\nlatentcross-modalitycorrelationsthathelpto improve thelearningperformance.\n4.2 Representation Learning fortheFused Data\nThevisualizationisshowninFigure 11(b).Thekeyoperationofthisfusionstrategyisgraphcon-\nstruction. In general, unlike the previous strategy which can have multiple sub-networks or sub-\nmodality branches, this strategy fuses the multimodal data in the graph construction before the\nrepresentationlearningprocess.Forexample,Huetal.[ 61]proposeamultimodalfusionmodelvia\ndeep graph convolution network for emotion recognition in conversation. The key contribution\nofthisarticleisthatitcreatesacousticnodes,textualnodes,andvisualnodesforutterances.Then,\nanytwonodesinthesamemodalityareconnectedinthegraph.Theedgebetweenthemiscalled\ntheintra-modaledge.Furthermore,eachnodeisconnectedwiththenodeswhichcorrespondtothe\nsame utterance but from different modalities. The edges connecting them are called inter-modal\nedges. This operation forces the model to exploit the intra-modality and inter-modality relation-\nships.Then,thestackedGCNisadoptedtoyieldthehigh-levelnoderepresentations.Finally,these\nnewrepresentationswillbeconcatenatedandsentthroughafullyconnectedlayertoformthepre-\ndictions.Similarly,Wangetal.[ 188]proposeaknowledge-drivenmultimodalGCNtodetectfake\nnews. Three modalities are involved in this application: text, image, and knowledge concept. The\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:17\nwords in the text are taken as the graph nodes and the relationship between words is taken as\nedges. Unlike the work in [ 142] that uses VGG-19 to extract high-level visual representation and\nconcatenateitwiththetextualrepresentation,thisworkutilizesYOLOv3[ 149]pre-trainedmodel\nto detect semantic objects in images. Then, it treats the textual labels of detected objects in im-\nages as words that occur in the text content. Therefore, these textual labels of images are used\nin the graph construction. This operation forces the interaction between different modalities to\nhappen. Then, GCN is applied to yield embedding vectors for these nodes based on the proper-\nties of their neighborhoods. In addition to GCN-based methods, a variety of approaches employ\nGAT to address diverse challenges. For instance, Jiang et al. [ 93] introduce a GAT-based network\ndesignedtoperformcross-modalfeaturecomplementationandmultimodalemotionclassification\ntasks.Theauthorsutilizethreeuni-modalencoderstoencodetheuni-modalfeaturesinitially.Sub-\nsequently, a GAT-based cross-modal feature complementation module is deployed to gather both\nlong-distance intra-modal contextual information and inter-modal interactive information. This\nmethod excels in preserving the consistency and diversity of multimodal features, pinpointing\nessential intra-modal contextual information and inter-modal interaction information, and mit-\nigating the heterogeneity gap prevalent in multimodal data. Similarly, Ding et al. [ 33]p r o p o s e\na GAT-based fusion model. This model integrates a multimodal GAT with a temporal convolu-\ntionnetworktodiscernthespatial-temporalcorrelationsinherentinmultimodaltimeseries.The\nauthors leverage various attention mechanisms, including self-attention and cross-attention, to\nexplicitlymodelthecorrelationsbetweendifferentmodalities,providingadeepunderstandingof\ntheinteractionsand relationshipswithinmultimodaldata.\nThe advantages of GNN-based fusion models, compared to other fusion methods, include (1)\ntheir ability to directly process graph-structured data by deep learning technologies without re-\nquiringtheprojectionofdataintoEuclideanspace,and(2)theirintuitivewayofexploitingrelation-\nshipsamongnodesinthegraph-structureddataandcanbeextendedtoexploittheintra-modality\nand inter-modality relationships in multimodal problems. However, the drawback of GNN-based\nfusion models is that the graph construction process is usually highly dependent on prior knowl-\nedgeofthecharacteristicsofthespecificinputdataandtask.Itistime-andspace-consumingand\nnoteasytobegeneralized.Sofar,wehavereviewedtheEncoder-Decoder-basedfusion,Attention-\nbased fusion, and GNN-based fusion. All of them can leverage the relationship among different\nmodalities to improve the performance of multimodal networks. However, such fusion methods\nhave difficultydealingwithmissingdataissues.\n5 GENERATIVENEURAL NETWORK-BASEDFUSION\nGenNNserveasafoundationalpillarinthedomainofdeeplearning,particularlyfortaskscentered\narounddatageneration,reconstruction,andmodeling.Thesenetworksaredesignedtocaptureand\nreplicatetheunderlyingdistributionsofdata,makingtheminvaluableforamyriadofapplications,\nfrom image synthesis to time-series forecasting. GenNN-based networks typically encompass a\nrangeofarchitectures,includingbutnotlimitedto, GenerativeAdversarialNetworks (GANs),\nVAEs, Flow-based and Diffusion-based models. The primary objective of GenNN is to generate\ndata that closely mirrors real-world distributions, either by directly modeling these distributions\nor by learning to transform simpler distributions into more complex ones. Their versatility and\ncapabilitytogeneratehigh-qualitydatahaveledtotheirwidespreadadoptioninbothsingle-modal\nand multimodal tasks, addressing challenges such as data imputation, augmentation, and fusion.\nGiventheirgenerativeprowess,generativemodelshavebecomeinstrumentalinscenarioswhere\nrealdataisscarce,noisy,orincomplete,providingarobustmechanismtosupplementandenhance\nexistingdatasets.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:18 F. Zhaoetal.\nFig.12. Thegeneral architectureof GenNN-based methods.\nFor multimodal data fusion tasks, the data collection process is relatively more sophisticated,\ncomparedtosingle-modaltasks.Missingdataisoneofthecommonissuesofmultimodaldatasets.\nHere, we define the missing data issue as the issue caused by the multimodal sample, in which\nat least one modality is missing. In most works related to multimodality, the researchers simply\nassume that the dataset is without any missing data, or apply data cleaning for removing all the\nsamples containing missing data. The consequence is that most of those works only work well\ntheoreticallyand cannot be adapted to real scenarios.To address the missing data issues of multi-\nmodaldatafusion,GenNNshaveemergedaspowerfultools,particularlyfortasksthatinvolvedata\ngeneration,reconstruction,andfusion.Inthecontextofmultimodaldatafusion,GenNN-basednet-\nworks offer a robust framework to address challenges such as missing data and the synthesis of\nnewdata modalities.\nThe intuition behind them is that the model can synthesize the missing modality based on the\nother modalities. The general idea is shown in Figure 12. For example, Gao et al. [ 48]p r o p o s ea\nmodel TPA-GAN, which is used to synthesize the positron emission tomography (PET)d a t a\nfromMRIdata.ThegeneratoroftheTPA-GANconsistsofthetypicalencoder-decoderarchitecture,\npyramid structure, and an attention mechanism. Once the synthesized PET data is generated, a\ndiscriminatorwilldiscriminatewhetherthePETdatais“Real”or“Fake”.TheauthorsusetheTPA-\nGAN model to impute the missing PET data of samples. Then, they propose a PT-DCN model,\nan encoder-decoder-based model, to fuse the MRI and PET data and form the final classification\nofAlzheimer’s disease (AD). In this work, the authors create two models separately: (1) TPA-\nGAN model to impute the missing PET images, and (2) PT-DCN model to fuse the multimodal\ndata and complete the classification task with the imputed multimodal images. Unlike the above\nwork imputing the missing data separately from the fusion process, Wang et al. [ 183]p r o p o s ea\ngenerativemulti-viewactionrecognitionmodel.Withinthefusionmodel,twogenerators G1and\nG2 generate representations conditionally based on the other subspace. This approach enhances\nthemodelrobustnessbyemployingadversarialtrainingandnaturallyhandlestheincompleteview\ncasebyimputingthemissingdata.\nExploringalongadifferentdirection,therearealotofmultimodalapplicationsusingGenNNas\naregularizertoleveragethesemanticcorrelationsamongmodalities.Forexample,SahuandVech-\ntomova [153] propose a novel GAN-based multimodal data fusion model, in which the authors\nleverageacomplementingmodalitytoregularizethelearnedlatentspaceofmultimodalities.Peng\nandQi[137]proposeaGAN-basedcross-modalrepresentationlearningmodel.Intheirmethod,a\ncross-modalGANarchitectureisproposedtomodelthejointdistributionoverthedataofdifferent\nmodalities. The authors proposed four discriminators: two intra-modality discriminators to keep\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:19\nthe semantic consistency within each modality, and two inter-modality discriminators to exploit\nthe cross-modal correlation. Similar to the above works, Wang et al. [ 189]p r o p o s eaV A E - b a s e d\nadversarial multimodal framework that reduces the distance difference between unimodal repre-\nsentations and transfers all unimodal representations to a joint embedding space, highlighting\nimportantsentimentalrepresentationsover time and modality.\nFurthermore, the capabilities of GenNN in synthesizing and modifying one modality based on\nother modalities have spurred the development of a myriad of tasks and architectures, such as\nDiffusion-based models, VAE-based models, and Flow-based models. These models are particu-\nlarly prominent in tasks such as text-conditional image generation and image style transfer. For\ninstance, Fan et al. [ 36] introduce a Flow-based model composed of a series of fully invertible\nbasic blocks, enabling lossless image reconstruction based on both text and image inputs. This\nmodelexemplifiestheinnovativeintegrationofdifferentmodalitiestoachievecoherentandhigh-\nfidelityreconstructions.Similarly,theworksby[ 34]and[145]leveragediscreteVAEstocompress\neach RGB image into an n × n grid of image tokens. These tokens, along with text tokens, are\nsubsequently fused in downstream networks, such as Transformers, illustrating the versatility of\nVAEinhandlingandintegratingdiversemodalities.FollowingthepioneeringworksofSongetal.\n[165]andHoetal.[ 58],whoproposeddiffusionprobabilisticmodelstosynthesizerealisticimages\nfromtextualprompts,asurgeofDiffusion-basedmethodshasemergedintherealmofmultimodal\nfusion.ExamplesincludetheDiffusion-basedtext-to-speechmodelby[ 73]andvariousDiffusion-\nbased image generation models such as DALL-E2 [ 144], Imagen [ 152], and Stable diffusion [ 151].\nThese models underscore the growing significance and adaptability of Diffusion-based methods\nin synthesizing and fusing multimodal information. These advancements illustrate the expand-\ning horizons of GenNN in multimodal fusion, showcasing their potential in synthesizing diverse\nmodalities andfosteringinnovationsin multimodalinteractionsand representations.\nIn summary, GenNN-based models can be used to address the missing data issues for multi-\nmodal tasks and can also be treated as a regularizer to leverage the semantic correlations among\nmultimodalities.However,whenitcomestoexploitingtheintra-andinter-modalityrelationships\namong multiple modalities to improve the performance of the models, the architecture flexibility\nof the GenNN-based networks is relatively low, and a lot of training skills are required. In this\nregard,theAttentionMechanismattractsmoreattentioninthescientificcommunity.Becausethe\nAttention Mechanism has a strong ability to reveal intra- and inter-relationship among different\nmodalities,it hasbeenwidelyusedin multimodal data fusion.\n6 OTHER CONSTRAINT-BASED METHODS\nMost of the fusion strategies that we reviewed above are based on joint representations, which\nmeansthattheinputmultimodaldatawillbemappedintoacommonlatentspace.Themodelwill\nlearnajointrepresentationoftheinputdata.However,thereisanotherlineofmethodnamedcoor-\ndinatedrepresentation-basedframework,whichlearnsseparatedbutcoordinatedrepresentations\nof eachmodality undercertainconstraints.\nAsFigure 13(a)shows,coordinatedrepresentationarchitecturehandlesindividualmodalitysep-\narately,butenforcescertainsimilarityconstraintsonthemtobringthemintoacoordinatedspace\n[15]. The learned representations of each modality can be compared against each other by using\ncanonical-correlation analysis (CCA) constraint, cosine distance constraint, L2 distance con-\nstraint,orotherconstraints[ 26,57].Thesesimilarityconstraintswillserveasaregularizationterm\nin thelossfunction.\nOther than the regularization-based methods, Zadeh et al. [ 224] propose the first tensor-based\nfusion network. It mainly considers both the inter- and intra-modality relationships. As shown\ninFigure 13(b),themethodexpandseachmodalityby1dimension,then,calculatestheCartesian\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:20 F. Zhaoetal.\nFig. 13. (a) shows the visualization of coordinated representation framework with certain constraints to\nkeep the learned representations semantically consistent. (b) shows the tensor fusion mechanism focusing\noninter andintra-modalityrelationship.\nproductofdifferentmodalities.Takingtwomodalitiesasanexample,theauthorsperformtheouter\nproduct (tensor product) after obtaining the features. It can be seen that after the expansion, the\nfeature correlation between the two modalities is obtained, and the information of each specific\nmodality is preserved. For three modalities, the method not only obtains feature correlations for\ntwo-modalandthree-modalcombinations,butalsopreservesthefeaturesofeachspecificmodality.\nHowever, with the increase in the number of multimodalities, the feature dimension will explode\nquickly. Moreover, when the number of parameters is too large, it easily increases the risk of\noverfitting. Furthermore, there are variations of this work, e.g., [ 108]. In this work, the authors\nproposealow-rankmultimodalfusionmethod,whichperformsmultimodalfusionusinglow-rank\ntensorsto improveefficiency.\nDifferently, Wang et al. [ 187] propose a channel-exchanging-network, which dynamically\nexchanges channels in different modal sub-networks. To be specific, the channel exchanging\nprocess is self-guided by individual channel importance measured by the magnitude of batch-\nnormalizationscalingfactorduringtraining.Thismethodisparameter-free.However,thismodel\nonlyworksonhomogeneousdata.Theperformanceofheterogeneousmultimodaldataislimited.\nOne of the major drawbacks of these coordinated representation-based fusions is that they are\nsuitableforscenarioswheretherearetwo/threeinputmodalities.Whenthenumberofmodalities\nislargerthanthree,thenetworkarchitecturecanbetoocomplicated,andtheperformancecannot\nbeguaranteed.\nSo far, we have reviewed the Encoder-Decoder-based methods, Attention mechanism-based\nmethods,GNN-basedmethods,GenNN-basedmethods,andotherConstraint-basedmethods.Cur-\nrently, most SOTA algorithms of multimodal data fusion are designed for a combination of two\nspecific modalities only. Therefore, we make a comparison among them according to how well\nthey can be generalized for a higher number of modalities (the number of modalities is bigger\nthantwo.)(1)Encoder-Decoder-basedmethods.Thearchitecturesofnetworksarerelativelyflexi-\nble.Itis easyto integratea new modality intoEncoder-Decoder-basedmodels,e.g.,adding anew\nsub-encoder branch for a new modality [ 23]. Moreover, the weights of all the sub-encoders can\nbe shared. When the number of sub-encoders increases, the computational cost will not increase\ndrastically [ 87]. (2) GenNN-based methods encompass a variety of architectures, including but\nnot limited to, GAN, VAE, Diffusion, and Flow-based models. As the diversity and the number\nof input modalities increase, the architecture of GenNN-based multimodal networks tends to be-\ncome increasingly intricate. This complexity arises due to the necessity to incorporate distinct\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:21\nTable 2. A Comparisonamong theFive Categories of FusionMethods\nTable 3. A Quantitative ComparisonamongtheFusionMethods\nTasks DatasetEncoder-Decoder Attention Mechanism Graph Networks Generative Networks\nMetric:Min[method] −Max[method] Metric:Min [method] −Max[method] Metric:Min[method] −Max[method] Metric:Min[method] −Max[method]\nReferringImage Segmentation ReferIt[80] IoU:52.81[ 119]−63.63[98] IoU:63.80[ 211]−72.97[104] IoU:65.53[ 67] IoU:60.31[ 143]\nVideoCaptioning YouCookII[ 242]METEOR:19.91[ 117] METEOR:18.23[ 138]−27.09[155] METEOR:21.77[ 74]−22.1[54] METEOR:19.77[ 225]\nBLEU-4:11.43[ 117] BLEU-4:9.82[ 138]−21.88[155] BLEU-4:11.74[ 74]−14.0[54] BLEU-4:12.14[ 225]\nVisualQuestionAnswering VQA 2.0 [ 53] ACC: 68.14[ 125] ACC: 67.34[ 156]−71.94[157] ACC: 65.89[ 132]−71.29[205] ACC: 57.87 −[25]\nObjectRecognition SUNRGB-D[ 164]ACC: 54.60[ 240] ACC: 60.8[ 236]−61.4[238] ACC: 57.0[ 141]−64.4[206] −\nmIoU:42.8[ 240]−50.7[64] mIoU:47.8[ 236]−51.8[238] mIoU:45.9[ 141]−51.8[206] −\nFaceAntiProofing CASIA-SURF[ 230] ACER(%):3.8[ 232] ACER(%):0.74[ 32]−0.20[180] − ACER(%):2.4[ 103]\nPersonRe-Identification RGBD-ID [ 13]mAP(%):14.42[ 212]−15.95[196] mAP(%)::27.91[ 76]−40.52[140] mAP(%):53.02[213] mAP(%): 29.2[ 191]−31.49[29]\nBLEU-4 [136],METEOR[ 12], CIDEr[ 178], Accuracy(ACC), Intersectionover Union(IoU),Average ClassificationErrorRate(ACER), mean Average Precision(mAP)\ngenerative structures or modules for each new modality, each requiring specialized handling and\nprocessing.Forinstance,inGAN-basedmodels,eachadditionalmodalitytypicallynecessitatesthe\nintegration of a new generator and a new discriminator into the networks. The inherent variabil-\nity in the data distribution and representation across different modalities necessitates meticulous\nfine-tuning and optimization to ensure coherent and meaningful fusion, making the generaliza-\ntion to multiple modalities non-trivial. (3) Attention mechanism-based methods. The Attention\nmechanism is widely used for exploring the inter- and intra-modality relationships between two\nmodalities. When the number of input modalities increases, the attention mechanism can still\nworkwellinexploringthecorrelationwithineachmodalityoracrossmodalitieswithoutincreas-\ning the computational cost drastically. (4) GNN-based methods. The graph construction process\nis the most time-consuming part of this method. When the number of input modalities increases,\nin general, a more sophisticated graph construction process is needed. However, GNN network\nstructureforextractingnewrepresentationsofnodeswillnotbechangedinmostcases.(5)Other\nConstraint-based methods. Most of them are based on coordinated representation learning. The\nkey idea of them is using sub-networks to learn separated but coordinated representations for\neach modality under certain constraints. And the weights of each sub-branch cannot be shared\nwith each other, i.e., when the number of modalities increases, the number of sub-branches will\nbeincreasedcorrespondingly.Thecomputationalcostwillalsoincreasedrastically.Therefore,the\nother Constraint-based methods are usually applied to problems containing only two modalities.\nThe comparison is summarized in Table 2. Unlike GenNN and other Constraint-based methods\nwhich can work well only on few (e.g., 2) modalities, the Encoder-Decoder-based methods and\nthe Attention mechanism-based methods can be easily generalized to three or more modalities.\nFurthermore,thesetwo typesof methodcan worktogetherverywell, e.g.,[ 19]and [198].\nInordertohaveamoregranularcomparisontodiscernthenuanceddifferencesintheirperfor-\nmancesacrossdiversetasks,wehavecompiledTable 3,providingadetailedquantitativecompari-\nsonamongthefusionmethodsacrossdiversetasks,withthebestperformershighlightedforeach\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:22 F. Zhaoetal.\nTable 4. PublicDatasets forCommon MultimodalData FusionTasks\nTasks Data Type Dataset\nReferringImage Segmentation Image +TextReferIt[80],Google-Ref [ 118], UNC[219], CLEVR-Ref+[ 106],\nVGPhraseCut[ 197], ScanRefer[ 21],ClevrTex [ 79]\nVideoCaptioning/Retrieval Video+ TextHowto100M[ 122],Alivol-10M [ 89],YouCookII[ 242],\nCharades[ 160],TGIF [99],MSR-VTT[ 202], Didemo [ 6]\nVisualQuestionAnswering Vision +Textvideo-text: TVQA[ 91], VQA1.0[ 7], VQA2.0[ 53],\nimage-text: DAQUAR[ 116],COCO-QA [ 150],FM-IQA[ 44],\nVisualGenome [ 86], CLEVR[ 78],FVQA[184]\nObjectRecognition RGB+ DepthSUNRGB-D [ 164],NYUDepthV1and V2[ 130,161],\nRobotPKU[ 105], B3DO[ 71],BIWI[127],\nObjectDetection/Semantic Segmentation RGB+LiDARKITTI[49], Urban[ 17],KAIST [ 72],\nRobortCar[ 114], US3D[40] College [ 146],H3D [84]\nHuman ActionRecognition RGB + InertialSensorsMHAD [133],C-MHAD [ 192],\nUTD-MHAD[ 20],CAS-MHAD [ 55],NCTU-MFD[ 166]\nFaceAnti Proofing RGB +ThermalCASIA-SURF[ 230],CASIA-SURF Cefa[ 102]Speakingfaces[ 1],\nThermalFace[ 8],TTVF[ 63], UL-FMTV[ 52],\nPersonRe-Identification RGB+ThermalRegDB [131], SYSU-MM01[ 196],BIWIRGBD-ID[ 127], RGBD-ID[ 13],\nSUCVLRGBD-ID[ 120], RobotPKU[ 105], KinectREID[ 135]\ntask. Firstly, models utilizing a foundational encoder-decoder architecture, even in the absence\nofsupplementarymethodssuchasattentionmechanisms,havedemonstratedreasonableefficacy\nacrossaspectrumoftasks.Thisunderscorestheinherentrobustnessandversatilityoftheencoder-\ndecoder paradigm in multimodal fusion tasks, highlighting its capability to integrate seamlessly\na variety of modalities. Secondly, attention mechanisms have emerged as a predominant and ver-\nsatile strategy, reflected by their widespread adoption and adaptability across a myriad of tasks.\nMost models leveraging attention mechanisms have manifested superior performance in various\ndomains,underscoringtheefficacyofattentionmechanismsindiscerningandemphasizingsalient\nfeaturesandrelationshipswithinthedata.Thirdly,GNN-basedmethods,whileexhibitingsubstan-\ntialpotentialandoutperformingattention-basedmodelsinspecifictaskssuchasobjectrecognition\nandpersonre-identification,areinherentlyconstrainedbythesuitabilityofthetasktographrep-\nresentations. Their performance accentuates their ability to encapsulate complex inter-relational\nstructures within the data, but their applicability is not universal and is contingent on the nature\nofthetask.Lastly,GenNN-basedmethods,similartographnetworks,excelinparticularcontexts\nbut are not universally applicable. Their performance is contingent on the alignment of the task\nwith their architectural strengths, indicating a need for task-specific considerations in selecting\nanappropriatemodel.\n7 APPLICATIONS ANDDATASET\nCurrently,thereareplentyofapplicationsrelatedtodeepmultimodaldatafusioninthescientific\ncommunityandindustry.Withtheincreaseinthevarietyofmodalities,thetypesofdownstream\ntasksofmultimodaldatafusionareincreasing.Here,wechoosesomeofthetrendingapplications\nrelated to multimodal data fusion and categorize them into three classes: Vision and Language,\nVisionandSensors,andOthers.ThedatasetsofthesetasksaresummarizedandshowninTable 4.\n7.1 Vision and Language\nIn the last decade, with the great success of deep learning in non-interdisciplinary research, e.g.,\nCVandNLP,moreandmoreattentionisgearedtowardsaddressinginterdisciplinaryproblemsby\nusing deep learning. In the vision and language fusion, many classical tasks have been proposed:\n(1) VQA, which is proposed by [ 7]. The objective of this task is to generate a textual answer for\na given image/video and its correspondingtextual question. In this task, the multimodal model is\nsupposed to be able to integrate the semantic information of the visual data and the textual data.\nTheexpectedanswer(groundtruth)canbewords,binaryvalues,phrases,orthenumberofobjects\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:23\ninthevisualdata,andsoon.(2)Image/videocaptioning.Theobjectiveofthistaskistogeneratea\ntextualdescriptionforagivenimage/video.Thedescriptionissupposedtosummarizethecontent\nof visual data. (3) Referring image/video segmentation. The objective of this task is to generate\nthesegmentationofacertainentityreferredtobyanaturallinguisticexpression.(4) Image-text\nmatching (ITM) or image-text retrieval. The goal of this task is to find the semantically closest\nitem of one modality to the query in the other modality. The query can be a sample in visual\nmodality or textual modality. (5) Reference expression comprehension. The objective of this task\nis togeneratetheboundingboxes intheimage correspondingtothegiven text.\n7.2 Visionand Sensors\nInrecentyears,autonomousdrivingsystemsbasedonmultimodalitieshaveattractedattentionin\nthe industry. There are an increasing number of applications related to the fusion of RGB modal-\nity and other modalities, e.g., LiDAR, Radar, and infrared data. Here, we introduce the two most\nfamous tasks of the fusion of vision data and other sensors: (1) Multimodal semantic segmenta-\ntion task. The objective of this task is to generate the semantic segment of objects based on the\nmultimodal input data. In this task, the combination of input data can vary, such as RGB-depth\ndata,RGB-infrareddata,RGB-LiDARdata,RGB-Radardata,RGB-depth-LiDARdata,RGB-skeleton\ndata, and CT-MRI data. (2) Multimodal object detection task. In this task, the model is supposed\nto generate the bounding box of each instance in the multimodal input data. (3) Multimodal sen-\ntimentanalysis.Unliketheconventionalsingle-modaldata-basedsentimentanalysismodels,e.g.,\ntext-based models or image-based models, the model is supposed to be able to fuse the multi-\nmodalinputdataandpredictthesentimentofthesample.(4)Multimodalpersonre-identification\ntask. Compared to the traditional person re-identification task, the major difference is that the\ninput data can be the combinations of different modalities, e.g., RGB+depth data, RGB+thermal\ndata, and RGB+depth+thermal data. (5) multimodal human activity recognition task. This task is\nan integral part of a variety of applications, such as video gaming and health monitoring. The\ninput data can be RGB+depth data, RGB+infrared data and RGB+inertial data, and so on. (6) Mul-\ntimodalFaceAnti-SpoofingTask.Inthistask,amodelisdesignedtodistinguishbetweengenuine\nand spoof face presentations by leveraging multimodal data, such as RGB images, depth informa-\ntion,andinfraredimages.Thefusionofdifferentmodalitiesenablesthemodeltocapturevarious\nfacial features and anomalies that are not discernible in single-modal data, enhancing the robust-\nness and accuracy of anti-spoofing systems. For instance, the combination of RGB and infrared\ndata can help in detecting the presence of masks or other artifacts used for spoofing, while the\nintegration of depth information can reveal abnormal facial structures or the absence of three-\ndimensional features in a spoof attempt. (7) Multimodal person re-identification task. This task\ninvolvesidentifyingindividualsacrossdifferentcameraviewsorsessionsbyfusingdatafrommul-\ntiple modalities such as RGB images, depth data, and thermal imagery. Unlike traditional person\nre-identification tasks that rely solely on RGB data, multimodal person re-identification models\nintegratediversesensoryinputstoextractmorecomprehensiveanddiscriminativefeaturesofin-\ndividuals.Forexample,thefusionofRGBandthermaldatacanbeparticularlyeffectiveinenviron-\nmentswithvaryinglightingconditions,allowingthemodeltorecognizeindividualsbasedontheir\nthermalsignatures.Similarly,incorporatingdepthdatacanprovideadditionalstructuralinforma-\ntion about the individual, enhancing the model’s ability to match persons in different poses or\nviewpoints.\n7.3 Other\nOther than the tasks mentioned above, there are many other miscellaneous multimodal data fu-\nsiontasks.Forexample,inthebiomedicalfield,therearedrug-protein/protein-proteininteraction\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:24 F. Zhaoetal.\npredictiontask,classificationtasksfordifferentdiseases,e.g.,ADandcancersubtype.Inthechem-\nistryfield,therearemultimodaltoxicityclassification,multimodalretentionindexprediction,and\nmulti-sensor-basedgas typeclassification,and soon.\n8 FUTURE DIRECTIONS\nDeep learning-based multimodal data fusion has seen a rapid growth in the recent decade. How-\never,thereare stillseveralresearchgaps:\n8.1 Missing ModalityChallenges\nIn real scenarios,there are two sub-classes: missing modality issue and noisy modality issue. The\nmissing modality issue is that at least one modality is absent in a multimodal sample. The noisy\nmodality issue is that at least one modality’s data is noisy or misaligned. Most SOTA methods\nare based on the assumption that there is no missing data issue in the dataset. Many SOTA deep\ndatafusionmodelsbasedonthisassumptioncanonlyworkwellunderidealconditions.Currently,\nthereareseveralworksaddressingmissingmodalityissues,e.g.,[ 179]proposesaGAN-basednet-\nwork to reconstruct the missing PET modality from available MRI data in samples; the work in\n[176]averagesthelearnedlatentvectorofindividualmodalityfromindependentencoders,andre-\nconstructseachmodalityfromtheaveragedlatentvector.However,thesemethodsnaivelyassume\nthat each modality is as same informative as the others and contributes to the final task equally.\nIn the future, (1) to create adaptive fusion models, which can learn the relative importance of dif-\nferent modalities for the final task, can be a potential direction of improvement in such models.\nTheadaptivefusionmodelsshouldbeabletoautomaticallydroptheinfluenceoflessinformative\nmodality on the task, and vice versa. (2) To develop fully distributed deep multimodal fusion. In\nthecentralizedsetting,localfeaturesaretransmittedorrelayedtoafusioncenterwherethefinal\ndecision is made. Devices or sensors may enter or leave the network dynamically, leading to un-\npredictablechangesinnetworksizeandtopology.Sensorsordevicesmaydisappearpermanently\neither due to damage to the nodes or drained batteries. Moreover, connectivity/communication\nbetweenthedevicesofeachmodalityandthefusioncenterisrarelyperfectbecauseofbandwidth\nconstrains and energy limitations [ 18,231]. This motivates us to consider a fully decentralized\ndeepmultimodalfusionframeworkwithoutafusioncenter,whereallthemodalitiescontinuously\nupdateand exchangethelocaldecisions/featureswiththeirneighborsto reacha consensus.\n8.2 Lack of Data\nMultimodaldatafusionisanemergingresearchfieldofartificialintelligence.Thepubliclyavailable\nmultimodaldatasetsarestilllimited.Asweknow,theperformanceofdeeplearning-basedmodels\nistypicallydependentonthenumberofsamplesusedduringthetrainingprocess.Ahigh-quality\nand large-scale dataset will hugely help the model learn accurate and comprehensive representa-\ntionsoftheobservedobjectsoractivities.Therefore,creatinglargerandhigher-qualitymultimodal\ndatasetsis oneof thecriticaltasksto move thefield forward.\n8.3 Lack of Large Pre-trained Models\nLarge pre-trained models are able to learn more comprehensive representations for multimodali-\nties. With transfer learning, the well-trained large pre-trained models can have competitive per-\nformance on downstream tasks compared to task-specific designed models. However, currently,\nexisting large pre-trained multimodal models only focus on the interdisciplinary field of CV and\nNLP.Inthefuture,creatinglargepre-trainedmultimodalmodelsforotherinterdisciplinaryfields\ncanbea potentialdirectionin multimodaldata fusion.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:25\n8.4 Interpretability ofModels\nDespite the incredible success of data driven approaches in different disciplines, there are some\ndrawbacksofdeeplearningmodelsthatlimittheirapplicability.Forinstance,ittypicallyrequires\namassivetrainingdataandintensivecomputationalresourcestolearnadesirablemapping,which\nare hardly available in communication degraded or energy constrained environments. Moreover,\nDNNarecommonlydesignedtobeblackboxesthatdonotexplainhowtounderstandandcharac-\nterize the prediction result and confident intervals [ 159]. In order to address this challenge, there\naretwopotentialways:(1)Combinestatisticalsignalprocessingframeworkanddeeplearningfor\nimprovedscalibiltyandinterpretability.Statisticalsignalprocessingandmodel-basedapproaches\nemployconcretemathematicalformulationsthatrepresenttheunderlyingphysics,priorinforma-\ntion and additional domain knowledge of the problem, thus offering interpretability, flexibility,\nversatility,scalibility,androbustness[ 159,229].Indeepmultimodalfusionundervariouspiratical\nconstraints,thenetworktopology,traffic,andchannelconditionsmaychangedynamically,andit\nisworthwhiletoinvestigatethecombinationofcomplementarystrengthsofbothstatisticalsignal\nprocessinganddeeplearningapproaches.Therefore,tocombinestatisticalsignalprocessingframe-\nworkanddeeplearningforimprovedscalibiltyandinterpretabilitycanbeapromisingdirectionof\nmultimodaldatafusion.(2)Inferenceanddecisionmakingwithhumansintheloop.Incriticalhigh-\nstake situations, incorporating human cognitive strengths and expertise in heterogeneous multi-\nmodalitynetworksisimperativetoimprovedecisionqualityandtoenhancesituationalawareness.\nThe next generation of smart manufacturing, intelligent IoT, augmented reality, and remote diag-\nnosissystemswouldrequiretheseamlessintegrationofthehumanandthemachine/deeplearning\nalgorithminthesameenvironmenttounderstandandsolveproblems.Forexample,innaturaldis-\nasterorpandemicearlywarning,alertandresponsesystemswherehumanlivesandassetsdepend\non a small detail being observed or missed, automatic ML based decision making may not be suf-\nficient, and it is necessary to incorporate human(s) in the loop of decision making, intelligence\ngathering, policy making and control. Modeling and analysis of human decision making is still a\nchallenging problem, as it needs to consider several factors including cognitive biases of humans,\nmechanisms to handle behavior uncertainties of humans, as well as human machine interactions,\nin contrastto decisionmakingprocessesconsistingof only machineagents [ 38,50].\n9 CONCLUSION\nIn this survey, we introduce the background and review the contemporary models of deep mul-\ntimodal data fusion. We provide a novel fine-grained taxonomy which groups SOTA multimodal\ndatafusionmethodsintofivecategories:Encoder-DecoderMethods,AttentionMechanismMeth-\nods, GNN Methods, GenNN Methods, and other Constraint-based Methods. In addition, a wide\nrange of applications and datasets related to multimodal fusion are also included. Finally, the fu-\ntureresearchdirectionsfor deepmultimodal datafusionare explored.\nREFERENCES\n[1] Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat Khassanov, Michael Lewis, and Huseyin Atakan\nVarol. 2021. Speakingfaces: A large-scale multimodal dataset of voice commands with visual and thermal video\nstreams.Sensors21,10(2021),3465.\n[2] SarahA.Abdu,AhmedH.Yousef,andAshrafSalem.2021.Multimodalvideosentimentanalysisusingdeeplearning\napproaches,asurvey. Information Fusion 76,C (2021),204–226.\n[3] HassanAkbari,LiangzheYuan,RuiQian,Wei-HongChuang,Shih-FuChang,YinCui,andBoqingGong.2021.Vatt:\nTransformersformultimodalself-supervisedlearningfromrawvideo,audioandtext. AdvancesinNeuralInformation\nProcessing Systems 34(2021),24206–24221.\n[4] Furqan Alam, Rashid Mehmood, Iyad Katib, Nasser N. Albogami, and Aiiad Albeshri. 2017. Data fusion and IoT for\nsmartubiquitous environments: Asurvey. IEEEAccess 5(2017),9533–9554.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:26 F. Zhaoetal.\n[5] Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. 2019. Fusion of detected objects in text for visual\nquestion answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe9th International JointConference onNatural Language Processing (EMNLP-IJCNLP) .2131–2140.\n[6] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. 2017. Localizing\nmoments in video with natural language. In Proceedings of the IEEE International Conference on Computer Vision .\n5803–5812.\n[7] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh.2015.Vqa:Visualquestionanswering.In ProceedingsoftheIEEEInternationalConferenceonComputerVision .\n2425–2433.\n[8] Roshanak Ashrafi, Mona Azarbayjani, and Hamed Tabkhi. 2022. Charlotte-ThermalFace: A fully annotated thermal\ninfraredfacedatasetwithvariousenvironmentalconditionsanddistances. InfraredPhysicsandTechnology 124(2022),\n104209.\n[9] Mehmet Aygün, Yusuf Hüseyin Şahin, and Gözde Ünal. 2018. Multi modal convolutional neural networks for brain\ntumor segmentation.arXiv:1809.06191.Retrievedfrom https://arxiv.org/abs/1809.06191\n[10] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to\nalignandtranslate.arXiv:1409.0473.Retrievedfrom https://arxiv.org/abs/1409.0473\n[11] TadasBaltrušaitis,ChaitanyaAhuja,andLouis-PhilippeMorency.2018.Multimodalmachinelearning:Asurveyand\ntaxonomy. IEEETransactions onPattern Analysisand Machine Intelligence 41,2(2018),423–443.\n[12] SatanjeevBanerjeeandAlonLavie.2005.METEOR:AnautomaticmetricforMTevaluationwithimprovedcorrela-\ntion with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for\nMachine Translation and/orSummarization . 65–72.\n[13] Igor Barros Barbosa, Marco Cristani, Alessio Del Bue, Loris Bazzani, and Vittorio Murino. 2012. Re-identification\nwith rgb-d sensors. In Proceedings of the Computer Vision–ECCV 2012. Workshops and Demonstrations: Florence, Part\nI1 2.Springer, 433–442.\n[14] NiharBendre,KevinDesai,andPeymanNajafirad.2021.Generalizedzero-shotlearningusingmultimodalvariational\nauto-encoder with semantic concepts. In Proceedings of the 2021 IEEE International Conference on Image Processing\n(ICIP).IEEE, 1284–1288.\n[15] YoshuaBengio,AaronCourville,andPascalVincent.2013.Representationlearning:Areviewandnewperspectives.\nIEEETransactions onPatternAnalysis andMachine Intelligence 35,8(2013),1798–1828.\n[16] Padmalochan Bera and Shobh. 2021. ModCGAN: A multimodal approach to detect new malware. In Proceedings of\nthe 2021 International Conference on Cyber Situational Awareness, Data Analytics, and Assessment (CyberSA) . IEEE,\n1–2.\n[17] José-Luis Blanco-Claraco, Francisco-Angel Moreno-Duenas, and Javier González-Jiménez. 2014. The Málaga urban\ndataset: High-rate stereo and LiDAR in a realistic urban scenario. The International Journal of Robotics Research 33,\n2 (2014),207–214.\n[18] Müjdat Cetin, Lei Chen, John W. Fisher, Alexander T. Ihler, Randolph L. Moses, Martin J. Wainwright, and Alan S.\nWillsky.2006.Distributedfusion in sensor networks. IEEESignal Processing Magazine 23,4(2006),42–55.\n[19] ChongqingChen,DezhiHan,andJunWang.2020.Multimodalencoder-decoderattentionnetworksforvisualques-\ntion answering. IEEEAccess 8(2020),35662–35671.\n[20] Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. 2015. UTD-MHAD: A multimodal dataset for human action\nrecognition utilizing a depth camera and a wearable inertial sensor. In Proceedings of the 2015 IEEE International\nConferenceonImage Processing (ICIP) .IEEE, 168–172.\n[21] Dave Zhenyu Chen, Angel X. Chang, and Matthias Nießner. 2020. Scanrefer: 3d object localization in rgb-d scans\nusing naturallanguage.In Proceedings of theEuropean ConferenceonComputerVision .Springer, 202–221.\n[22] FeilongChen,MinglunHan,HaozhiZhao,QingyangZhang,JingShi,ShuangXu,andBoXu.2023.X-llm:Bootstrap-\npingadvancedlargelanguagemodelsbytreatingmulti-modalitiesasforeignlanguages.arXiv:2305.04160.Retrieved\nfromhttps://arxiv.org/abs/2305.04160\n[23] Huai Chen, Yuxiao Qi, Yong Yin, Tengxiang Li, Xiaoqing Liu, Xiuli Li, Guanzhong Gong, and Lisheng Wang. 2020.\nMMFNet: A multi-modality MRI fusion network for segmentation of nasopharyngeal carcinoma. Neurocomputing\n394(2020),27–40.\n[24] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020.\nUniter: Universal image-text representation learning. In Proceedings of the European Conference on Computer Vision .\nSpringer, 104–120.\n[25] Jae Won Cho, Dong-Jin Kim, Hyeonggon Ryu, and In So Kweon. 2023. Generative bias for robust visual question\nanswering.In Proceedings oftheIEEE/CVFConferenceonComputer VisionandPattern Recognition . 11681–11690.\n[26] Jonghyun Choi, Hyunjong Cho, Jungsuk Kwac, and Larry S. Davis. 2014. Toward sparse coding on cosine distance.\nInProceedings of the2014 22ndInternational ConferenceonPatternRecognition . IEEE, 4423–4428.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:27\n[27] Camille Couprie, Clément Farabet, Laurent Najman, and Yann LeCun. 2013. Indoor semantic segmentation using\ndepthinformation.arXiv:1301.3572.Retrievedfrom https://arxiv.org/abs/1301.3572\n[28] LeandroCruz,DjalmaLucio,andLuizVelho.2012.Kinectandrgbdimages:Challengesandapplications.In Proceed-\nings ofthe2012 25th SIBGRAPI Conferenceon Graphics, Patterns and Images Tutorials . IEEE, 36–49.\n[29] PingyangDai,RongrongJi,HaibinWang,QiongWu,andYuyuHuang.2018.Cross-modalitypersonre-identification\nwithgenerativeadversarial training.In Proceedings oftheIJCAI .6.\n[30] ChengDeng,ZhaojiaChen,XianglongLiu,XinboGao,andDachengTao.2018.Triplet-baseddeephashingnetwork\nfor cross-modal retrieval. IEEETransactions onImage Processing 27,8(2018),3893–3903.\n[31] LiuyuanDeng,MingYang,TianyiLi,YueshengHe,andChunxiangWang.2019.RFBNet:Deepmultimodalnetworks\nwith residual fusion blocks for RGB-D semantic segmentation. arXiv:1907.00135. Retrieved from https://arxiv.org/\nabs/1907.00135\n[32] Pengchao Deng, Chenyang Ge, Hao Wei, Yuan Sun, and Xin Qiao. 2023. Attention-aware dual-stream network for\nmultimodalfaceanti-spoofing. IEEETransactions onInformation Forensics andSecurity 18(2023),4258–4271.\n[33] Chaoyue Ding, Shiliang Sun, and Jing Zhao. 2023. MST-GAT: A multimodal spatial–temporal graph attention net-\nwork for timeseries anomalydetection. Information Fusion 89(2023),527–536.\n[34] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxiaYang,andJieTang.2021.Cogview:Masteringtext-to-imagegenerationviatransformers. AdvancesinNeu-\nral Information Processing Systems 34(2021),19822–19835.\n[35] Denis Dresvyanskiy, Elena Ryumina, Heysem Kaya, Maxim Markitantov, Alexey Karpov, and Wolfgang Minker.\n2020. An audio-video deep and transfer learning framework for multimodal emotion recognition in the wild.\narXiv:2010.03692.Retrievedfrom https://arxiv.org/abs/2010.03692\n[36] Weichen Fan, Jinghuan Chen, Jiabin Ma, Jun Hou, and Shuai Yi. 2022. Styleflow for content-fixed image to image\ntranslation.arXiv:2207.01909.Retrievedfrom https://arxiv.org/abs/2207.01909\n[37] ZeinabFarhoudiandSaeedSetayeshi.2021.Fusionofdeeplearningfeatureswithmixtureofbrainemotionallearning\nfor audio-visual emotionrecognition. Speech Communication 127(2021),92–103.\n[38] Charles Findling and Valentin Wyart. 2021. Computation noise in human learning and decision-making: Origin,\nimpact,function. CurrentOpinion in Behavioral Sciences 38(2021),124–132.\n[39] FahimehFooladgarandShohrehKasaei.2019.Multi-modalattention-basedfusionmodelforsemanticsegmentation\nof RGB-depthimages.arXiv:1912.11691.Retrievedfrom https://arxiv.org/abs/1912.11691\n[40] K.Foster,G.Christie,andM.Brown.2020.IEEEDataport:Urbansemantic3Ddataset. https://doi.org/10.21227/9frn-\n7208\n[41] ValentinGabeur,ChenSun,KarteekAlahari,andCordeliaSchmid.2020.Multi-modaltransformerforvideoretrieval.\nInProceedings oftheEuropean ConferenceonComputer Vision .Springer, 214–229.\n[42] Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, and Xilin Chen. 2020. Multi-modal graph neural network for joint\nreasoningonvisionandscenetext.In ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecog-\nnition.12746–12756.\n[43] Difei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yi Yang, and Mike Zheng Shou. 2023. MIST: Multi-modal iterative\nspatial-temporaltransformerforlong-formvideoquestionanswering.In ProceedingsoftheIEEE/CVFConferenceon\nComputer Vision andPattern Recognition . 14773–14783.\n[44] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to a machine?\ndataset and methods for multilingual image question. Advances in Neural Information Processing Systems 28 (2015),\n2296–2304.\n[45] Jing Gao, Peng Li, Zhikui Chen, and Jianing Zhang. 2020. A survey on deep learning for multimodal data fusion.\nNeural Computation 32,5 (2020),829–864.\n[46] Mingliang Gao, Jun Jiang, Guofeng Zou, Vijay John, and Zheng Liu. 2019. RGB-D-based object recognition using\nmultimodalconvolutionalneural networks: A survey. IEEEAccess 7(2019),43110–43136.\n[47] Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven C. H. Hoi, Xiaogang Wang, and Hongsheng Li. 2019.\nDynamic fusion with intra-and inter-modality attention flow for visual question answering. In Proceedings of the\nIEEE/CVFConferenceonComputer Visionand PatternRecognition . 6639–6648.\n[48] Xingyu Gao, Feng Shi, Dinggang Shen, and Manhua Liu. 2022. Task-induced pyramid and attention GAN for multi-\nmodal brain image imputation and classification in alzheimers disease. IEEE Journal of Biomedical and Health Infor-\nmatics26,1(2022),36–43.\n[49] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. 2013. Vision meets robotics: The kitti dataset.\nTheInternational Journal ofRobotics Research 32,11(2013),1231–1237.\n[50] BaochengGengandPramodK.Varshney.2019.Ondecisionmakinginhuman-machinenetworks.In Proceedingsof\nthe2019 IEEE16th International ConferenceonMobile AdHoc and SensorSystems (MASS) .IEEE, 37–45.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:28 F. Zhaoetal.\n[51] EsamGhaleb,MirelaPopa,andStylianosAsteriadis.2019.Multimodalandtemporalperceptionofaudio-visualcues\nforemotionrecognition.In Proceedingsofthe20198thInternationalConferenceonAffectiveComputingandIntelligent\nInteraction (ACII) .IEEE, 552–558.\n[52] Reza Shoja Ghiass, Hakim Bendada, and Xavier Maldague. 2018. Université laval face motion and time-lapse video\ndatabase(ul-fmtv).In Proceedings ofthe14th International ConferenceonQuantitative Infrared Thermography .\n[53] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter:\nElevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition . 6904–6913.\n[54] Xin Gu, Guang Chen, Yufei Wang, Libo Zhang, Tiejian Luo, and Longyin Wen. 2023. Text with knowledge graph\naugmented transformer for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition . 18941–18951.\n[55] YananGuo,DapengTao,WeifengLiu,andJunCheng.2016.Multiviewcauchyestimatorfeatureembeddingfordepth\nandinertialsensor-basedhumanactionrecognition. IEEETransactionsonSystems,Man,andCybernetics:Systems 47,\n4(2016),617–627.\n[56] Ning Han, Jingjing Chen, Hao Zhang, Huanwen Wang, and Hao Chen. 2022. Adversarial multi-grained embedding\nnetwork for cross-modal text-video retrieval. ACM Transactions on Multimedia Computing, Communications, and\nApplications 18,2(2022),1–23.\n[57] DavidR.Hardoon,SandorSzedmak,andJohnShawe-Taylor.2004.Canonicalcorrelationanalysis:Anoverviewwith\napplicationto learningmethods. Neural Computation 16,12(2004),2639–2664.\n[58] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilisticmodels. AdvancesinNeuralInfor-\nmation Processing Systems 33(2020),6840–6851.\n[59] Danfeng Hong, Lianru Gao, Renlong Hang, Bing Zhang, and Jocelyn Chanussot. 2022. Deep encoder-decoder net-\nworksforclassificationofhyperspectralandLiDARdata. IEEEGeoscienceandRemoteSensingLettersLetters 19(2022),\n1–5.\n[60] Chih-Chung Hsu, Pi-Ju Tsai, Ting-Chun Yeh, and Xiu-Yu Hou. 2022. A comprehensive study of spatiotemporal fea-\nture learning for social medial popularity prediction. In Proceedings of the 30th ACM International Conference on\nMultimedia . 7130–7134.\n[61] JingwenHu,YuchenLiu,JinmingZhao,andQinJin.2021.MMGCN:Multimodalfusionviadeepgraphconvolution\nnetwork for emotion recognition in conversation. In Proceedings of the 59th Annual Meeting of the Association for\nComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:Long\nPapers). Online,5666–5675.\n[62] Ronghang Hu and Amanpreet Singh. 2021. Unit: Multimodal multitask learning with a unified transformer. In Pro-\nceedings oftheIEEE/CVFInternational ConferenceonComputer Vision .1439–1449.\n[63] ShuowenHu,JonghyunChoi,AlexL.Chan,andWilliamRobsonSchwartz.2015.Thermal-to-visiblefacerecognition\nusing partialleastsquares. JOSAA32,3(2015),431–442.\n[64] XinxinHu,KailunYang,LeiFei,andKaiweiWang.2019.Acnet:Attentionbasednetworktoexploitcomplementary\nfeaturesforrgbdsemanticsegmentation.In Proceedingsofthe2019IEEEInternationalConferenceonImageProcessing\n(ICIP).IEEE, 1440–1444.\n[65] Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and Huchuan Lu. 2020. Bi-directional relationship inferring net-\nwork for referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition . 4424–4433.\n[66] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. 2021. Text-guided graph neural networks\nfor referring 3dinstancesegmentation.In Proceedings of theAAAIConferenceonArtificial Intelligence .1610–1618.\n[67] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. 2020. Referring\nimagesegmentationviacross-modalprogressivecomprehension.In ProceedingsoftheIEEE/CVFConferenceonCom-\nputer Vision and Pattern Recognition . 10488–10497.\n[68] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-bert: Aligning image pixels\nwithtextbydeepmulti-modaltransformers. arXiv:2004.00849.Retrievedfrom https://arxiv.org/abs/2004.00849\n[69] Shang-WeiHung,Shao-YuanLo,andHsueh-MingHang.2019.Incorporatingluminance,depthandcolorinformation\nby a fusion-based network for semantic segmentation. In Proceedings of the 2019 IEEE International Conference on\nImage Processing (ICIP) .IEEE, 2374–2378.\n[70] O.Iosifova,I.Iosifov,andO.Rolik.2020.Techniquesandcomponentsfornaturallanguageprocessing. MoMLeT&DS\n2631,I(2020),57–67.\n[71] Allison Janoch, Sergey Karayev, Yangqing Jia, Jonathan T. Barron, Mario Fritz, Kate Saenko, and Trevor Darrell.\n2013. A category-level 3d object dataset: Putting the kinect to work. In Proceedings of the Consumer Depth Cameras\nfor ComputerVision .Springer, 141–165.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:29\n[72] JinyongJeong,YounggunCho,Young-SikShin,HyunchulRoh,andAyoungKim.2019.Complexurbandatasetwith\nmulti-level sensors from highly diverse urban environments. The International Journal of Robotics Research 38, 6\n(2019),642–657.\n[73] MyeonghunJeong,HyeongjuKim,SungJunCheon,ByoungJinChoi,andNamSooKim.2021.Diff-tts:Adenoising\ndiffusion modelfor text-to-speech. arXiv:2104.01409.Retrievedfrom https://arxiv.org/abs/2104.01409\n[74] Lei Ji, Rongcheng Tu, Kevin Lin, Lijuan Wang, and Nan Duan. 2022. Multimodal graph neural network for video\nprocedural captioning. Neurocomputing 488(2022),88–96. DOI:https://doi.org/10.1016/j.neucom.2022.02.062\n[75] Xiangen Jia, Min Jiang, Yihong Dong, Feng Zhu, Haocai Lin, Yu Xin, and Huahui Chen. 2023. Multimodal heteroge-\nneous graph attentionnetwork. Neural Computing andApplications 35,4(2023),3357–3372.\n[76] Jianguo Jiang, Kaiyuan Jin, Meibin Qi, Qian Wang, Jingjing Wu, and Cuiqun Chen. 2020. A cross-modal multi-\ngranularityattentionnetwork forRGB-IRperson re-identification. Neurocomputing 406(2020),59–67.\n[77] LeiJiangandZuqiangMeng.2023.Knowledge-basedvisualquestion answeringusingmulti-modalsemanticgraph.\nElectronics 12,6(2023),1390.\n[78] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick.\n2017.Clevr:Adiagnosticdatasetforcompositionallanguageandelementaryvisualreasoning.In Proceedings ofthe\nIEEEConferenceonComputer VisionandPattern Recognition . 2901–2910.\n[79] Laurynas Karazija, Iro Laina, and Christian Rupprecht. 2021. Clevrtex: A texture-rich benchmark for unsupervised\nmulti-objectsegmentation.arXiv:2111.10265.Retrievedfrom https://arxiv.org/abs/2111.10265\n[80] SaharKazemzadeh,VicenteOrdonez,MarkMatten,andTamaraBerg.2014.Referitgame:Referringtoobjectsinpho-\ntographsofnaturalscenes.In Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing\n(EMNLP).787–798.\n[81] DhruvKhattar,JaipalSinghGoud,ManishGupta,andVasudevaVarma.2019.Mvae:Multimodalvariationalautoen-\ncoder for fakenewsdetection.In Proceedings oftheWorldWide Web Conference .2915–2921.\n[82] Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, and Byoung-Tak Zhang. 2018. Multimodal dual attention memory\nfor video story question answering.In Proceedings oftheEuropean ConferenceonComputer Vision(ECCV) .673–688.\n[83] DiederikP.KingmaandMaxWelling.2013.Auto-encodingvariationalbayes.arXiv:1312.6114.Retrievedfrom https:\n//arxiv.org/abs/1312.6114\n[84] Michael Kölle, Dominik Laupheimer, Stefan Schmohl, Norbert Haala, Franz Rottensteiner, Jan Dirk Wegner, and\nHugo Ledoux. 2021. The Hessigheim 3D (H3D) benchmark on semantic segmentation of high-resolution 3D point\nclouds and textured meshes from UAV LiDAR and Multi-View-Stereo. ISPRS Open Journal of Photogrammetry and\nRemote Sensing 1(2021),100001.\n[85] D.N.Krishna.2021.Usinglargepre-trainedmodelswithcross-modalattentionformulti-modalemotionrecognition.\narXiv:2108.09669.Retrievedfrom https://arxiv.org/abs/2108.09669\n[86] RanjayKrishna,KenjiHata,FredericRen,LiFei-Fei,andJuanCarlosNiebles.2017.Dense-captioningeventsinvideos.\nInProceedings oftheIEEEInternational ConferenceonComputer Vision .706–715.\n[87] RyoheiKuga,AsakoKanezaki,MasakiSamejima,YusukeSugano,andYasuyukiMatsushita.2017.Multi-tasklearning\nusingmulti-modalencoder-decodernetworkswithsharedskipconnections.In ProceedingsoftheIEEEInternational\nConferenceonComputer VisionWorkshops .403–411.\n[88] YannLeCun,Yoshua Bengio,andGeoffrey Hinton. 2015.Deep learning. Nature521,7553(2015),436–444.\n[89] Chenyi Lei, Shixian Luo, Yong Liu, Wanggui He, JiamangWang, Guoxin Wang, Haihong Tang, Chunyan Miao, and\nHouqiang Li. 2021. Understanding chinese video and language via contrastive multimodal pre-training. In Proceed-\nings ofthe29th ACMInternational ConferenceonMultimedia . 2567–2576.\n[90] JieLei,LinjieLi,LuoweiZhou,ZheGan,TamaraL.Berg,MohitBansal,andJingjingLiu.2021.Lessismore:Clipbert\nforvideo-and-languagelearningviasparsesampling.In Proceedings oftheIEEE/CVFConferenceonComputerVision\nand Pattern Recognition . 7331–7341.\n[91] JieLei,LichengYu,MohitBansal,andTamaraL.Berg.2018.Tvqa:Localized,compositionalvideoquestionanswering.\narXiv:1809.01696.Retrievedfrom https://arxiv.org/abs/1809.01696\n[92] GenLi,NanDuan,YuejianFang,MingGong,andDaxinJiang.2020.Unicoder-vl:Auniversalencoderforvisionand\nlanguagebycross-modal pre-training.In Proceedings of theAAAIConferenceonArtificial Intelligence .11336–11344.\n[93] JiangLi,XiaopingWang,GuoqingLv,andZhigangZeng.2024.Graphcfc:Adirectedgraphbasedcross-modalfeature\ncomplementationapproachformultimodalconversationalemotionrecognition. IEEETransactionsonMultimedia 26\n(2024),77–89.\n[94] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. 2020. Hero: Hierarchical encoder\nfor video+ language omni-representation pre-training. arXiv:2005.00200. Retrieved from https://arxiv.org/abs/2005.\n00200\n[95] Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. 2019. Relation-aware graph attention network for visual question\nanswering.In Proceedings oftheIEEE/CVFInternational ConferenceonComputerVision .10313–10322.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:30 F. Zhaoetal.\n[96] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and perfor-\nmantbaselinefor vision andlanguage.arXiv:1908.03557.Retrieved from https://arxiv.org/abs/1908.03557\n[97] RuiyuLi,KaicanLi,Yi-ChunKuo,MichelleShu,XiaojuanQi,XiaoyongShen,andJiayaJia.2018.Referringimageseg-\nmentation via recurrent refinement networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition . 5745–5753.\n[98] RuiyuLi,KaicanLi,Yi-ChunKuo,MichelleShu,XiaojuanQi,XiaoyongShen,andJiayaJia.2018.Referringimageseg-\nmentation via recurrent refinement networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) .IEEE, 5745–5753. DOI:https://doi.org/10.1109/CVPR.2018.00602\n[99] YunchengLi,YaleSong,LiangliangCao,JoelTetreault,LarryGoldberg,AlejandroJaimes,andJieboLuo.2016.TGIF:\nAnewdatasetandbenchmarkonanimatedGIFdescription.In ProceedingsoftheIEEEConferenceonComputerVision\nand Pattern Recognition . 4641–4650.\n[100] YaoyuanLiang,XinWang,XuguangDuan,andWenwuZhu.2021.Multi-modalcontextualgraphneuralnetworkfor\ntextvisualquestionanswering.In Proceedingsofthe202025thInternationalConferenceonPatternRecognition(ICPR) .\nIEEE, 3491–3498.\n[101] JunyangLin,AnYang,YichangZhang,JieLiu,JingrenZhou,andHongxiaYang.2020.Interbert:Vision-and-language\ninteraction formulti-modalpretraining.arXiv:2003.13198.Retrievedfrom https://arxiv.org/abs/2003.13198\n[102] AjianLiu,ZichangTan,JunWan,SergioEscalera,GuodongGuo,andStanZ.Li.2021.Casia-surfcefa:Abenchmark\nformulti-modalcross-ethnicityfaceanti-spoofing.In ProceedingsoftheIEEE/CVFWinterConferenceonApplications\nof ComputerVision .1179–1187.\n[103] AjianLiu,ZichangTan,JunWan,YanyanLiang,ZhenLei,GuodongGuo,andStanZ.Li.2021.Faceanti-spoofingvia\nadversarialcross-modalitytranslation. IEEETransactionsonInformationForensicsandSecurity 16(2021),2759–2772.\n[104] Chang Liu, Henghui Ding, Yulun Zhang, and Xudong Jiang. 2023. Multi-modal mutual attentionand iterative inter-\naction forreferring imagesegmentation. IEEETransactions onImage Processing 32(2023),3054–3065.\n[105] Hong Liu, Liang Hu, and Liqian Ma. 2017. Online RGB-D person re-identification based on metric model update.\nCAAITransactionsonIntelligence Technology 2,1 (2017),48–55.\n[106] RuntaoLiu,ChenxiLiu,YutongBai,andAlanL.Yuille.2019.Clevr-ref+:Diagnosingvisualreasoningwithreferring\nexpressions. In Proceedings of theIEEE/CVFConferenceonComputer Visionand PatternRecognition . 4185–4194.\n[107] Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical\ntransformerwithmomentumcontrastforvideo-textretrieval.In ProceedingsoftheIEEE/CVFInternationalConference\nonComputer Vision .11915–11925.\n[108] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-Philippe\nMorency. 2018. Efficient low-rank multimodal fusion with modality-specific factors. arXiv:1806.00064. Retrieved\nfromhttps://arxiv.org/abs/1806.00064\n[109] Zhengyi Liu, Song Shi, Quntao Duan, Wei Zhang, and Peng Zhao. 2019. Salient object detection for RGB-D image\nbysingle stream recurrent convolution neural network. Neurocomputing 363(2019),46–57.\n[110] Serveh Lotfi, Mitra Mirzarezaee, Mehdi Hosseinzadeh, and Vahid Seydi. 2021. Detection of rumor conversations in\nTwitterusing graphconvolutional networks. AppliedIntelligence 51,7(2021),4774–4787.\n[111] Houhong Lu, Yangyang Zhu, Ming Yin, Guofu Yin, and Luofeng Xie. 2022. Multimodal fusion convolutional neu-\nral network with cross-attention mechanism for internal defect detection of magnetic tile. IEEE Access 10 (2022),\n60876–60886.\n[112] JiasenLu,DhruvBatra,DeviParikh,andStefanLee.2019.Vilbert:Pretrainingtask-agnosticvisiolinguisticrepresen-\ntations forvision-and-language tasks. Advances in Neural Information ProcessingSystems 32(2019),13–23.\n[113] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming\nZhou. 2020. Univl: A unified video and language pre-training model for multimodal understanding and generation.\narXiv:2002.06353.Retrievedfrom https://arxiv.org/abs/2002.06353\n[114] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 2017. 1 year, 1000 km: The Oxford RobotCar\ndataset.TheInternational Journal ofRobotics Research 36,1(2017),3–15.\n[115] Mateusz Malinowski, Carl Doersch, Adam Santoro, and Peter Battaglia. 2018. Learning visual question answering\nbybootstrappinghard attention.In Proceedings oftheEuropean ConferenceonComputer Vision(ECCV) .3–20.\n[116] MateuszMalinowskiandMarioFritz.2014.Amulti-worldapproachtoquestionansweringaboutreal-worldscenes\nbased onuncertain input. Advances in Neural Information Processing Systems 27(2014),1682–1690.\n[117] XinMan,DeqiangOuyang,XiangpengLi,JingkuanSong,andJieShao.2022.Scenario-awarerecurrenttransformer\nfor goal-directed video captioning. ACM Transactions on Multimedia Computing, Communications, and Applications\n18,4(2022),1–17.\n[118] JunhuaMao,JonathanHuang,AlexanderToshev,OanaCamburu,AlanL.Yuille,andKevinMurphy.2016.Generation\nand comprehension of unambiguous object descriptions. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition . 11–20.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:31\n[119] Edgar Margffoy-Tuay, Juan C. Pérez, Emilio Botero, and Pablo Arbeláez. 2018. Dynamic multimodal instance seg-\nmentationguidedbynaturallanguagequeries.In ProceedingsoftheEuropeanConferenceonComputerVision(ECCV) .\n630–645.\n[120] KAMAL UDDIN MD. 2021. Cross-modal and multi-modal person re-identification with RGB-D sensors. Array12\n(2021),100089.\n[121] Tong Meng, Xuyang Jing, Zheng Yan, and Witold Pedrycz. 2020. A survey on machine learning for data fusion.\nInformation Fusion 57(2020),115–129.\n[122] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019.\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings\noftheIEEE/CVFInternationalConferenceonComputer Vision .2630–2640.\n[123] Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K. Roy-Chowdhury. 2018. Learning joint em-\nbedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on International\nConferenceon Multimedia Retrieval . 19–27.\n[124] Niluthpol C. Mithun, Juncheng Li, Florian Metze, and Amit K. Roy-Chowdhury. 2019. Joint embeddings with multi-\nmodalcues for video-textretrieval. International Journal of Multimedia Information Retrieval 8,1(2019),3–18.\n[125] Safaa Abdullahi Moallim Mohamud, Amin Jalali, and Minho Lee. 2023. Encoder–decoder cycle for visual question\nansweringbasedon perception-actioncycle. Pattern Recognition 144(2023),109848.\n[126] Satyam Mohla, Shivam Pande, Biplab Banerjee, and Subhasis Chaudhuri. 2020. Fusatnet: Dual attention based spec-\ntrospatial multimodal fusion network for hyperspectral and lidar classification. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision andPattern Recognition Workshops .92–93.\n[127] Matteo Munaro, Andrea Fossati, Alberto Basso, Emanuele Menegatti, and Luc Van Gool. 2014. One-shot person\nre-identificationwith aconsumer depthcamera. PersonRe-Identification (2014),161–181.\n[128] ChemsEddineLouahemM’Sabah,AhmedBouziane,andYoucefFerdi.2021.Asurveyondeeplearningmethodsfor\ncancer diagnosis using multimodal data fusion. In Proceedings of the 2021 International Conference on e-Health and\nBioengineering (EHB) .IEEE, 1–4.\n[129] Medhini Narasimhan, Svetlana Lazebnik, and Alexander Schwing. 2018. Out of the box: Reasoning with graph con-\nvolution nets for factual visual question answering. Advances in Neural Information Processing Systems 31 (2018),\n2654–2665.\n[130] PushmeetKohliNathanSilberman,DerekHoiemandRobFergus.2012.Indoorsegmentationandsupportinference\nfromRGBDimages.In Proceedings oftheECCV .\n[131] DatTienNguyen,HyungGilHong,KiWanKim,andKangRyoungPark.2017.Personrecognitionsystembasedon\nacombinationof body imagesfrom visible lightandthermalcameras. Sensors17,3(2017),605.\n[132] Will Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot. 2018. Learning conditioned graph structures for inter-\npretablevisual question answering. Advances in Neural Information ProcessingSystems 31(2018),8334–8343.\n[133] Ferda Ofli, Rizwan Chaudhry, Gregorij Kurillo, René Vidal, and Ruzena Bajcsy. 2013. Berkeley mhad: A comprehen-\nsivemultimodalhumanactiondatabase.In Proceedingsofthe2013IEEEWorkshoponApplicationsofComputerVision\n(WACV).IEEE, 53–60.\n[134] OzanOktay,JoSchlemper,LoicLeFolgoc,MatthewLee,MattiasHeinrich,KazunariMisawa,KensakuMori,Steven\nMcDonagh, Nils Y. Hammerla, Bernhard Kainz, Ben Glocker, and Daniel Ruecker. 2018. Attention u-net: Learning\nwhere tolook forthe pancreas.arXiv:1804.03999.Retrieved from https://arxiv.org/abs/1804.03999\n[135] FedericoPala,RiccardoSatta,GiorgioFumera,andFabioRoli.2015.MultimodalpersonreidentificationusingRGB-D\ncameras. IEEETransactions onCircuits and Systems for Video Technology 26,4(2015),788–799.\n[136] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation\nof machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics .\n311–318.\n[137] YuxinPengandJinweiQi.2019.CM-GANs:Cross-modalgenerativeadversarialnetworksforcommonrepresentation\nlearning. ACMTransactions on Multimedia Computing, Communications, and Applications 15,1(2019),1–24.\n[138] Jeripothula Prudviraj, Malipatel Indrakaran Reddy, Chalavadi Vishnu, and Chalavadi Krishna Mohan. 2022. AAP-\nMIT:Attentiveatrouspyramidnetworkandmemoryincorporatedtransformerformultisentencevideodescription.\nIEEETransactions onImage Processing 31(2022),5559–5569.\n[139] DiQi,LinSu,JiaSong,EdwardCui,TaroonBharti,andArunSacheti.2020.Imagebert:Cross-modalpre-trainingwith\nlarge-scaleweak-supervised image-textdata.arXiv:2001.07966.Retrievedfrom https://arxiv.org/abs/2001.07966\n[140] Meibin Qi, Suzhi Wang, Guanghong Huang, Jianguo Jiang, Jingjing Wu, and Cuiqun Chen. 2021. Mask-guided dual\nattention-aware network for visible-infrared person re-identification. Multimedia Tools and Applications 80 (2021),\n17645–17666.\n[141] XiaojuanQi,RenjieLiao,JiayaJia,SanjaFidler,andRaquelUrtasun.2017.3dgraphneuralnetworksforrgbdsemantic\nsegmentation.In Proceedings of theIEEEInternational ConferenceonComputerVision .5199–5208.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:32 F. Zhaoetal.\n[142] Shengsheng Qian, Jun Hu, Quan Fang, and Changsheng Xu. 2021. Knowledge-aware multi-modal adaptive graph\nconvolutionalnetworksforfakenewsdetection. ACMTransactionsonMultimediaComputing,Communications,and\nApplications 17,3(2021),1–23.\n[143] ShuangQiu,YaoZhao,JianboJiao,YunchaoWei,andShikuiWei.2019.Referringimagesegmentationbygenerative\nadversarial learning. IEEETransactions onMultimedia 22,5(2019),1333–1344.\n[144] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional\nimagegenerationwithcliplatents. arXiv preprint arXiv:2204.06125 1,2(2022),3.\n[145] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.\n2021.Zero-shottext-to-imagegeneration.In ProceedingsoftheInternationalConferenceonMachineLearning .PMLR,\n8821–8831.\n[146] MiladRamezani,YiduoWang,MarcoCamurri,DavidWisth,MatiasMattamala,andMauriceFallon.2020.Thenewer\ncollegedataset:Handheldlidar,inertialandvisionwithgroundtruth.In Proceedingsofthe2020IEEE/RSJInternational\nConferenceonIntelligentRobots and Systems (IROS) . IEEE, 4353–4360.\n[147] Sahithya Ravi, Aditya Chinchure, Leonid Sigal, Renjie Liao, and Vered Shwartz. 2023. VLC-BERT: Visual question\nanswering with contextualized commonsense knowledge. In Proceedings of the IEEE/CVF Winter Conference on Ap-\nplications ofComputer Vision .1155–1165.\n[148] Adrià Recasens, Jason Lin, Jo ¯ao Carreira, Drew Jaegle, Luyu Wang, Jean-baptiste Alayrac, Pauline Luc, Antoine\nMiech, Lucas Smaira, Ross Hemsley, and Andrew Zisserma. 2023. Zorro: The masked multimodal transformer.\narXiv:2301.09595.Retrievedfrom https://arxiv.org/abs/2301.09595\n[149] JosephRedmonandAliFarhadi.2018.Yolov3:Anincrementalimprovement.arXiv:1804.02767.Retrievedfrom https:\n//arxiv.org/abs/1804.02767\n[150] Mengye Ren, Ryan Kiros, and Richard Zemel. 2015. Exploring models and data for image question answering. Ad-\nvances in Neural Information Processing Systems 28(2015),2953–2961.\n[151] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer.2022.High-resolutionimage\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition . 10684–10695.\n[152] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi.\n2022.Photorealistictext-to-imagediffusionmodelswithdeeplanguageunderstanding. AdvancesinNeuralInforma-\ntion ProcessingSystems 35(2022),36479–36494.\n[153] GauravSahuandOlgaVechtomova.2019.Adaptivefusiontechniquesformultimodaldata.In Proceedingsofthe16th\nConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics:MainVolume .Online,3156–3166.\n[154] Raeid Saqur and Karthik Narasimhan. 2020. Multimodal graph networks for compositional generalization in visual\nquestion answering. Advances in Neural Information Processing Systems 33(2020),3070–3081.\n[155] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. 2022. End-to-end Generative Pretraining\nforMultimodalVideoCaptioning.In IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR’22) ,Los\nAlamitos, CA,17938–17947.\n[156] Xiang Shen, Dezhi Han, Chin-Chen Chang, and Liang Zong. 2022. Dual self-guided attention with sparse question\nnetworks for visual question answering. IEICETRANSACTIONSonInformationand Systems 105,4(2022),785–796.\n[157] XiangShen,DezhiHan,ZihanGuo,ChongqingChen,JieHua,andGaofengLuo.2023.Localself-attentionintrans-\nformer for visualquestion answering. Applied Intelligence 53,13(2023),16706–16723.\n[158] Gen Shi, Yifan Zhu, Wenjin Liu, and Xuesong Li. 2021. A heterogeneous graph based framework for multimodal\nneuroimagingfusion learning.arXiv:2110.08465.Retrievedfrom https://arxiv.org/abs/2110.08465\n[159] NirShlezinger,JayWhang,YoninaC.Eldar,andAlexandrosGDimakis.2020.Model-baseddeeplearning. Proc.IEEE\n111,5(2023),465–499.\n[160] Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. 2018. Charades-ego: A\nlarge-scale dataset of paired third and first person videos. arXiv:1804.09626. Retrieved from https://arxiv.org/abs/\n1804.09626\n[161] N. Silberman and R. Fergus. 2011. Indoor scene segmentation using a structured light sensor. In Proceedings of the\nInternational Conference onComputerVision- Workshopon3D Representationand Recognition .\n[162] KarenSimonyanandAndrewZisserman.2014.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.\narXiv:1409.1556.Retrieved from https://arxiv.org/abs/1409.1556\n[163] LovejitSingh.2022.Deepbi-directionalLSTMnetworkwithCNNfeaturesforhumanemotionrecognitioninaudio-\nvideo signals. International Journal ofSwarm Intelligence 7,1(2022),110–122.\n[164] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. 2015. Sun rgb-d: A rgb-d scene understanding benchmark\nsuite. InProceedings oftheIEEEConferenceonComputer Visionand PatternRecognition . 567–576.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:33\n[165] YangSongandStefanoErmon.2019.Generativemodelingbyestimatinggradientsofthedatadistribution. Advances\nin Neural Information Processing Systems 32(2019),11918–11930.\n[166] SabrinaI.Soraya,Shao-PingChuang,Yu-CheeTseng,Tsì-Uíİk,andYu-TaiChing.2019.Acomprehensivemultisen-\nsor dataset employing RGBD camera, inertial sensor and web camera. In Proceedings of the 2019 20th Asia-Pacific\nNetwork Operations and Management Symposium (APNOMS) .IEEE, 1–4.\n[167] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. 2019. Learning video representations using con-\ntrastivebidirectional transformer.arXiv:1906.05743.Retrievedfrom https://arxiv.org/abs/1906.05743\n[168] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019. Videobert: A joint model for\nvideo and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer\nVision.7464–7473.\n[169] Licai Sun, Bin Liu, Jianhua Tao, and Zheng Lian. 2021. Multimodal cross-and self-attention network for speech\nemotion recognition. In Proceedings of the ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) .IEEE, 4275–4279.\n[170] Siyang Sun, Xiong Xiong, and Yun Zheng. 2022. Two stage multi-modal modeling for video interaction analysis\nin deep video understanding challenge. In Proceedings of the 30th ACM International Conference on Multimedia .\n7040–7044.\n[171] Yuxiang Sun, Weixun Zuo, and Ming Liu. 2019. Rtfnet: Rgb-thermal fusion network for semantic segmentation of\nurbanscenes. IEEERobotics andAutomation Letters 4,3(2019),2576–2583.\n[172] Hao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder representations from transformers. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJointConferenceonNatural Language Processing(EMNLP-IJCNLP) ,Hong Kong, 5100–5111.\n[173] YunPeng Tan, Fangyu Liu, BoWei Li, Zheng Zhang, and Bo Zhang. 2022. An efficient multi-view multimodal data\nprocessingframeworkforsocialmediapopularityprediction.In Proceedingsofthe30thACMInternationalConference\non Multimedia . 7200–7204.\n[174] ZinengTang,JieLei,andMohitBansal.2021.Decembert:Learningfromnoisyinstructionalvideosviadensecaptions\nandentropyminimization.In Proceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationfor\nComputational Linguistics: Human Language Technologies .2415–2426.\n[175] Zhulin Tao, Yinwei Wei, Xiang Wang, Xiangnan He, Xianglin Huang, and Tat-Seng Chua. 2020. Mgat: Multimodal\ngraphattentionnetwork forrecommendation. Information Processing andManagement 57,5(2020),102277.\n[176] Gijs van Tulder and Marleen de Bruijne. 2018. Learning cross-modality representations from multi-modal images.\nIEEETransactions on Medical Imaging 38,2(2018),638–648.\n[177] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,ŁukaszKaiser,andIllia\nPolosukhin.2017.Attentionisallyouneed. AdvancesinNeuralInformationProcessingSystems 30(2017),5998–6008.\n[178] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description\nevaluation. In Proceedings oftheIEEEConferenceonComputerVision andPattern Recognition . 4566–4575.\n[179] JananiVenugopalan,LiTong,HamidRezaHassanzadeh, andMayDWang.2021.Multimodaldeeplearningmodels\nfor earlydetectionof Alzheimer’s disease stage. Scientific Reports 11,1(2021),1–13.\n[180] GuoqingWang,ChuanxinLan,HuHan,ShiguangShan,andXilinChen.2019.Multi-modalfacepresentationattack\ndetectionviaspatialandchannelattentions.In ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern\nRecognition Workshops .0–0.\n[181] Huiqing Wang, Jingjing Wang, Chunlin Dong, Yuanyuan Lian, Dan Liu, and Zhiliang Yan. 2020. A novel approach\nfor drug-target interactions prediction based on multimodal deep autoencoder. Frontiers in Pharmacology 10 (2020),\n1592.\n[182] Junbo Wang, Wei Wang, Liang Wang, Zhiyong Wang, David Dagan Feng, and Tieniu Tan. 2020. Learning visual\nrelationship andcontext-awareattentionfor imagecaptioning. Pattern Recognition 98(2020),107075.\n[183] Lichen Wang, Zhengming Ding, Zhiqiang Tao, Yunyu Liu, and Yun Fu. 2019. Generative multi-view human action\nrecognition. In Proceedings oftheIEEE/CVFInternational ConferenceonComputerVision .6212–6221.\n[184] PengWang,QiWu,Chunhua Shen, Anthony Dick,andAnton VanDenHengel.2017.Fvqa: Fact-basedvisualques-\ntionanswering. IEEETransactions onPatternAnalysis andMachine Intelligence 40,10(2017),2413–2427.\n[185] Tongxin Wang, Wei Shao, Zhi Huang, Haixu Tang, Jie Zhang, Zhengming Ding, and Kun Huang. 2021. MOGONET\nintegratesmulti-omicsdatausinggraphconvolutionalnetworksallowingpatientclassificationandbiomarkeriden-\ntification. Nature Communications 12,1(2021),1–13.\n[186] XiaolongWang,RossGirshick,AbhinavGupta,andKaimingHe.2018.Non-localneuralnetworks.In Proceedingsof\ntheIEEEConferenceonComputer VisionandPattern Recognition . 7794–7803.\n[187] Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Yu Rong, and Junzhou Huang. 2020. Deep multimodal\nfusion bychannelexchanging. Advances in Neural Information Processing Systems 33(2020),4835–4845.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:34 F. Zhaoetal.\n[188] YouzeWang,ShengshengQian,JunHu,QuanFang,andChangshengXu.2020.Fakenewsdetectionviaknowledge-\ndrivenmultimodalgraphconvolutionalnetworks.In Proceedingsofthe2020InternationalConferenceonMultimedia\nRetrieval. 540–547.\n[189] Yanan Wang, Jianming Wu, Kazuaki Furumai, Shinya Wada, and Satoshi Kurihara. 2022. VAE-based adversarial\nmultimodaldomaintransferfor video-level sentimentanalysis. IEEEAccess 10(2022),51315–51324.\n[190] Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, and Jure Leskovec. 2022. Vqa-gnn: Reasoning with\nmultimodal semantic graph for visual question answering. arXiv:2205.11501. Retrieved from https://arxiv.org/abs/\n2205.11501\n[191] ZhixiangWang,ZhengWang,YinqiangZheng,Yung-YuChuang,andShin’ichiSatoh.2019.Learningtoreducedual-\nleveldiscrepancyforinfrared-visiblepersonre-identification.In ProceedingsoftheIEEE/CVFConferenceonComputer\nVision and Pattern Recognition . 618–626.\n[192] Haoran Wei, Pranav Chopada, and Nasser Kehtarnavaz. 2020. C-MHAD: Continuous multimodal human action\ndatasetof simultaneous video andinertialsensing. Sensors20,10(2020),2905.\n[193] YinweiWei,XiangWang,LiqiangNie,XiangnanHe,andTat-SengChua.2020.Graph-refinedconvolutionalnetwork\nfor multimedia recommendation with implicit feedback. In Proceedings of the 28th ACM International Conference on\nMultimedia . 3541–3549.\n[194] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-\nmodalgraphconvolutionnetworkforpersonalizedrecommendationofmicro-video.In Proceedingsofthe27thACM\nInternational Conference on Multimedia . 1437–1445.\n[195] Michael Wray, Hazel Doughty, and Dima Damen. 2021. On semantic similarity in video retrieval. In Proceedings of\ntheIEEE/CVFConferenceonComputerVision andPattern Recognition . 3650–3660.\n[196] AncongWu,Wei-ShiZheng,Hong-XingYu,ShaogangGong,andJianhuangLai.2017.RGB-infraredcross-modality\nperson re-identification. In Proceedings oftheIEEEInternational ConferenceonComputer Vision .5380–5389.\n[197] Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and Subhransu Maji. 2020. Phrasecut: Language-based image\nsegmentation in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n10216–10225.\n[198] ChunleiWu,YiweiWei,XiaoliangChu,SunWeichen,FeiSu,andLeiquanWang.2018.Hierarchicalattention-based\nmultimodalfusion forvideo captioning. Neurocomputing 315(2018),362–370.\n[199] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023. NExT-GPT: Any-to-Any Multimodal LLM.\narXiv:2309.05519.Retrievedfrom https://arxiv.org/abs/2309.05519\n[200] Yang Wu, Pengwei Zhan, Yunjian Zhang, Liming Wang, and Zhen Xu. 2021. Multimodal Fusion with Co-Attention\nNetworks for Fake News Detection. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 .\n2560–2569.\n[201] Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze,\nand Luke Zettlemoyer. 2021. VLM: Task-agnostic video-language model pre-training for video understanding.\narXiv:2105.09996.Retrievedfrom https://arxiv.org/abs/2105.09996\n[202] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-vtt: A large video description dataset for bridging video and\nlanguage.In Proceedings oftheIEEEConferenceonComputerVision andPattern Recognition . 5288–5296.\n[203] Kele Xu, Ming Feng, and Weiquan Huang. 2022. Seeing Speech: Magnetic Resonance Imaging-Based Vocal Tract\nDeformationVisualizationUsingCross-ModalTransformer.In Proceedingsofthe30thACMInternationalConference\non Multimedia . 6947–6949.\n[204] RuiqingXu,ChaoLi,JunchiYan,ChengDeng,andXianglongLiu.2019.GraphConvolutionalNetworkHashingfor\nCross-Modal Retrieval.In Proceedings oftheIjcai . 982–988.\n[205] ZhaoyangXu,JinguangGu,MaofuLiu,GuangyouZhou,HaidongFu,andChenQiu.2023.Aquestion-guidedmulti-\nhop reasoning graph network for visual question answering. Information Processing and Management 60, 2 (2023),\n103207.\n[206] Enquan Yang, Wujie Zhou, Xiaohong Qian, Jingsheng Lei, and Lu Yu. 2023. DRNet: Dual-stage refinement network\nwith boundary inference for RGB-D semantic segmentation of indoor scenes. Engineering Applications of Artificial\nIntelligence 125(2023),106729.\n[207] QiYang,SergeyNikolenko,AlfredHuang,andAleksandrFarseev.2022.Personality-DrivenSocialMultimediaCon-\ntentRecommendation.In Proceedings ofthe30th ACMInternational ConferenceonMultimedia . 7290–7299.\n[208] Xu Yang, Jiawei Peng, Zihua Wang, Haiyang Xu, Qinghao Ye, Chenliang Li, Ming Yan, Fei Huang, Zhangzikang Li,\nandYuZhang.2023.TransformingVisualSceneGraphstoImageCaptions.In Proceedingsofthe61stAnnualMeeting\nof theAssociation for Computational Linguistics (Volume1: LongPapers) . Toronto, 12427–12440.\n[209] XuYang,KaihuaTang,HanwangZhang,andJianfeiCai.2019.Auto-encodingscenegraphsforimagecaptioning.In\nProceedings oftheIEEE/CVFConferenceonComputer VisionandPattern Recognition . 10685–10694.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\nDeep Multimodal Data Fusion 216:35\n[210] TingYao,YingweiPan,YehaoLi,andTaoMei.2018.Exploringvisualrelationshipforimagecaptioning.In Proceedings\noftheEuropean ConferenceonComputer Vision(ECCV) .684–699.\n[211] LinweiYe,MrigankRochan,ZhiLiu,andYangWang.2019.Cross-modalself-attentionnetworkforreferringimage\nsegmentation.In Proceedings of theIEEE/CVFConferenceonComputer VisionandPattern Recognition . 10502–10511.\n[212] Mang Ye, Xiangyuan Lan, Jiawei Li, and Pong Yuen. 2018. Hierarchical discriminative learning for visible thermal\nperson re-identification.In Proceedings of theAAAIConferenceonArtificial Intelligence .\n[213] Mang Ye, Jianbing Shen, David J. Crandall, Ling Shao, and Jiebo Luo. 2020. Dynamic dual-attentive aggregation\nlearningforvisible-infraredpersonre-identification.In ProceedingsoftheComputerVision–ECCV2020:16thEuropean\nConference,Part XVII16 . Springer,229–247.\n[214] ChengxiangYin,KunWu,ZhengpingChe,BoJiang,ZhiyuanXu,andJianTang.2021.Hierarchicalgraphattention\nnetworkforfew-shotvisual-semanticlearning.In ProceedingsoftheIEEE/CVFInternationalConferenceonComputer\nVision.2177–2186.\n[215] Jeewoo Yoon, Chaewon Kang, Seungbae Kim, and Jinyoung Han. 2022. D-vlog: Multimodal vlog dataset for depres-\nsion detection.In Proceedings of theAAAIConferenceonArtificial Intelligence .12226–12234.\n[216] JunYu,JingLi,ZhouYu,andQingmingHuang.2019.Multimodaltransformerwithmulti-viewvisualrepresentation\nfor imagecaptioning. IEEETransactions onCircuits andSystems forVideo Technology 30,12(2019),4467–4480.\n[217] JingYu,Yuhang Lu,WeifengZhang,ZengchangQin,YanbingLiu,andYueHu. 2020.Learningcross-modal correla-\ntions byexploringinter-word semanticsandstacked co-attention. Pattern Recognition Letters 130(2020),189–198.\n[218] Jing Yu, Chenghao Yang, Zengchang Qin, Zhuoqian Yang, Yue Hu, and Zhiguo Shi. 2019. Semantic modeling of\ntextual relationships in cross-modal retrieval. In Proceedings of the International Conference on Knowledge Science,\nEngineering andManagement . Springer, 24–32.\n[219] LichengYu,PatrickPoirson,ShanYang,AlexanderC.Berg,andTamaraL.Berg.2016.Modelingcontextinreferring\nexpressions. In Proceedings oftheEuropean ConferenceonComputer Vision .Springer, 69–85.\n[220] Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao. 2018. Beyond bilinear: Generalized multimodal\nfactorized high-order pooling for visual question answering. IEEE Transactions on Neural Networks and Learning\nSystems29,12(2018),5947–5959.\n[221] Yu Yuan, Jiaqi Wu, Zhongliang Jing, Henry Leung, and Han Pan. 2022. Multimodal Image Fusion based on Hy-\nbrid CNN-Transformer and Non-local Cross-modal Attention. arXiv:2210.09847. Retrieved from https://arxiv.org/\nabs/2210.09847\n[222] Yuan Yuan, Zhitong Xiong, and Qi Wang. 2019. Acm: Adaptive cross-modal graph convolutional neural networks\nfor rgb-dscene recognition. In Proceedings oftheAAAIConferenceonArtificial Intelligence .9176–9184.\n[223] Zhaoquan Yuan, Siyuan Sun, Lixin Duan, Changsheng Li, Xiao Wu, and Changsheng Xu. 2020. Adversarial multi-\nmodalnetwork formoviestory question answering. IEEETransactions onMultimedia 23(2020),1744–1756.\n[224] AmirZadeh,MinghaiChen,SoujanyaPoria,ErikCambria,andLouis-PhilippeMorency.2017.Tensorfusionnetwork\nfor multimodal sentiment analysis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing , Copenhagen,Denmark,1103–1114.\n[225] YawenZeng,YiruWang,DongliangLiao,GongfuLi,JinXu,XiangminXu,BoLiu,andHongMan.2024.Contrastive\ntopic-enhanced networkfor video captioning. Expert Systems with Applications 237(2024),121601.\n[226] BeibeiZhang,YaqunFang,TongweiRen,andGangshanWu.2022.Multimodalanalysisfordeepvideounderstanding\nwithvideolanguagetransformer.In Proceedingsofthe30thACMInternationalConferenceonMultimedia .7165–7169.\n[227] Hao Zhang, Han Xu, Xin Tian, Junjun Jiang, and Jiayi Ma. 2021. Image fusion meets deep learning: A survey and\nperspective. Information Fusion 76(2021),323–336.\n[228] Renrui Zhang, Ziyao Zeng, Ziyu Guo, and Yafeng Li. 2022. Can language understand depth?. In Proceedings of the\n30th ACMInternational ConferenceonMultimedia . 6868–6874.\n[229] Shan Zhang, Baocheng Geng, Pramod K. Varshney, and Muralidhar Rangaswamy. 2019. Fusion of deep neural net-\nworks for activity recognition: A regular vine copula based approach. In Proceedings of the 2019 22th International\nConferenceonInformation Fusion(FUSION) .IEEE, 1–7.\n[230] Shifeng Zhang, Ajian Liu, Jun Wan, Yanyan Liang, Guodong Guo, Sergio Escalera, Hugo Jair Escalante, and Stan Z.\nLi. 2020. Casia-surf: A large-scale multi-modal benchmark for face anti-spoofing. IEEE Transactions on Biometrics,\nBehavior, andIdentity Science 2,2(2020),182–193.\n[231] Shan Zhang, Pranay Sharma, Baocheng Geng, and Pramod K. Varshney. 2022. Distributed estimation in large scale\nwireless sensor networks via a two step group-based approach. arXiv:2203.09567. Retrieved from https://arxiv.org/\nabs/2203.09567\n[232] Shifeng Zhang, Xiaobo Wang, Ajian Liu, Chenxu Zhao, Jun Wan, Sergio Escalera, Hailin Shi, Zezheng Wang, and\nStan Z. Li. 2019. A dataset and benchmark for large-scale multi-modal face anti-spoofing. In Proceedings of the\nIEEE/CVFConferenceonComputer Visionand PatternRecognition . 919–928.\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n216:36 F. Zhaoetal.\n[233] YulunZhang,KunpengLi,KaiLi,LichenWang,BinengZhong,andYunFu.2018.Imagesuper-resolutionusingvery\ndeep residual channel attention networks. In Proceedings of the European Conference on Computer Vision (ECCV) .\n286–301.\n[234] Yifei Zhang, Olivier Morel, Marc Blanchon, Ralph Seulin, Mojdeh Rastgoo, and Désiré Sidibé. 2019. Exploration of\ndeep learning-based multimodal fusion for semantic road scene segmentation. In Proceedings of the VISIGRAPP (5:\nVISAPP).336–343.\n[235] YifeiZhang,DésiréSidibé,OlivierMorel,andFabriceMériaudeau.2021.Deepmultimodalfusionforsemanticimage\nsegmentation:A survey. Image and VisionComputing 105(2021),104042.\n[236] Yang Zhang, Chenyun Xiong, Junjie Liu, Xuhui Ye, and Guodong Sun. 2023. Spatial-information guided adaptive\ncontext-awarenetworkfor efficientrgb-d semanticsegmentation. IEEESensorsJournal 23,19(2023),23512–23521.\n[237] DexinZhao,ZhiChang,andShutaoGuo.2019.Amultimodalfusionapproachforimagecaptioning. Neurocomputing\n329(2019),476–485.\n[238] Qiankun Zhao, Yingcai Wan, Jiqian Xu, and Lijin Fang. 2023. Cross-modal attention fusion network for RGB-D\nsemanticsegmentation. Neurocomputing 548(2023),126389.\n[239] XiaoleiZhao,JingZhang,JimiaoTian,LiZhuo,andJieZhang.2020.Residualdensenetworkbasedonchannel-spatial\nattentionfor thescene classificationof ahigh-resolution remotesensing image. Remote Sensing 12,11(2020),1887.\n[240] ZhijiaZheng,DonghanXie,ChunlinChen,andZhangqingZhu.2020.Multi-resolutioncascadednetworkwithdepth-\nsimilar residual module for real-time semantic segmentation on RGB-D images. In Proceedings of the 2020 IEEE In-\nternational ConferenceonNetworking, Sensing andControl(ICNSC) .IEEE, 1–6.\n[241] JieZhou,GanquCui,ShengdingHu,ZhengyanZhang,ChengYang,ZhiyuanLiu,LifengWang,ChangchengLi,and\nMaosong Sun. 2020.Graph neuralnetworks: A review of methods andapplications. AIOpen1(2020),57–81.\n[242] LuoweiZhou,ChenliangXu,andJasonJ.Corso.2018.Towardsautomaticlearningofproceduresfromwebinstruc-\ntionalvideos. In Proceedings ofthe32ndAAAIConferenceonArtificial Intelligence , 7590–7598.\n[243] Tongxue Zhou, Su Ruan, and Stéphane Canu. 2019. A review: Deep learning for medical image segmentation using\nmulti-modalityfusion. Array3(2019),100004.\n[244] Linchao Zhu and Yi Yang. 2020. Actbert: Learning global-local video-text representations. In Proceedings of the\nIEEE/CVFConferenceonComputer VisionandPattern Recognition . 8746–8755.\nReceived 6November 2022; revised 25 October 2023; accepted 31 January 2024\nACMComput.Surv., Vol. 56,No. 9,Article 216.Publicationdate:April 2024.\n\n\n</文献>\n\n<我的研究方法>\n在本硕士研究中，我围绕血液试管的自动化识别与分类展开，设计并实现了一个基于计算机视觉的多模态检测系统 —— BloodScan。该系统以深度学习为核心，结合机械臂控制与高分辨率图像采集，实现了对血液样本中不同分层（血清层、白膜层、血浆层）的高精度识别与评估。\n\n研究的核心创新在于提出了跨模态注意力机制（CrossModalAttention），通过蓝光与白光图像的融合，提升了血液分层边界的识别精度。在模型设计上，采用双Backbone结构分别提取不同光源下的特征，并在多个尺度上实现跨模态特征融合，兼顾检测精度与计算效率。相比传统YOLO架构，本研究提出的跨模态注意力策略在准确率、召回率上取得显著提升。\n\n系统模块涵盖了数据采集与增强、YOLO模型训练与评估、注意力可视化、以及机械臂和摄像头的软硬件联动控制。为提升模型训练效率，还构建了6通道融合数据集，并制定了严格的“精确一次检测”医学评估标准。\n\n本项目计划不仅在方法上具有创新性，也希望在工程实现层面达到高完整度，具备良好的可复现性和扩展性，为临床血液分析的自动化提供了有力支撑。\n</我的研究方法>\n\n<分析要求>\n1. 请分析上述文献的主要研究目标、方法和结论，重点关注其在血液分析检测、图像识别、深度神经网络、图像处理、计算机视觉、多模态融合或医学图像分析等方面的内容\n2. 总结该文献的提纲和关键信息\n\n请以学术严谨的方式进行分析，避免过度推断或主观评价。针对每个方面提供具体、详细的比较分析，而不是笼统的概述。\n</分析要求>\n"}], "temperature": 0.7, "max_tokens": 8000, "response_format": {"type": "text"}, "top_p": 1, "frequency_penalty": 0.0, "presence_penalty": 0.0}}
